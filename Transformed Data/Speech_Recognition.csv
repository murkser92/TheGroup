"","Datum","Titel","Artikel"
"1",2014-12-20,"Eyes speak louder than words Sensomotoric Instruments develops eye-tracking technology","Eyes speak louder than words Sensomotoric Instruments develops eye-tracking technology381 words12 August 2014Märkische AllgemeineMARKALTabloideGermanCopyright 2014 Märkische Allgemeine – Brandenburgs beste Seiten. All rights reserved. For further information see   http://www.MaerkischeAllgemeine.de[http://www.MaerkischeAllgemeine.de]By Angelika PentsiArnd Rose from Potsdam puts on a pair of black plastic glasses, plugs a cable into his smartphone, and starts looking around the room. A table catches his attention and his gaze lingers there for a moment. A small dot on his smartphone screen follows every movement his eyes make. Rose (36) is demonstrating his company's eye-tracking technology. Sensomotoric Instruments (SMI) in Teltow (district of Potsdam-Mittelmark) has spent more than 20 years working on ways of recording eye movements and lines of sight.Eberhard Schmidt, SMI's managing director, explains that the company grew out of a university research project on maintaining balance during space flight. It has since become the world's second largest supplier of eye-tracking systems. Schmidt says SMI's clients include technology heavyweights like Sony and Google, and the Babylab at the University of Potsdam. The systems are used in everything from market research to neurological studies.The basic assumption behind the technology is that eyes speak louder than words. Eyes will tell you how children learn to perceive their environment, how hand-eye coordination works for surgeons, and which cereal box design is most attractive to customers. Schmidt, an electrical engineer who joined SMI in 1997, says the eye-tracking system is based on a very simple idea: ""The camera takes a picture of your face, or rather your eyes, and then calculates a series of parameters. These allow the system to work out how your eyes are moving and where you're looking at any given moment."" Over the past two decades, and thanks in part to EU funding, SMI has been able to get the technology to a highly sophisticated stage of development.Rose says that in the early days users would have to push the computer equipment around with them in a supermarket trolley. Now all they need are the glasses and a smartphone.Schmidt predicts that eye-tracking systems will develop in much the same way as speech-recognition software. He says our eyes will soon become part of many everyday interactive applications. When that happens, you'll be able to answer your phone just by looking at it.3821794Maerkische Verlags-und Druck-Gesellschaft mbH PotsdamDocument MARKAL0020140812ea8c000hf"
"2",2005-03-20,"Weniger Pflegeaufwand und mehr Effizienz für den Benutzer  ","Weniger Pflegeaufwand und mehr Effizienz für den Benutzer  Alex Wolschann  468 words3 May 2005ComputerweltCMPWLT19 / 2005German© 2005 Info Technologie Verlag GmbH. All rights reserved. For further information see  http://www.computerwelt.at[http://www.computerwelt.at]Version 2.0 der HiPath OpenScape Software-Suite aus dem Hause Siemens wartet mit einem Software Development Kit und natürlicher Spracherkennung auf. Dadurch sollen Kommunikation und Applikationsentwicklung effizienter ablaufen können.  Wien – HiPath OpenScape ist die Software-Suite des Siemens-Bereichs Information and Communication Networks (ICN) für Presence Management, Conferencing und Kollaboration. Die Anwendung zur Echtzeitkommunikation soll für effizientere Teamarbeit und optimierte Arbeitsabläufe sorgen. Interaktionsintensive Geschäftsprozesse könnten Siemens zufolge mit HiPath OpenScape besser organisiert werden.  Version 2.0 der HiPath OpenScape-Applikationssuite, die auf der Cebit 2004 erstmals vorgestellt wurde, ist jetzt am Markt verfügbar und bietet als neue Funktionen zum Beispiel ein automatisches Update des Verfügbarkeits- und Erreichbarkeitsstatus und ein »Software Development Kit« für eine einfachere Integration in Geschäftsprozesse und Applikationen. Mit einem neuen automatischen Update des Verfügbarkeits- und Erreichbarkeitsstatus wird der Pflegeaufwand für den einzelnen Benutzer reduziert. Dabei werden Einträge aus dem Microsoft-Outlook-Kalender automatisch übernommen und als Information über die Verfügbarkeit des Teilnehmers angezeigt, zum Beispiel ""im Meeting"", ""auf Dienstreise"" oder Ähnliches. Diese Informationen werden über den neuen Presence Indicator an den Anrufer übermittelt und zeigen dessen Erreichbarkeitsstatus an. Gleichzeitig wird er informiert, über welches Endgerät die gewünschte Person im Moment kontaktiert werden kann.  INTEGRATION VON GESCHÄFTSPROZESSEN  HiPath OpenScape basiert auf der Web-Services-Architecture und offenen Standards wie Session Internet Protocol (SIP), SIMPLE (SIP for Instant Messaging and Presence Leveraging Extensions), SALT (Speech Application Language Tags), SOAP (Simple Object Access Protocol). Damit können präsenzorientierte Features leicht in gängige IT-gestützte Geschäftsprozesse eingebunden werden, beispielsweise in CRm-Datenbanken (Customer Relationship Management) oder in Back-Office-Plattformen.  Um die Entwicklung individueller Lösungen rund um HiPath OpenScape zu erleichtern und andere Anwendungen besser anbinden zu können, ist für Partner und Kunden ein.  Software Development Kit (SDK) verfügbar. Bestandteil dieses SDK ist unter anderem ein Web Services Toolkit mit ausführlich dokumentierten Schnittstellen, so genannten API (Applikation Programm Interfaces) für Entwickler und Integratoren. Es stellt eine SIP- und SOAP/XML-basierende, offene Applikationsumgebung zur Verfügung, die unabhängig von bestehenden Plattformen zur Sprachkommunikation betrieben werden kann.  Ein weiteres neues Features der Version 2.0 ist zum Beispiel die Funktion »Tell Me When«: Wenn ein dringend gewünschter Gesprächspartner im Moment nicht verfügbar ist, kann der Anrufer veranlassen, dass er eine sofortige Benachrichtigung erhält, sobald dieser wieder erreichbar ist. Weitere Leistungsmerkmale sind eine verbesserte Skalierbarkeit gegenüber Version 1.0 und natürliche Spracherkennung (Natural Speech Recognition) für sprachunterstützte Funktionen. Die Rules Engine wird um einen Assistenten zur Regeldefinition erweitert, über den auf Wunsch und bei entsprechender Voreinstellung die Änderungen des Verfügbarkeitsstatus und des bevorzugten Kommunikationsmittels automatisiert werden kann. Es wird dann zum Beispiel auf Grund der Einstellung ""im Urlaub"" in der OpenScape-Umgebung der Abwesenheits- Assistent für E-Mails (Microsoft Outlook) auf ""Bin zur Zeit nicht im Haus"" gesetzt.  Info Technologie Verlag GmbHDocument CMPWLT0020050505e1530001g"
"3",2004-02-13,"Voice XML 2.0 hat Standardreife.  ","Voice XML 2.0 hat Standardreife.  197 words13 February 2004ComputerwocheCPWCHEGerman(c) Computerwoche 2004  MÜNCHEN (IDG) - Das Web-Konsortium W3C hat die Version 2.0 der Spezifikationen für Voice XML als ""Proposed Recommendation"" veröffentlicht. Damit steht das Papier kurz vor der Standardisierung.  Voice XML soll es ermöglichen, auf Internet-Funktionen mittels Sprachbefehlen oder so genannten Touch-Tones (Töne, die zum Beispiel auf Telefontastaturen hinterlegt sind) zuzugreifen. Im Rahmen des vom W3C entwickelten ""Speech Interface Framework"" bildet Voice XML das Kernstück, das durch die Speech Synthesis Markup Language (SSML) ergänzt wird, die die Sprachausgabe des Rechners steuert, sowie durch die Speech Recognition Grammar Specification (SRGS), die Sprachbefehle definiert.  Die ""Candidate Recommendation"" hatte das W3C bereits vor einem Jahr herausgebracht. Bereits damals rechneten Experten damit, dass sich die Standardisierung in die Länge ziehen werde. Der Grund liegt in lizenzrechtlichen Ansprüchen, denn Voice XML basiert zum Teil auf Technologien, die sich Philips Electronics und die Rutgers University in New Jersey patentieren ließen. Das W3C wollte jedoch nur Standards verabschieden, die lizenzfrei zu nutzen sind. Eine neue Patentrichtlinie, nach der in Einzelfällen Ausnahmen zugelassen werden können, verabschiedete das Konsortium im Mai 2003. Die daraufhin eingesetzte Patent Advisory Group (PAG) konnte die lizenzrechtlichen Fragen mit den Patentinhabern klären. (lex/ue).  IDG Business Verlag GmbHDocument CPWCHE0020040216e02d0000a"
"4",2017-04-20,"newsbox: CREALOGIX erwirbt Machine Learning Technologie für Daten- und","newsbox: CREALOGIX erwirbt Machine Learning Technologie für Daten- und697 words4 September 201713:45AWP Original Press ReleasesAWOPDEGerman© 2017, awp Finanznachrichten AG. All rights reserved. Medienmitteilung Zürich, 4. September 2017 CREALOGIX erwirbt Machine Learning Technologie für Daten- und VideoanalyticsCREALOGIX übernimmt die führende Artificial Intelligence (AI)-Technologie vonKoemei. Die Lösung des Schweizer Startups und Spin-offs des ForschungsinstitutsIDIAP (Partner der Eidgenössischen Technischen Hochschule Lausanne EPFL)ermöglicht dank Machine Learning die automatisierte Umsetzung von Audio- undVideoinhalten in Textdaten für Analytics und die Optimierung. Dadurcherleichtert sie insbesondere die Verwertung multimedialer Inhalte.Datenkategorien, die in Zukunft massiv wachsen und deren Analyse heute beiUnternehmen noch stark vernachlässigt werden, können so effizient genutztwerden. Eine umfangreiche Konzeptsuche sowie Benutzungsanalytics vereinfachendas Handling zusätzlich. Der Digital Banking und Digital Learning Spezialistergänzt mit diesen innovativen AI-Funktionen sein bestehendes digitalesProdukteportfolio.Koemei entwickelte eine skalierbare Plattform im Umgang mit grossen multimediaDatensätzen, zur medienübergreifenden Suche und Analytics von Audio- undVideoinhalten. Ab Herbst 2017 wird die Technologie die Produktepalette desAnbieters für Digital Banking und Digital Learning Services bereichern. Mitdieser Integration können die ständig wachsenden Datenmengen für die Benutzerschnell verwendbar gemacht werden. Urs Widmer, CEO Digital Learning beiCREALOGIX: ""Wir sind begeistert, dass wir die marktführende Plattform vonKoemei erwerben konnten. Kombiniert mit unseren Digital Banking und DigitalLearning Produkten, verhilft diese neue Technologie unseren Kunden zu Mehrwertund Wettbewerbsvorteilen."" Machine Learning Software für optimale DatenanalyseDie Lösung von Koemei erreicht eine der weltweit besten Erkennungsqualitätenbei der Inhaltsanalyse von Audibles, Podcasts und Videos. Die Inhalte werdenhierfür über Automated Speech Recognition (ASR), also der automatisiertenSpracherkennung, in Texte umgewandelt. Anschliessend werden mit ArtificialIntelligence (AI) Tools und Natural Language Processing (NLP) die Texteanalysiert und die Inhalte gemäss vordefinierten Taxonomien und Themenautomatisch klassifiziert, wobei die Datensicherheit zu jeder Zeitgewährleistet ist. In den Videos werden spezifische Schlüsselbegriffeautomatisch erkannt und mit Tags versehen. Nutzer können dank diesen die fürsie relevanten Aussagen in jedem Video finden und direkt dort hinspringen. Diessteigert den Wert von Video- und Audio-Content enorm und spart Zeit bei derRecherche. Die Universität Genf hat bereits über 5'000 Stunden Vorlesungen mitder Technologie in benutzerfreundlicher Weise aufbereitet und diese nahtlos mitder Suche von Textdokumenten integriert. CREALOGIX passt die intelligentenFunktionen mit Konzeptsuche, Inhaltsanalytics und Statistiken den spezifischenAnforderungen seiner Kunden an.Über CREALOGIXDie CREALOGIX Gruppe (http://crealogix.com/[http://crealogix.com/]) ist ein unabhängiges SchweizerSoftwarehaus und gehört als Fintech Top 100 Unternehmen zu den Marktführern imDigital Banking. CREALOGIX entwickelt und implementiert innovativeFintech-Lösungen für die digitale Bank von morgen. Mit den Lösungen vonCREALOGIX antworten Banken auf die sich ändernden Kundenbedürfnisse im Bereichder Digitalisierung, um sich dadurch in einem extrem anspruchsvollen unddynamischen Markt zu behaupten und ihren Mitbewerbern stets einen Schritt vorauszu sein. Die 1996 gegründete Gruppe beschäftigt weltweit über 400Mitarbeitende. Die Aktien der CREALOGIX Gruppe (CLXN) werden an der SIX SwissExchange gehandelt. Die vorliegende Mitteilung enthält zukunftsgerichtete Aussagen, die mitgewissen Risiken, Unsicherheiten und Veränderungen behaftet sein können, dienicht voraussehbar sind und sich der Kontrolle der CREALOGIX Gruppe entziehen.CREALOGIX kann daher keine Zusicherungen machen bezüglich der Richtigkeitsolcher zukunftsgerichteter Aussagen, deren Auswirkung auf die finanziellenVerhältnisse der CREALOGIX Gruppe oder den Markt, in dem Aktien und anderenWertschriften der CREALOGIX Gruppe gehandelt werden.  Medienkontakt CREALOGIX AGJasmin Epp, Corporate Communications Officer Gerne vereinbare ich einen Interviewtermin mit der Geschäftsführung vonCREALOGIX für Sie. Bitte rufen Sie mich unter der Nummer +41 58 404 86 52 anoder senden Sie mir eine E-Mail: mailto:jasmin.epp@crealogix.com * Medienmitteilung (PDF): http://e1.marco.ch/publish/crealogix/55_2217/20170904-CREALOGIX_erwirbt_Machine_Learning_Software_zur_Daten-_und_Videoanalyse.pdf[http://e1.marco.ch/publish/crealogix/55_2217/20170904-CREALOGIX_erwirbt_Machine_Learning_Software_zur_Daten-_und_Videoanalyse.pdf] ________________________________________________________________________________This message has been created and distributed using the Software Suite marCo(R) - Market Communication Office(R). marCo(R) is essential to handle business critical information. Contact the developers of marCo(R) - Market Communication Office(R):A. Tensid EQS AG, Schochenmühlestrasse 4, 6340 Baar, SwitzerlandT. +41 41 763 00 50 (around-the-clock)E. marco@tensid.chVisit http://www.tensid.ch[http://www.tensid.ch] and get your marCo(R) today.--------------------------------------------------------------------------------By using this message (read, copy, etc.) you are bound to the disclaimer:https://www.tensid.ch/home-en/abouttensid/disclaimer/[https://www.tensid.ch/home-en/abouttensid/disclaimer/]--------------------------------------------------------------------------------Videoanalyticsawp Finanznachrichten AGDocument AWOPDE0020170904ed94000m9"
"5",2006-01-30,"Vier von fünf Anfragen im Call Center betreffen Standardvorgänge – Spracherkenner verstehen mittlerweile auch Dialekt und Slang gut genug - Sprachportal rechnet sich im Service zügig   ","Vier von fünf Anfragen im Call Center betreffen Standardvorgänge – Spracherkenner verstehen mittlerweile auch Dialekt und Slang gut genug - Sprachportal rechnet sich im Service zügig   1370 words30 January 2006Computer ZeitungCZTUNG016German(c) 2006 Konradin Verlag.All Rights Reserved. For further information see   http://www.computer-zeitung.de[http://www.computer-zeitung.de]Rund eine Million Mark sollte das Kundenkontaktcenter für die Sparda-Bank Hamburg 2001 kosten. Da stellte man sich bei der Bank die Frage, ob man für Standardanwendungen wie Auszahlungen einen Menschen braucht. Am Automat in der Vorhalle geht es schließlich auch anders.   Die Recherche des eigenen Kontostands und ähnliche Routinevorgänge machen immerhin 80 Prozent aller Kundenanrufe aus. Daraufhin begaben sich Jürgen Mehring, Experte für Inhouse Consulting und Organisation der Sparda Hamburg, und seine Kollegen auf die Suche nach einer sprechenden Maschine. Eine klassische Interactive-Voice Response-Lösung, wie Kunden sie bis heute meist bei computergesteuerten Hotlines finden, sollte es aber nicht sein. Vielmehr suchte man nach einer Lösung für ein Sprachportal, ein Natural Language Understanding System (NLU). „Wir hatten den Anspruch, das Beste zu finden“, betont Mehring im Rückblick.   Fast ein Jahr lang erkundeten die Banker den Markt und entschieden sich schließlich sich gegen alle Varianten mit vorher aufgezeichneten Stimmen und für das Text-to-Speech-System der schottischen Firma Rhetorical. „Wir mussten bis Schottland gehen für die beste deutsche Stimme“, so Mehring.   Rhetorical, im Jahr 2000 in Edinburgh gegründet, hatte gerade die weltweit erste Text-to-Speech-Konferenz veranstaltet. Inzwischen, so Mehring, wurde Rhetorical von der heutigen Nuance übernommen, von der die Sparda-Bank auch das Modul zur Spracherkennung eingekauft hat. Die Verknüpfung von Dialogen und notwendigen Daten in den verschiedenen Datenbanken, die Core Machine, realisiert Semantic Edge aus Berlin.   Das 2003 nach Tests in verschiedenen Einzelfilialen nach und nach für alle Zweigstellen der Sparda-Bank in Hamburg in Betrieb genommene System wurde 2004 bei den Voicedays gleich als beste Branchenlösung ausgezeichnet. Bis heute, so betont Mehring, müsse man suchen, um vergleichbar gute Lösungen zu finden.   Breite Anwendungen sind nun im Portal   Das NLU der Sparda-Bank Hamburg zeichne sich, so versichern Mehring und sein Kollege Rüdiger Möller, Leiter des Qualitätsmanagements, dadurch aus, dass Kunden frei von der Leber weg ihre Wünsche kundtun können. Zwar könnten Slang, Dialekt oder auch extreme Jugendsprache die Maschine verwirren. Wenn etwa selbst ein menschlicher Mitarbeiter Schwierigkeiten hätte, jemanden wegen starken Dialekts oder Akzents zu verstehen, dann versage auch die Maschine, so Mehring: „Doch wenn in dem, was der Anrufer sagt, das Wort Kontostand zu erkennen ist, bekommen Sie, was Sie wollen.“   Kunden können über das NLU derzeit neben der Konto- und Umsatzabfrage Überweisungen und Umbuchungen vornehmen, Bestellungen für Schecks oder ähnliches aufgeben, allgemeine Informationen erfragen oder sich weitervermitteln lassen zur Filiale oder dem Telefonberatungsteam. Jederzeit, so Mehring, kann der Anrufer der Maschine klar machen, dass er jetzt lieber einen Mitarbeiter sprechen will.   Nicht jeder spricht mit einer Maschine   Trotzdem hat man mit Blick auf die weniger technikaffinen Kunden das System einmal komplett neu eingestellt. Ursprünglich hatten die Sparda-Banker in ihrer Begeisterung alle Anrufe über das NLU-System geleitet und die Kunden einfach von dort aus zu den traditionellen Kanälen weitergeleitet. „Manche haben es einfach rundweg abgelehnt, mit einer Maschine zu sprechen,“ so Möller. „Da die Kundenzufriedenheit für uns sehr wichtig ist, haben wir reagiert.“   Inzwischen arbeitet die Bank wieder mit zwei separaten Service-Nummern, einer für das Call Center mit Menschen und der anderen für das Sprachportal mit „fast menschlicher Dialogführung“, wie es in einer Präsentation der Bank formuliert ist. Auch den Avatar Michaela May gibt es nicht mehr: „Diese Gleichsetzung der Maschine mit dem Menschen war vielen Kunden fremd“, berichtet Mehring.   Die neuen Zufriedenheitswerte seien gut, sagt Möller, das zeigte eine eigens extern in Auftrag gegebene Befragung von Kunden. In den Mailings und Briefen der Bank, im Kundenmagazin und sogar auf den Kontoauszügen wird das System weiterhin intensiv beworben, denn nach wie vor entfallen gerade bei den Anrufen in Filialen rund 50 Prozent auf Tätigkeiten, die übers NLU gut abgewickelt werden könnten.   Diese Gespräche kosten das Unternehmen 30 Cent pro Anruf. Zum Vergleich: Die Konversation mit dem Telefonberatungsteam kommt auf durchschnittlich vier Euro. Bei rund 80 000 Anrufen lässt sich die Kosteneinsparung ausrechnen. Bei den Gesamtkosten sei man unter den geplanten 500 000 Euro geblieben.   Natürlich, so Möller, werde das Gespräch mit dem Mitarbeiter immer erhalten bleiben. Das Sprachportal sei Bestandteil eines Gesamtsystems. Speziell das als eigene Abteilung arbeitende Telefonberatungsteam, das bevorzugt Neukunden betreut oder zu persönlichen Krediten, Baufinanzierungen, Fonds- oder Depotanlagen Auskunft gibt, ist nicht wegzudenken. Rund ein Viertel der Anrufe landet hier.   Umpolen würde die Sparda-Bank dagegen gerne noch mehr Direktanrufe, die bislang in der Filiale herauskommen. „Übrigens gibt es durchaus auch ältere Kunden, die das inzwischen gerne annehmen“, so Möller. Die Technikmuffel macht Mehring eher bei den um die 50-Jährigen aus. Möglich auch, dass nicht zuletzt ältere Kunden den Spareffekt beim Telefon-Banking schätzen. Um das Sprachportal zusätzlich schmackhaft zu machen, kostet der Anruf dort weniger als im Beratungscenter. Zusätzlich gibt es Bonuspunkte. Klappern, so Mehring, gehöre zum Ge- schäft. Monika Ermert/pg   „Verifikation der Kunden ist geplant“   Wie sicher ist es denn, wenn man seine Überweisungen dem Sparda-Sprachportal anvertraut? Und werden die Anrufe aufgezeichnet?   Für die Fälle, in denen Geld bewegt wird, zeichnen wir Gespräche in der Tat auf. Darüber werden die Kunden allerdings vorab informiert. Sie stimmen dem in der Vereinbarung für die Teilnahme am Sprachportalsystem explizit zu. Wenn Sie aber lediglich ihren Kontostand abrufen, wird von uns natürlich nichts aufgezeichnet.   Können Ihre Kunden die Sprachaufzeichnung vermeiden?   Sie können aus Datenschutzgründen in solchen Fällen ihre Befehle auch per Tastatur eingeben.   Bei Börsengeschäften würde dann auch alles aufgezeichnet?   Ja. Börsengeschäfte werden allerdings im Moment noch nicht über das Sprachportal erledigt. Für die Zukunft ist das aber denkbar, dass man über das Sprachportal auch ordern kann. Prinzipiell ist es möglich, das einzubinden. Wir haben sehr darauf geachtet, dass das System offen ist für weitere Dienste. Die Basis dafür sind die Standards HTML und XML. Was uns auch brennend interessiert, ist die Verifikation der Kunden über Sprache. Sie brauchen sich dann keine lästigen Geheimzahlen mehr zu merken, der Computer begrüßt sie vielmehr nach einigen Worten gleich persönlich.   Macht Ihr Beispiel eigentlich Schule bei anderen Banken und hatten Sie auch selbst Vorbilder?   Vorbilder hatten wir. Wir haben uns zum Beispiel das Sprachportal der Schweizer UBS sehr genau angesehen und uns nur deshalb für einen eigenen Weg entschieden, weil wir nicht den gleichen Aufwand in Bezug auf die Mehrsprachigkeit für notwendig gehalten haben. Die UBS muss alle Sprachen der Schweiz abdecken. Auch Nachahmer gibt es, aber wir halten unser System immer noch für eines der besten im deutschen Raum. Monika Ermert/pg   Voice-Lösung streicht Auszeichnung ein   Die Sparda-Bank Hamburg gehört mit rund 1,8 Millarden Euro Kundeneinlagen und 180 000 Kunden zu den großen Genossenschaftsbanken in Deutschland. Sie ist eines der technologisch innovativen Institute der Finanzbranche. So hat sie die erste reine Internetbank, die Netbank AG, gegründet. 2003 wurde das Sprachportal gestartet, über das Bankgeschäfte im Dialog mit einer Computerstimme abwickeln können. 2004 erhielt das Projekt den Voice Award. Zwar sind manche Kunden noch ängstlich, bei der Sparda ist man aber vom Konzept überzeugt. Monika Ermert/pg   Der ungeliebte Avatar verschwindet wieder   In der Anfangszeit wollte die Sparda-Bank mit dem Avatar Michaela May auf ihre Kunden zugehen. Doch das virtuelle Menschenimitat kam nicht gut an und ist mittlerweile in der Schublade verschwunden.   Technisch verbirgt sich hinter dem Sprachportal eine von Semantic Edge realisierte Plattform, die eine Automatic-Speech-Recognition-Software (ASR) des Herstellers Nuance zur Erkennung gesprochener Sprache beinhaltet. Für die andere Richtung, die Umsetzung von Text in Sprache, ist ebenfalls eine Lösung von Nuance im Einsatz. Sie bietet Schnittstellen zu verschiedenen Datenbanken und Applikationen.   Ein Siemens Hipath-4000-Router reicht die Gespräche über ein Phoneboard des Herstellers NMS an die dreiteilige Plattform beziehungsweise an die IP-Infrastruktur des Unternehmens weiter. Das Phoneboard ermöglicht es auch, vorhandene analoge Geräte an die Internet-Telefonieinfrastruktur anzuschließen.   Die Betrachtung der kompletten eingesetzen Telefonielösung gehörte für die Sparda-Bank zu dem veränderten Konzept. Die Offenheit für künftige Applikationen wiederum soll durch die XML-, HTML- und Java-Fähigkeit der Plattform gewährleistet werden.   Zwar wurde Michaela May inzwischen wieder von einem sachlich-realistischen Portal abgelöst, doch das System soll weiter ausgebaut und in Zukunft auch in Richtung Spracherkennung zur Authentifizierung getrimmt werden. Entsprechende Lösungen sind derzeit noch in Arbeit.   Monika Ermert/pg   Konradin Verlag Robert Kohlhammer GmbHDocument CZTUNG0020060130e21u0000g"
"6",2005-07-20,"W3C arbeitet an Standard zur Sprecheridentifizierung  ","News; ThemenüberblickW3C arbeitet an Standard zur Sprecheridentifizierung  Malte Jeschke  187 words7 December 2005tecCHANNELTECCHAGerman(c) 2005 tecCHANNEL.  www.tecchannel.de[http://www.tecchannel.de]Die nächste Version von VoiceXML soll biometrische Verfahren zur Personenerkennung integrieren.  Das W3-Consortium gab bekannt, dass es mit VoiceXML 3.0 auf Forderungen nach Sicherheitsfunktionen eingehen werde. Um dem Diebstahl von Passwörtern und Betrug vorzubeugen, wird die Auszeichnungssprache zukünftig um die Möglichkeit erweitert, biometrische Authentifizierungsverfahren zu integrieren. Derzeit nutzen verschiedene Hersteller proprietäre Extensionen für diesen Zweck. Sie sollen mit VoiceXML überflüssig werden.  Zweck der VoiceXML ist es, auf Internet-Funktionen mittels Sprachbefehlen oder Wähltönen (die zum Beispiel auf Telefontastaturen hinterlegt sind) zuzugreifen. Im Rahmen des vom W3C entwickelten ""Speech Interface Framework"" bildet VoiceXML das Kernstück, das durch die SRGS (Speech Recognition Grammar Specification), die Sprachbefehle definiert, und die SSML (Speech Synthesis Markup Language) ergänzt wird. Sie steuert die Sprachausgabe des Rechners und wird in der nächsten Version um die Unterstützung für asiatische Sprachen erweitert. Sie berücksichtigt die Tatsache, dass etwa im Chinesischen die Tonhöhe für die Bedeutung eines Worts ausschlaggebend ist. (Wolfgang Sommergut/mje)  Links zum Thema Internet   AngebotBookshop                   Bücher zum ThemaeBooks (50 % Preisvorteil) eBooks zum ThemaSoftware-Shop              Internet  433485IDG Communications Verlag AGDocument TECCHA0020060117e1c7002t5"
"7",2000-03-20,"Fonix bringt ihre Texte in Sprache umwandelnde Software FAAST TTS für Linux x86 auf den Markt.","Fonix bringt ihre Texte in Sprache umwandelnde Software FAAST TTS für Linux x86 auf den Markt.707 words3 August 2000OTS - OriginaltextserviceOTSGerman(c) 2000 News AktuellDie neue Version soll den Standard für sprachfähige Internet-Anwendungen auf der Linux-Plattform setzen. Die Fonix Corporation (OTC Bulletin Board: FONX), ein führender Anbieter von Benutzerschnittstellenlösungen für Mobilfunk-und mobile Geräte, Fahrzeugtelematik, Internet-und Telefonsysteme, gab die kommerzielle Einführung ihres FAAST (Fonix Accelerated Application Solutions Technology) Text-To-Speech (TTS) Software Development Kit (SDK) für Linux x86 bekannt.Die Markteinführung des Fonix FAAST TTS Linux x86 SDK bereitet den Weg für die Technologien von Fonix, den Standard für sprachfähige Anwendungen auf der Linux-Plattform zu setzen. Die neue Version ist für Entwickler gedacht, die sich schnelle Sprachintegration in ein Open Source-Hochgeschwindigkeitsbetriebssystem wünschen, und ermöglicht die nahtlose Integration der TTS-Software mit außergewöhnlicher Qualität und unbegrenztem Vokabular sowie eine speziell angepasste Vokabular-TTS.Die derzeitigen Kunden sowie die Partner bei der Beta-Version setzen Fonix TTS in den Unified Messaging Services, den Email-Readern, den HTML-basierten Web-Readern, der englischen Lernsoftware, den Auto-Begleitern, den Aktienkurssystemen, den auf dem Web basierenden College-Kursen, den Short Messaging Services, den Sprachportalen, den IVR-Systemen und weiteren Applikationen ein.""Ohne Fonix FAAST TTS für Linux würde unser Produkt Silent Messenger zwei PCs benötigen,"" sagte Jerry Geis, Vice President für Software Development von MessageNet Systems. ""Die neue Version wird unsere zusätzlichen Hardware-und Softwarekosten praktisch beseitigen.""Der Fonix FAAST TTS SDK für Linux x86 beinhaltet einen hochwertigen TTS Application Programmer Interface (API)-Satz, der die Kunden in die Lage versetzt, die Texte in Sprache umwandelnde Software (Text-To-Speech TTS) von Fonix problemlos in ihre Produkte zu integrieren. Darüber hinaus erleichtert die Nutzung der Red Hat Package Manager-Dateien durch den SDK die schnelle Installation und versetzt die Programmierer in die Lage, mit der Entwicklung fast sofort zu beginnen. Die Fonix FAAST API unterstützt außerdem die Programmiersprachen C++ und Java.""Um die wachsende Gemeinschaft der Linux-Softwareentwickler zu unterstützen, hat Fonix ein vielseitiges TTS-Entwicklungssystem hervorgebracht, das schnell Marktanteile gewinnen kann, indem es die Entwickler in die Lage versetzt, für ihre spezifischen Anwendungen die geeignetsten natürlich klingenden TTS einzusetzen,"" sagte Thomas A. Murdock, Chairman und Chief Executive Officer von Fonix. ""Wir verstehen die Notwendigkeit, die Entwicklung mehrerer Plattformen zu unterstützen, und freuen uns, Linux in die wachsende Familie der Plattformen aufzunehmen, die wir unterstützen.""Firmenprofil der Fonix CorporationDie Fonix Corporation (OTC Bulletin Board: FONX) entwickelt und vermarket Software-Produkte, die eine intuitive menschliche Interaktion mit Computern, Verbraucherelektronik und anderen intelligenten Geräten ermöglicht. Führende Hersteller von Chips, unabhängige Software-und Hardware-Verkäufer sowie andere Internet Content Provider integrierten die Fonix-Technologie, um ihre Produkte in ihrer Anwendung leichter und angenehmer zu gestalten. Die Fonix-Produkte, einschließlich Text-To-Speech (TTS), Automatic Speech Recognition (ASR) (automatische Spracherkennung) und Handwriting Recognition (HWR) (Erkennung von Handschriften) bieten die natürlichsten Kommunikationslösungen, die auf dem Markt sind. Weitere Informationen erhalten Sie unterwww.fonix.com[http://www.fonix.com/]oder telefonisch unter (USA) (801) 553-6600.Hinweis: Die von Fonix Corporation herausgegebenen Informationen, die nicht rein historischer Natur sind, beinhalten zukunftsbezogene Aussagen im Sinne der ""Safe Harbor""-Vorbehaltsklauseln des Private Securities Litigation Reform Act von 1995. Dazu gehören auch die Aussagen bezüglich der Erwartungen, Hoffnungen, Absichten und Strategien des Unternehmens für die Zukunft. Investoren werden ausdrücklich darauf hingewiesen, dass die zukunftsbezogenen Aussagen Risiken und Unwägbarkeiten unterliegen, die die Geschäftsaussichten und - leistungen des Unternehmens beeinträchtigen können. Es ist wichtig, zur Kenntnis zu nehmen, dass die tatsächlichen Ergebnisse des Unternehmens erheblich von denen in den zukunftsbezogenen Aussagen abweichen können. Die Risikofaktoren, einschließlich der allgemeinen wirtschaftlichen, wettbewerbsbedingten, staatlichen und technologischen Faktoren, sind in den bei der Securities and Exchange Commission (SEC - US-Börsenaufsicht) auf den Formularen 10-K, 10-Q und 8-K vom Unternehmen eingereichten Unterlagen aufgeführt. Das Unternehmen lehnt jegliche Verantwortung ab, die in dieser Pressemeldung enthaltenen zukunftsbezogenen Aussagen auf den neuesten Stand zu bringen.ots Originaltext: Fonix Corporation Im Internet recherchierbar:http://recherche.newsaktuell.de[http://recherche.newsaktuell.de]Rückfragen bitte an: Vertriebs-& Produktinformationen: Scott Lindsey, Tel.: +1 801-553-6600, sales@fonix.com, Medieninformationen: Kurt Herrmann, Tel.: +1 801-553-6600, mediarel@fonix.com, oder Investoreninformationen: Michelle Aamodt, Tel.: +1 801-328-0161 invrel@fonix.com, alle von der Fonix CorporationWebsite:http://www.fonix.com[http://www.fonix.com].Document ots0000020010816dw83005n2"
"8",2003-02-28,"Microsofts mobiles Duett im Vergleich.","Microsofts mobiles Duett im Vergleich.1541 words28 February 2003ComputerwocheCPWCHEGerman(c) Computerwoche 2003HANNOVER (hi) - Mobile Computing steht in diesem Jahr bei Microsoft hoch imKurs. Das Unternehmen präsentiert den Messebesuchern mit der ""Windows XPTablet PC Edition"" und den ""Windows Powered Smart Displays"" gleich zweiKonzepte für das mobile Arbeiten. Entsprechende Geräte findet der Besucherzudem bei den zahlreichen Hardwarepartnern von Microsoft.Mit dem Tablet PC und den Windows Smart Displays demonstriert Microsoft(Halle 4) auf der CeBIT gleich zwei Ansätze, die dem PC-Benutzer zu mehrBewegungsfreiheit abseits des Schreibtischs verhelfen sollen.Der grundlegende Unterschied zwischen den Tablet PCs und den Windows SmartDisplays, die auf der CeBIT ihre Europa-Premiere feiern, besteht darin, dasses sich bei Ersteren um komplette PCs mit Stifteingabe handelt, während dieSmart Displays quasi als Fernsteuerung für den stationären PC fungieren.Obwohl die Geräte eher im Consumer-Bereich positioniert sind, erfolgt dieVerbindung zum PC über Wireless LAN (802.11b) anstatt mit Bluetooth.Freiheit für zu HauseAuf diese Weise kann der Benutzer etwa von der Couch oder aus dem GartenE-Mails abrufen, im Web surfen, Musik hören oder andere Applikationenstarten, die auf dem PC installiert sind. Entsprechende Einsatzszenariendemonstriert Microsoft unter anderem im Zelt ""Windows Powered Smart DisplayExperience Center"" zwischen Halle 17 und 18. Obwohl bereits eine ganze Reihevon Herstellern entsprechende Geräte angekündigt haben, wird derMessebesucher nur bei wenigen Ausstellern fündig. So haben Philips (Halle21, Stand B06), Viewsonic (Halle 21, Stand B57), NEC (Halle 21, Stand B57)sowie Wyse Technologies (Halle 6, Stand A38) entsprechende Geräte sicher imGepäck. Bei Samsung und LG Electronics stand dies bis Redaktionsschluss nochnicht definitiv fest. LG zeigt eventuell in Halle 24, Stand C4, einenPrototypen.Damit die Kombination aus PC und Smart Display funktioniert, muss auf demArbeitsplatzrechner Windows XP Professional installiert sein, denn die SmartDisplays verwenden zur Kommunikation das ""Remote Desktop Protocol"" desBetriebssystems.Zugegeben, die Smart Displays sind vom Design her ein Augenschmaus, dochangesichts der hohen Verkaufspreise - das 15-Zoll-Display von Philips kostetim Komplettpaket fast 1700 Euro - stellt sich die Frage, ob der Anwendernicht mit einem der im November 2002 vorgestellten Tablet PCs besser bedientist. Diese kosten zwar noch mehr, fungieren dafür aber als kompletter PC,wie Microsoft auf dem Messestand in Halle 4 zeigt.Zwar hat Microsoft den Hardwareherstellern gewisse Anforderungen für dieTablet-Rechner ins Stammbuch geschrieben, doch dies schränkt lautProdukt-Manager Bastian Braun die Gestaltungsmöglichkeiten wenig ein. Sofordert Microsoft im Gegensatz zu den durch Berührung zu steuerndenBildschirmen (Touchscreens) der Pocket PCs oder Smart Displays für denTablet ein elektromagnetisches Digitizer-Display, das über einen Stiftbedient wird. Der Vorteil dabei ist, dass der Stift nicht direkt auf demBildschirm aufliegen muss, sondern bereits aus einem Abstand von einemZentimeter wirkt. Gleichzeitig ersetzt das Werkzeug die übliche Maus oderden Trackball.Ferner hat der Tablet PC dem Microsoft-Pflichtenheft zufolge aufherkömmliche Schnittstellen wie serielle und parallele Ports zu verzichten.Der Anschluss von Peripheriegeräten erfolgt mittels USB und ergänzendFirewire. Eine weitere wichtige Anforderung war, dass der Bildschirm derdigitalen Schreibbretter ein Arbeiten sowohl im Hoch-als auch im Querformaterlaubt.Angesichts dieser Vorgaben kristallisierten sich zwei Konstruktionskonzepteheraus: Hybridsysteme, die einem Subnotebook mit Tastatur und Touchpadähneln, sowie reine Tablets(Slates), an die sich optional eine Tastaturanschließen lässt. Aufgeklappt lassen sich die Hybride wie ein Notebooknutzen, während sie zugeklappt als reines Tablet fungieren. Vertreter diesesAnsatzes sind hierzulande Acer (Halle 25, Stand D40), Toshiba (Halle 1,Stand 6h2) sowie HP/Compaq (Halle 1,Stand 7i2). Fujitsu-Siemens (Halle 1,Stand 5e2), Viewsonic (Halle 21, Stand B57) und Paceblade (auf demMicrosoft-Stand) integrieren dagegen keine Tastatur.LeistungsspektrumSieht man von diesem Unterschied einmal ab, so wird die weite Preisspanneder Geräte von 2400 bis 3800 Euro nicht auf den ersten Blick verständlich.Erst eine Betrachtung des technischen Innenlebens der Tablet PCs erklärt dieDifferenzen. Vom Mobile Pentium 3 mit 800 Megahertz über denTransmeta-Prozessor Crusoe TMS 5800 mit einem Gigahertz bis hin zum mit 1,33Gigahertz getakteten Pentium reicht das Leistungsspektrum. Paart einHersteller, wie etwa Acer, nun den Pentium 800 mit einem günstigenGrafikchipsatz, der nur über 8 MB RAM verfügt, so ist die Produktion einesgünstigen Tablet für 2400 Euro möglich. Bei grafikintensiven, bewegtenAnwendungen geht das Gerät jedoch chancenlos in die Knie. Hier erlaubenKonkurrenzmodelle wie HP/Compaqs ""TC 1000"" mit einer Grafikkarte ""Geforce 2Go 100"" von Nvidia oder Toshibas ""Protégé 3500"" ein deutlich flotteresArbeiten.Ein weiteres Differenzierungsmerkmal ist die Bildschirmgröße. WährendPaceblade und Toshiba eine 12,1-Zoll-Anzeige verwenden, setzt der Rest derHersteller auf das preisgünstigere 10,4-Zoll-Format. Darüber hinaus hattePaceblade bei der Realisierung seines Displays eine pfiffige Idee: Es istDigitizer und Touchscreen in einem. Auf dem Bildschirm lässt sich eine großeTastatur einblenden, so dass das ""Pacebook Tablet"" etwa Journalisten alselektronische Reiseschreibmaschine dienen kann. Der Anschlag einermechanischen Tastatur wird dabei mit Hilfe eines Klicktons simuliert.AusstattungsmerkmaleSieht man einmal vom billigsten Acer-Modell ab, das ohne eingebauteWLAN-Ausstattung ausgeliefert wird (der größere Bruder mit WLAN kostet 200Euro mehr), zeigen sich alle Geräte kontaktfreudig: Fast Ethernet, Modem undWLAN-Zugang sind Standard. Besser ausgestattete Modelle verfügen zudem überBluetooth und Infrarot-Schnittstelle. Ergänzt um integrierte Lautsprecher,Mikrofon, PC-Card-Slot und Anschlussmöglichkeiten für Tastatur oderPeripheriegeräte wie DVD oder CD-ROM stellen die Tablet PCs einenvollständigen Notebook-Ersatz dar.Mit ihren für den Tischeinsatz konzipierten Brüdern haben die Tablets einManko gemeinsam: Sie wollen pfleglich behandelt werden. Selbst edleDesigner-Gehäuserahmen aus Magnesium-oder Titan-Legierung undglasfaserverstärkte Kunststoffbauteile können über eines nichthinwegtäuschen:Ein lässig während der morgendlichen Patientenvisite in der Armbeugegehaltener Tablet PC überlebt einen Absturz auf den Boden nicht unbeschadet.Ferner vermitteln die Abdeckungen der Peripherie-Anschlüsse teilweise - wieetwa bei HP/Compaqs TC 1000 - einen so filigranen Eindruck, dass derBenutzer Angst bekommt, sie könnten bereits beim genauen Hinschauenabbrechen.Kritikpunkte, die aber der Begeisterung der Anwender wohl keinen Abbruchtun. Während Marktforscher wie IDC den Tablet PCs für 2003 kaum einMarktpotenzial zutrauten, eroberte die neue Gerätegeneration bereits imvierten Quartal 2002 nach Angaben des Marktforschungsinstituts Context ausdem Stand einen Marktanteil von einem Prozent unter den portablen Rechnern.Und dies, obwohl der Verkauf erst am 7.November begann. Mit 38,5 Prozentstand das HP/Compaq-Modell dabei in der Käufergunst an erster Stelle,gefolgt von Acer mit 24,3 Prozent.Zwar lassen sich unter Microsofts Windows XP Tablet Edition alle Anwendungennutzen, die mit dem Betriebssystem Windows XP laufen. Ihr wahres Potenzialoffenbart die neue Gerätegeneration jedoch mit Software, die vollständig aufdie Stifteingabe abgestimmt ist.Stiftfähige Applikationen hatten zum Start des Tablet PC bereits über 20Softwareanbieter angekündigt, darunter bekannte Namen wie Adobe, Corel, SAP,Siebel oder Autodesk. Lediglich ein kleiner Teil dieser Hersteller zeigtseine Anwendungen jedoch auf der CeBIT.SAP (Halle 4, Stand D12 - 18)hat Mysap CRM um mobile Gerätefunktionen wieStifteingabe, Handschriftenerkennung und Spracheingabe erweitert. Auf dieseWeise können sich Außendienstmitarbeiter mit dem Stift durch dieverschiedenen Arbeitsebenen navigieren und handschriftlich direkt auf demBildschirm Notizen eingeben. Zudem können so Aufträge direkt auf dem TabletPC unterschrieben werden und die digitale Signatur dann in dasUnternehmenssystem einfließen.FormularbearbeitungScansoft Inc. (Halle 9, Stand E30, sowie Messestände von HP und Microsoft),bekannt durch Anwendungen wie ""Omnipage"" oder ""Omniform"", hat ebenfallsStiftlösungen erarbeitet. Auf der Messe demonstriert Scansoft, wie mitOmniform, einer Applikation zum Umwandeln von Papierformularen in XML-,HTML-oder Portable Document Format (PDF), digitale Formulare von Handausgefüllt werden. Die Handschriftenerkennung des Tablet PC digitalisiertdiese Informationen dann. Ferner stellt das Unternehmen seine Tools für dieUmwandlung von Text in Sprache sowie automatische Spracherkennung (AutomaticSpeech Recognition = ASR) nun auch Tablet-PC-Entwicklern zur Verfügung.Ebenfalls Tablet-PC-fähig ist ""Paperport"", eine Anwendung zum Scannen,Organisieren und Verteilen von Papier-und Digitaldokumenten.Nicht auf der Messe anzutreffen ist dagegen die kanadische SoftwareschmiedeCorel, die mit ""Grafigo"" ein Tool zur Digitalisierung von Skizzen imPortfolio hat. Mit Hilfe einer Symbolerkennung, so verspricht Corel, wandeltGrafigo handschriftliche Zeichnungen automatisch in präzise Grafiken um.Geografische SkizzenDer Dokumentation von räumlichen Daten oder Skizzen auf dem Tablet PC widmetsich die Esri Geoinformatik GmbH aus Kranzberg bei München. Der Herstellervon geografischen Informationssystemen (GIS), der in Hannover auf demAcer-Stand (Halle 25, Stand D40) zu finden ist, sieht das Einsatzgebietseiner Software ""Arcgis"" etwa bei der Unfallaufnahme im Straßenverkehr, derDokumentation von Leitungsnetzen oder der Kartierung von Naturgefahren.Eine Liste mit weiteren Herstellern, die spezielle Applikationen für denTablet PC anbieten, ist im Internet unterhttp://www.microsoft.com/windowsxp/tabletpc/partners/default.asp[http://www.microsoft.com/windowsxp/tabletpc/partners/default.asp]zu finden.AusstellerTablet PCMicrosoft, Halle 4Acer, Halle 25, Stand D40HP, Halle 1, 7i2Fujitsu-Siemens, Halle 1, 5a2Paceblade, Halle 4 bei MicrosoftToshiba, Halle 1, 6h2Viewsonic, Halle 21, B57Applikationen für Tablet PCsSAP, Halle 4, D12 - 18Scansoft, Halle 9, E30, sowie Messestände von HP und MicrosoftEsri Geoinformatik, Halle 25, D40Windows Smart DisplaysMicrosoft, Halle 4Smart Display Experience Center, Zelt zwischen Halle 17 und 18LG Electronics Halle 21, C4NEC, Halle 2, B20Philips, Halle 21, B06Viewsonic, Halle 21, B57Wyse Technologies, Halle 6, A38Die Liste erhebt keinen Anspruch auf Vollständigkeit und basiert aufHerstellerangaben.Document cpwche0020030228dz2s0000d"
"9",1996-03-19,"DIE SPRACHVERARBEITUNG IM FACHJARGON.","DIE SPRACHVERARBEITUNG IM FACHJARGON.105 words3 October 1996Der StandardDSTANGerman(c) 1996, Der Standard.http://www.derstandard.at/automatic speech recognition; automatische Spracherkennung durch entsprechende Software.SIR - speaker independent recognition; sprecherunabhängige Erkennung.STT - speech-to-text; Gesprochenes wird automatisch in ein EDV-kompatibles Format übertragen.TTS - text-to-speech; Bezeichnet den umgekehrten Forgang: Die Umwandlung von auf einem Computer gespeicherten Text in gesprochene Worte.IVR - Unter interactive voice response ist die Benutzung des Telefons zur Interaktion mit einem Computer zu verstehen.VM System - voice mail system; ist ein Nachrichtenaufzeichnungssystem, das u.a. gesprochene Messages wie jede andere Nachricht im Netzwerk verteilt.AA - Der automated attendant führt automatisch die Tätigkeiten einer Person durch, wie Telefonate vermitteln oder Bestellungen entgegennehmen.(truz).Standard Verlagsgesellschaft M.B.H.Document dstan00020011017dsa3004ow"
"10",2001-06-22,"Spracherkennung automatisiert das Call-Center.","Spracherkennung automatisiert das Call-Center.456 words22 June 2001ComputerwocheCPWCHEGerman(c) Computerwoche 2001MÜNCHEN (IDG) - Die Erreichbarkeit eines Call-Centers ist für den Kunden oft ein Ärgernis: Minutenlange Wartezeiten am Telefon sind die Regel. Abhilfe kann der Einsatz eines Spracherkennungsystems bringen.Beim Kundenservice der amerikanischen Fluglinie AirTran Airways mussten Anrufer bisher viel Geduld mitbringen: Um zum Beispiel Verspätungen zu erfragen, wartete ein Kunde durchschnittlich sieben Minuten, bis der Anruf im Call-Center entgegengenommen wurde. Und weitere zweieinhalb Minuten brauchte der Agent, um die Fragen zu beantworten. Seit kurzem wird ein Anruf bei AirTran innerhalb von zwei Sekunden angenommen. Und nach durchschnittlich etwas über einer Minute ist die Frage des Kunden beantwortet. Der Grund: automatische Spracherkennung.Die ""automatic speech recognition"" (ASR) ist aus den Kinderschuhen raus. Zu den ""early adopters"" dieser Technologie gehören insbesondere Fluglinen, die ihren Kundenservice mit dieser Technologie ausrüsten.ASR statt Agent Unter anderem haben AirTran und United Airlines allgemeine Flugauskünfte vom Call-Center-Agenten auf ein ASR-System verlagert, das auf die Stimme des Kunden reagiert. Für die Airlines und besonders die Call-Center ist das eine lohnende Sache, da nicht nur Wartezeiten und Telefonkosten deutlich gesenkt werden, sondern auch das Call-Center-Personal für andere Aufgaben wie Verkaufsinitiativen frei wird.ASR kann zudem einfache Anfragen in anderen Bereichen automatisieren - etwa als Telefonvermittlung, bei der ein Anrufer nur den Namen des gewünschten Gesprächspartners sagen muss, um durchgestellt zu werden. Auch Börsenkurse oder Nachrichten können automatisch über das Telefon durchgegeben werden.Wie bei den meisten IT-Projekten gibt es auch bei ASR keine Lösung ""Off the Shelf"". Im Falle von AirTran waren für die Evaluation, Koordination und Feinabstimmung des Systems acht Monate notwendig, bevor ASR den ersten Anruf beantworten konnte.Eines der größten Probleme sei gewesen, die neue Technologie in die bereits bestehende Infrastruktur zu integrieren, so AirTran-CIO Rocky Wiggins. Legacy-Systeme, unterschiedliche Datenbanken und nicht-standardisierte Plattformen mussten in die neue Struktur eingepasst werden.Feinabstimmung fehlt noch Trotz der langen Entwicklungs-und Anpassungszeit läuft die ASR noch immer nicht ganz fehlerfrei. Das automatische Fluginformationssystem antwortet zwar schnell, die Interaktion zwischen ASR und Anrufer ist aber noch immer nicht so reibungslos wie zwischen zwei Menschen. Pausen oder wiederholte Abfragen unterbrechen manchmal den Dialog. Durch eine bessere Feinabstimmung des Systems soll die Kommunikation zwischen Mensch und Maschine noch flüssiger werden.Für Elizabeth Herrell, Research Director der Giga Information Group, steht der große Durchbruch von ASR nun bevor: ""Zahlreiche ASR-Applikationen werden bis zum nächsten Jahr auf den Markt drängen."" Die Genauigkeit der ASR-Technologie habe sich so weit verbessert, dass die Anwender das System mit natürlicher Sprache steuern können. Wegen der hohen Kosten für ein ASR-Projekt würder, so Herrell, bislang nur sehr große Organisationen in diese Technologie investieren. Für mittlere Unternehmen sieht die Unternehmensforscherin jedoch die Option von externen Voice-Diensten.Document cpwche0020010710dx6m001yf"
"11",2005-08-26,"Im Interview: Erich Prem (BMVIT) zu den Rahmenbedingungen für semantische Systeme in Österreich  ","Im Interview: Erich Prem (BMVIT) zu den Rahmenbedingungen für semantische Systeme in Österreich  Tassilo Pellegrini, Semantic Web School  1226 words26 August 2005Computerwelt OnlineCMPONL20050826German© 2005 Info Technologie Verlag GmbH. All rights reserved. For further information see  http://www.computerwelt.at[http://www.computerwelt.at]Anfang 2005 startete die zweite Ausschreibung Semantic Systems im Technologieförderprogramm FIT-IT des BMVIT. Tassilo Pellegrini sprach mit Dr. Erich Prem über Erfahrungswerte des letzten Calls, den Begriff der radikalen Innovation und thematische Schwerpunkte des neuen Calls.  Anfang 2005 startete die zweite Ausschreibung Semantic Systems im Technologieförderprogramm FIT-IT des BMVIT. Tassilo Pellegrini sprach mit Dr. Erich Prem über Erfahrungswerte des letzten Calls, den Begriff der radikalen Innovation und thematische Schwerpunkte des neuen Calls.  TP: 2004 gab es die erste Ausschreibung von FIT-IT zu Semantic Systems. Wie ist der Call verlaufen?  Prem: Die erste Ausschreibung von FIT-IT Semantic Systems endete am 16. August 2004. Es wurden 27 Projekte mit Gesamtkosten von 10.900.000 EUR eingereicht und Förderungen in der Höhe von 8.100.000 EUR beantragt. Mit diesen beeindruckenden Zahlen startete das IT- Forschungsprogramm des BMVIT ""Semantic Systems"". Die von internationalen Evaluatoren augewählten Projekte decken einen äußerst weiten Bereich von Sprachtechnologie über Ontologien bis hin zu Web-Serviceintegration ab. Die Projekte sind wissenschaftlich hervorragend fundiert und verfügen alle über glaubwürdige Kooperationen mit österreichischen Unternehmen, die hochinnovative neue Dienste auf den Markt bringen werden. Unser Zielzeitraum ist dabei ja 3 bis 8 Jahre time-to-market, also wesentlich längerfristig als andere Förderprogramme in Österreich. Dies wird auch in Zukunft so bleiben, weil wir wollen, dass Unternehmen den Wert mittelfristiger Innovation erkennen. Wir unterstützen sie, indem wir einen Teil des Risikos solcher langfristigen Perspektiven abfangen.  TP: Wie sind die Rahmenbedingungen für Forschung in Österreich zu semantischen Systemen?  Prem: Die Rahmenbedingungen für semantische Systeme sind in Österreich ausgezeichnet. Es beginnt bei hochqualifiziertem Personal für diesen Bereich, reicht über international erfolgreiche Unternehmen bis zu typisch österreichischen Wirtschaftsfeldern, die besonders interessant für Österreich sind. Der gesamte Bereich touristischer Informationssysteme, in dem Österreich traditionell stark ist, eignet sich hervorragend für den Einsatz innovativer semantischer Technologien. Dies reicht von der Unterstützung der Suche nach Diensten im Internet, über Aspekte der internationalen Anpassung von Webinhalten (Lokalisierung) bis zur komplexen Integration von elektronischen Dienstleistungen zu hochwertigen, teilautomatisierten Services. Aber auch in traditionellen Bereich wie der Automobilzulieferindustrie kommen semantische Systeme für Integrationszwecke immer häufiger zum Einsatz. Aus unserer Sicht gilt es jetzt, entscheidende Technologieverbesserungen einzuführen, um auch in den nächsten Jahren international Vorreiter zu sein. Ebenso gilt es, die Sichtbarkeit dieser hohen Qualität and technologischer Leistungsfähigkeit sowohl im In- als auch Ausland - zu verbessern.  TP: Wie bewährt sich der Forschungsstandort Österreich zu Semantic Systems im internationalen Vergleich?  Prem: Österreich befindet sich in einer hervorragenden Ausgangslage: es gibt gleich mehrere Spitzenforschungsinstitute im Bereich semantischer Systeme, aber auch z.B. dem verwandten Gebiet der Artificial Intelligence. Die dort agierenden Personen sind international bekannt und in europäische, aber auch weltweite Netzwerke eingebunden. Dies ist eine notwendige Voraussetzung, um auch längerfristig eine hervorragende Position im Bereich semantischer Systeme sicherzustellen. Zusätzlich sind aber viele der Wissenschafter auch wirtschaftlich aktiv: sie haben entweder eigene Unternehmen in diesem Bereich oder kooperieren eng mit Firmen, die zumeist eigene Werkzeuge für semantische Systeme produzieren.  TP: FIT-IT arbeitet mit der Metapher der radikalen Innovation. Wo sehen Sie diesen Sachverhalt bei Semantic Systems gegeben?  Prem: Die Frage nach der radikalen Innovation hat uns sehr stark in FIT-IT beschäftigt. Das Programm ist mit dem Ziel angetreten, sich deutlich von anderen Förderprogrammen in Österreich abzuheben. Unser Ansatz der radikalen Innovation ist strukturell zu verstehen und richtet sich an Unternehmen, die längerfristiger planen und denken, wo Wissenschaftler gemeinsam mit Unternehmen in Hinblick auf die übernächste Produktgeneration innovativ neue Dienstleistungen und Produkte anbieten bzw. innovative Ansätze ausprobieren.  Bisher gab es in Österreich den FWF und seit neuerem die FFG: letztere ist zuständig für die wirtschaftliche Förderung, erstere für die wissenschaftliche Forschung. Und dazwischen, das wurde mehrmals attestiert, klafft eine Lücke. Dies hat dazu geführt, dass wir die österreichischen Unternehmen sehr häufig nur als inkrementelle Innovatoren sehen, d.h. sie innovieren im internationalen Vergleich gut und viel, aber sie denken nur über einzelne Verbesserungen aber nicht grundsätzlich neue Produkte und Möglichkeiten nach.  Genau diese Lücke versucht FIT-IT zu schließen. Das haben wir am Anfang radikale Innovation genannt. Leider ist das ein Begriff, der weder besonders gut geklärt, noch besonders gut verstanden wird, sodass wir es in letzter Zeit einfach konkreter gemacht haben mit time-to-market von 3 bis 8 Jahre.  TP: FIT-IT verfolgt drei Forschungsschwerpunkte. Es begann im Jahr 2003 mit Embedded Systems, wurde 2004 um Semantic Systems ergänzt und 2005 kommt Systems on Chip dazu. Gibt es einen inhaltlichen Zusammenhang zwischen diesen drei Programmen?  Prem: Die notwendige Voraussetzung für einen Programmschwerpunkt in FIT-IT ist, dass es hervorragende Forscher gibt und dass es Unternehmen gibt, die in diesem Bereich etwas tun. Für ein Land wie Österreich sind das bereits relativ schwierige Vorgaben, da ein Förderprogramm auch eine gewisse kritische Masse braucht. Durch die Aufstockung der FIT-IT Mittel durch den Rat für Technologieentwicklung haben wir jetzt mit ca. 12 Millionen EURO das notwendige Volumen, um die drei Programmschwerpunkte entsprechend finanziell ausstatten zu können. Im übergeordneten Kontext dieser Programmlinien ist leider der suboptimale Eindruck entstanden, dass FIT-IT das österreichische IT-Förderprogramm ist, einfach aus dem Grund, weil es kein anderes Förderprogramm mit diesem IT-fokussierten Anspruch gibt. Die ausgesuchten drei Bereiche sind natürlich nicht die gesamte IT, auch sind wir nicht das einzige IT-Förderprogramm in Österreich. FIT-IT möchte aber nicht – und das ist ein ganz wichtiger Punkt – in allen IT-Bereichen irgendwelche Maßnahmen setzen, da wir glauben, dass eine entscheidende Stärke unseres Programms die Fokussierung ist. Diese ist zwar manchmal schmerzhaft, aber garantiert, dass man in Österreich auf eine solide Basis trifft, und man davon ausgehen kann, dass es auch noch in mindestens zehn Jahren interessante Aktivitäten gibt.  TP: Haben Sie dazu ein Beispiel?  Prem: Nehmen wir etwa ein FIT-IT gefördertes Projekt zum Thema Speech Recognition. Dieses ist im Bereich „Embedded Systems“ angesiedelt, hat aber starke Verweise in den Bereich „Semantic Systems“. Hier haben wir ein Projekt, dessen Ergebnis irgendwann einmal in mobilen Geräten zur Anwendung kommen kann, das aber von der inhaltlichen Ausrichtung sehr viel mit Semantik zu tun hat. Bei fast allen mobilen Geräten stellt sich die Frage des Anwendungskontextes, der Situation des Benutzers und die Erleichterung der Bedienung, die die Bedeutung einer Situation einbindet.  Zwischen diesen beiden Bereichen sehen wir eine ganz enge Verbindung, und es wäre auch nahe liegend, wenn man das ganze noch miniaturisiert und auf einen Chip bringt. Und da ist man bereits bei System on Chip. Das wäre ein Anwendungsbeispiel.  Es gibt aber durchaus tiefere Gebiete, die in die Richtung gehen über Semantik z.B. Entwicklungsprozesse beim Entwurf von System on Chip zu berücksichtigen. Hier passieren die Verbindungen mehr auf einer methodischen Ebene und das ist auch sehr spannend für uns.  *) Erich Prem ist DI der Informatik und Doktor der technischen Wissenschaften. Er war als Wissenschafter am Österreichischen Forschungsinstitut für Artificial Intelligence - ÖFAI und am Massachusetts Institute of Technology - MIT (USA) tätig und ist Autor von mehr als 40 wissenschaftlichen Publikationen. Er hat an mehreren internationalen Forschungsprojekten als Wissenschafter oder Projektmanager mitgewirkt und war Leiter der Abteilung Informations- und Kommunikationstechnologien am Büro für Internationale Forschungs- und Technologiekooperationen - BIT und österreichischer Experte im Information Society Technologies Committee der EU. Er ist als Evaluator für IT-Forschungsprojekte für die Europäische Kommission tätig und Programm-Manager für das BMVIT Programm FIT-IT Embedded Systems.  Info Technologie Verlag GmbHDocument CMPONL0020050826e18q0002t"
"12",2011-11-21,"iPhone-Hack: Siri steuert Thermostat; Proxy-Server erweitert Möglichkeiten des Sprachassistenten","HightechiPhone-Hack: Siri steuert Thermostat; Proxy-Server erweitert Möglichkeiten des SprachassistentenGeorg Pichler, pressetext.redaktion   391 words21 November 201110:30PressetextPRESSEGerman© 2011 Pressetext – News and Press Service for opinion leaders. All rights reserved. For further information see   http://www.pressetext.de[http://www.pressetext.de]St. Louis/Cupertino (pte010/21.11.2011/10:30) - Der Sprachassistent des iPhone 4S, Siri, könnte in Zukunft die Inneneinrichtung von Wohnungen steuern. Ein Entwickler, nur bekannt unter dem Twitter-Pseudonym @plamoni http://twitter.com/plamoni[http://twitter.com/plamoni] , hat einen Proxy-Server entwickelt, der die Funktionalität des Steuerungstools erweitert. In einem Demonstrationsvideo übernimmt er die Kontrolle über seinen Thermostat.St. Louis/Cupertino (pte010/21.11.2011/10:30) - Der Sprachassistent des iPhone 4S, Siri, könnte in Zukunft die Inneneinrichtung von Wohnungen steuern. Ein Entwickler, nur bekannt unter dem Twitter-Pseudonym @plamoni http://twitter.com/plamoni[http://twitter.com/plamoni] , hat einen Proxy-Server entwickelt, der die Funktionalität des Steuerungstools erweitert. In einem Demonstrationsvideo übernimmt er die Kontrolle über seinen Thermostat.Telefon ändert Temperatur via WLANErst vor einigen Tagen ist es den Entwicklern der Firma Applidium http://applidium.com[http://applidium.com] geglückt, das proprietäre Protokoll von Siri via Reverse Engineering zu knacken. Die Bastler legten ihre Entdeckung offen und stellten eine komplette Anleitung und Dateien ins Web.Die so gelegte, technische Basis hat sich plamoni zu Nutze gemacht und einen Proxy-Server entwickelt, der Anfragen von Siri auf dem Rückweg in einem WLAN umleiten kann. Konkret gelingt ihm das in einem Video mit seinem netzwerkfähigen Temperaturregler. Über sein iPhone 4S und den Sprachassistenten kann er die aktuelle Raumtemperatur und die Einstellung der Heizung abfragen. Ist es zu kalt oder warm, so lässt sich die Konfiguration per Fernzugriff regeln. Derzeit dürfte die Steuerung auf diesem Wege auf Geräte im gleichen Drahtlosnetzwerk beschränkt sein.Offener Quellcode als Chance für TüftlerDen Quellcode seiner Software, entwickelt mit der freien Programmiersprache Ruby, hat der findige Programmierer online gestellt. Damit steht Experimenten mit weiteren Features nichts mehr im Wege.Dank der Entschlüsselung des Siri-Protokolls kann das mächtige Speech-Recognition-Tool theoretisch auch von älteren iPhone-Modellen, auf iPads oder auch via Android genutzt werden, sofern jemand entsprechende Anwendungen schreibt. Dies ist möglich, da die Sprachauswertung online erfolgt.Der Proxy-Server ist aktuell nur mit dem iPhone 4S kompatibel, ein Jailbreak ist für die Nutzung nicht erforderlich.Quellcode des Siri-Proxy:https://github.com/plamoni/SiriProxy[https://github.com/plamoni/SiriProxy](Ende)Aussender: pressetext.redaktionAnsprechpartner: Georg PichlerTel.: +43-1-81140-303E-Mail: pichler@pressetext.atWebsite: www.pressetext.com[http://www.pressetext.com]Pressetext Nachrichtenagentur GmbHDocument PRESSE0020111121e7bl000gp"
"13",2000-03-30,"Ad hoc-Service - Telegate AG dt./engl.","Ad hoc-Service - Telegate AG dt./engl.1270 words30 March 200011:32Reuters Direkt ServiceREUTDSGerman(c) 2000 Reuters LimitedAd hoc-Service: Telegate AG  dt./engl.Ad hoc-Mitteilung verarbeitet und übermittelt durch die DGAP.Für den Inhalt der Mitteilung ist der Emittent verantwortlich.------------------------------------------------------------------------------Die telegate AG steigt bei PhoneCom ein:Ausbau des Service-Portals wird weiter vorangetriebenMünchen, 30. März 2000. Die telegate AG, München, hat einen neuen Partnergewonnen: Die Nr. 2 auf dem deutschen Telefonauskunftsmarkt hat sich mit35 Prozent an der Münchner Telemarketingfirma PhoneCom KommunikationsDiensteGmbH beteiligt. PhoneCom ist mit der technologischen Vernetzung von CallCenterund Sprachcomputer einer der führenden Anbieter in diesem Segment.""Unsere Aktivitäten im Call Center-Bereich werden durch die bishereinzigartigeLösung von PhoneCom perfekt ergänzt"", sagt telegate-Technikvorstand PeterWünsch. ""Ganz neue Services sind möglich, speziell für den WachstumsmarktSpieleund Freizeit. Zudem spielt die Verbindung von Sprache und Daten für unsereinternationalen Servicedienstleistungen und den Aufbau unseres Service-Portalsim Internet eine bedeutende Rolle.""PhoneCom wurde 1995 gegründet und hat sich innerhalb kurzer Zeit mitcomputergestützten Telefondiensten zu einem der führenden Dienstleister imCall Center-Markt entwickelt. Interaktive Spracherkennung, Voicemail sowieInbound- und Outbound-Services gehören zum Kernangebot von PhoneCom. Im Jahr1999 erzielten rund 300 Mitarbeiter einen Umsatz von rund 9,2 Millionen Mark.Zu den Kunden gehören im staatlichen Lotteriebereich die SKL und die NKL,außerdem Verlage wie der Bauer-Verlag und Medienunternehmen wie derBayerischeRundfunk.Über den Preis vereinbarten beide Seiten Stillschweigen. ""Der Erlös fließtvollständig in die Weiterentwicklung des Unternehmens"", sagt PhoneCom-Geschäftsfuhrer Gerrit Neuhaus. ""Mit einem starken Partner wie telegatekönnenwir unsere Aktivitäten wesentlich breiter aufstellen, größere Aufträgeannehmenund Kapazitäten innerhalb der Call Center verlagern. Insbesondere im Phone-Commerce wie etwa bei Auktionen am Telefon sehen wir enorme Wachstumschancenfürdie Zukunft.""Die Partnerschaft mit telegate sieht vor, dass PhoneCom ihre hochwertigenInbound- und Outbound-Lösungen als ""Maßanzüge"" für telegate weiter entwickeltund Lösungen für große Projekte bietet. Die enge Zusammenarbeit der beidenUnternehmen soll durch eine ständige Arbeitsgruppe gewährleistet werden, dievorallem Synergie-Potenziale erschließen soll. Peter Wünsch: ""Damit werden wirunsere Wettbewerbsposition als internationaler Service-Dienstleister weiterausbauen."" So werden Sprachcomputer beispielsweise für die Spracherkennung iminternationalen Geschäft zum Einsatz kommen. ""Auf die persönliche Auskunftunddie freundliche Stimme muss kein Kunde verzichten. Er kommt jedoch schnellerans Ziel, indem er etwa bei Gewinnspielaktionen oder Verlosungen im Fernsehenoder im Internet direkt an die dafür geschulte Call Center-Crewweitergeleitetwird."" Außerdem planen telegate und Phone-Com, gemeinsam nationale undinternationale Telemarketing-Projekte zu realisieren.Die PhoneCom KommunikationsDienste GmbH mit Sitz in München wurde 1995 mitvierMitarbeitern gegründet und verfügt über rund 600 Telefonleitungen fürcomputergestützte Telefondienste. Heute beschäftigt das Unternehmen 520Mitarbeiter für ein Kontingent von 810 Telefonleitungen. Zu denKernkompetenzengehören Outbound- und Inbound-Dienste, Voicemail, interaktive Spracherkennungund Faxdienste jeder Art.Die Münchener telegate wurde 1996 gegründet und ist seit April 1999 am NeuenMarkt der Frankfurter Wertpapierbörse notiert. Das Unternehmen konnte 1999einen Umsatz von 175,5 Millionen Mark verzeichnen. Zum Angebotsportfoliogehörteine Vielzahl von Leistungen wie Telefonnummern-Auskunft für das Festnetz undalle deutschen Mobilfunknetze, Weitervermittlung an rund 4.500Service-Hotlines,Telefon- und Adress-Auskünfte per Post und Fax. Das im Januar 2000 gegründeteTochterunternehmen 11880.com bietet ein Online-Branchenverzeichnis. WeitereInternet-Serviceleistungen sind in Planung. Die telegate AG beschäftigt rund2.500 Mitarbeiter.Finanzinformationen:				    Presseinformationen:telegate AG Wolfgang Brand			    Hoschke & ConsortenFraunhofer Str. 20					Anja Meyer, Andreas Hoschke82152 München-Martinsried			     Deichstraße 29Tel.: 089/8954-1120				     20459 HamburgFax:  089/8954-1810				     Tel.: 040 / 36 11 46e-mail: wb@telegate.de				  Fax:  040 / 36 11 44								e-mail: a.meyer@hoschke.de-------------------------------------------------------------------------------telegate AG takes a holding in PhoneCom:Expansion of the service portal to be speeded upMunich, 31 March 2000. telegate AG, Munich, has gained a new partner:the number 2 in the German telephone directory inquiries market has taken a35percent holding in PhoneCom KommunikationsDienste GmbH, the telephonemarketingcompany based in Munich. With the technological integration of call centresandvoice computers, PhoneCom is one of the leading suppliers in this segment.""PhoneCom's unique solution perfectly complements our activities in thecall-centre field"", says telegate's technical director, Peter Wünsch.""Completely new services are possible, especially for the growth markets ofgames and leisure. Furthermore, the combination of voice and data has animportant role to play for our international services and the establishmentofour internet service portal.""PhoneCom was founded in 1995 and developed into one of the leading serviceenterprises in the call-centre market within a short space of time.Interactivevoice recognition, voicemail and inbound and outbound services rank among thecompany's core areas of competence. In 1999, the approximately 300 PhoneComemployees achieved total sales revenues of approx. DM 9.2 million. In thestatelottery sector, PhoneCom's customers include SKL and NKL Among the company'sother customers are the Bauer publishing company, media enterprises, such asBayerischer Rundfunk.Both sides have agreed full secrecy with regard to the price agreed.""The proceeds will flow fully into the further development of the company"",saysPhoneCom's general manager, Gerrit Neuhaus. ""With a powerful partner liketelegate, we can cover a much wider spectrum with our activities, acceptlargerorders and transfer capacities within the call centre. We see enormousopportunities for growth in the future, especially in the Phone-Commercesegment, for instance, auctions by telephoned.""The partnership with telegate calls for PhoneCom to further develop itshigh-grade inbound and outbound solutions as 'customised' services fortelegate,and to provide solutions for major projects. Ensuring a close workingrelationship between the two companies will be a working group that willconcentrate particularly on discovering potential synergies.Peter Wünsch: ""This will help us continue to improve our competitive positionasan international service supplier."" Thus, voice computers will be used forspeech recognition in  the international sector. ""Customers will not have togowithout personal information and a friendly voice. However, they will be putthrough more quickly, for instance, during competitions on television, whentheywill enjoy faster connections to a specially trained crew."" Additionally,telegate and PhoneCom are planning to undertake joint national andinternationaltelemarketing projects.Founded in 1995, PhoneCom Kommunikationsdienste GmbH, Munich began with astaffof four and around 600 telephone lines for computer-aided telephone services.Today, the company employs 520 employees for a contingent of 810 telephonelines. The company's core competences include outbound and inbound services,voicemail, interactive voice recognition and all kinds of fax service.The Munich-based telegate company was founded in 1996 and has been listed ontheNew Market of Frankfurt Stock Exchange since April 1999. Last year, thecompanyrecorded total revenues in excess of DM 175.5 million. The range of servicesoffered includes numerous facilities, such as directory inquiries for thefixed-line network and all German mobile-telephone networks, call forwardingtoaround 4.500 service hotlines and telephone and address inquiries by postand fax. 11880.com, the telegate subsidiary founded in January 2000, providesanonline directory service. Additional internet services are being planned.telegate AG employs approximately 2,500 people.Financial Information:				  Press information:telegate AG						 Hoschke & ConsortenWolfgang Brand					    Anja Meyer, Andreas HoschkeFraunhofer Str. 20					Deichstrasse 29D-82152 Munich-Martinsried			    D-20459 HamburgTel.: +49 89 8954-1120				  Tel: +49 40 361146Fax:  +49 89 8954-1810				  Fax: +49 40 361144E-mail: wb@telegate.de				  E-mail: a.meyer@hoschke.deEnde der Mitteilung.(c) Reuters Limited 2000.Document reutds0020010816dw3u00190"
"14",1999-03-17,"Computer, bitte zum Diktat.","Computer, bitte zum Diktat.666 words17 March 1999Süddeutsche ZeitungSDDZGerman(c) 1999 Süddeutsche ZeitungTrotz moderner Technik verstehen Spracherkennungssysteme immer noch Nebengeräusche als WörterGlaubt man den Ankündigungen zur Cebit, hat die Technik der maschinellen Spracherkennung wieder einen neuen Stand erreicht. Ein ganzes Arsenal von Diktiergeräten, Anrufbeantwortern und kleinen, doch leistungsfähigen Speicherchips steht bereit, die Technik unter das Volk zu bringen. Der Computer kommt in den verschiedenen Szenarien kaum vor. Denn die Technik entwickelt sich dahin, den Manager wie gewohnt diktieren, den Anrufbeantworter wie bisher besprechen zu lassen. Diese Aufnahmen werden digital gespeichert und können dann sofort vom Computer und einer Fachkraft bearbeitet werden.Noch in den letzten Jahren waren Spracherkennungen schwer zu bedienen: Sie funktionierten nach der sogenannten ""diskreten Spracherkennung"". Bei dieser Technik muß zwischen jedem Wort eine Pause liegen, damit der Computer erkennen kann, wann ein Wort zu Ende ist. Heute ist dieses Kunst-Stottern (auf das trainierte Zeitgenossen nach wie vor schwören) durch die ""kontinuierliche Spracherkennung"" abgelöst. Bei ihr spricht man wie in ein Diktiergerät, von gesprochenen Start-und Stop-Befehlen des ""Bandes"" einmal abgesehen. Der Fortschritt verdankt sich vor allem der Weiterentwicklung der Rechnertechnik, denn Spracherkennung braucht wirklich leistungsfähige Systeme: Unter einem 300-MHz-Pentium und 48-MByte-Arbeitsspeicher fallen die kontinuierlich mitrechnenden Spracherkenner unweigerlich in den alten Stottermodus zurück. Büroausstatter wie Olympus und Sony haben entsprechende Diktiergeräte im Angebot, bei denen das herkömmliche Bandlaufwerk durch eine Speicherkarte ersetzt ist. Olympus setzt bei seinem Angebot auf IBMs ViaVoice 98. Die Konkurrenz ist Naturally Speaking der amerikanischen Dragon Systems und Voice Xpress der belgischen Firma Lern-out & Hauspie. Diese drei Firmen teilen den gesamten Markt praktisch unter sich auf, da alle anderen Anbieter mit Sublizenzen der großen drei arbeiten.So ist das System von Philips fast schon ein Newcomer, da es lange Zeit nur für spezielle Anwendungen wie etwa Arztberichte im Einsatz war. Abgespeckte Versionen aller Sprachspezialisten finden gerade Eingang in die intelligenten Bordsysteme der Zukunftsautos oder in die Vermittlungsstellen und Sprachboxen der Telephonsysteme, die aus einem Anruf eine E-Mail machen. ""1999 ist das Jahr von Telephon und Spracherkennung als neue Benutzerschnittstelle zu E-Business und E-Commerce"" radebrecht eine Presserklärung der IBM. Deshalb widmet sie heuer dem Publikum auf der Cebit gleich einen kompletten Stand unter dem Motto ""That's @Speech Recognition"".In der deutschen Sprache sei das Motto nicht ""klingend"", so die Auskunft der Sprachspezialisten. Wie gut die Spracherkenner sind, darüber gehen die Meinungen weit auseinander. Geworben wird mit einer Trefferquote von 100 Prozent, doch sie ist im Alltag illusionär. Je disziplinierter eine Person das herkömmliche Diktieren beherrscht, desto besser fallen die Resultate aus. Erfahrene Diktierer kommen auf 95 Prozent und das heißt immerhin noch: Jedes zwanzigste Wort wird falsch erkannt. Einigkeit besteht indes darin, daß das Training durch den Sprechenden die allerwichtigste Komponente der Sprachverarbeitung ist. Das Versprechen der Anbieter, bereits in fünf Minuten akzeptable Ergebnisse zu erzielen, ist pure Bauernfängerei. Jeder Spracherkenner muß für den Sprechenden ein sogenanntes ""Sprachmodell"" erzeugen. Zwischen 30 Minuten und zwei Stunden werden dafür benötigt, anschließend rechnet auch der schnellste Rechner eine Stunde lang das Sprachmodell zusammen. Das einmal gefundene Sprachmodell muß vom Sprechenden kontinuierlich weitergepflegt werden. Es kann aber auch verhunzt werden: Andere Personen können nicht ""mal eben"" mit der Software arbeiten. Ähnliches gilt für Sprecherzustände, die die Stimmlage verändern. Alle Programme warnen davor, die Spracherkennung im Falle einer Erkältung einzusetzen. Auch die Akustik eines Raumes hat einen Einfluß auf die Spracherkennung: die Software eignet sich wohl nur für Steuerberater, die in einem einzigen Raum ihre Diktate aufnehmen. Zudem darf dieser Raum über keine extreme Geräuschkulisse verfügen: Denn die Erkenner versuchen, auch das Türenschlagen oder Telephonklingeln als Wort zu interpretieren.Diese gravierende Einschränkung für Großraumbüros soll indes bald fallen: IBM entwickelt zusammen mit dem Konkurrenten Lernout & Hauspie billige ""intelligente Mikrophone"", die die Nebengeräusche wegfiltern. Gegenwärtig kosten diese softwaregesteuerten Mikrophone um 4000 Mark und sind damit für den Normalsprecher viel zu teuer.Der ""Heilige Gral"" der Sprachforscher, die absolut nahtlose Integration der Spracheingabe in die tägliche Arbeit, liegt eben immer ein Stückchen in der Zukunft. DETLEF BORCHERS.(c) 1999 Süddeutsche Zeitung.Document sddz000020010910dv3h01jmx"
"15",1993-01-25,"Der Computer schreibt mit   ","PerspektivenForschung und TechnikDer Computer schreibt mit   120 words25 January 1993FocusFOCUSGerman© 1993 FOCUS – DAS MODERNE NACHRICHTENMAGAZIN, FOCUS MAGAZIN VERLAG GMBH,   www.focus.de[http://www.focus.de]Am Klinikum der Technischen Hochschule Aachen sprechen Mediziner ihre Befunde direkt in den Computer. Der Rechner hört mit und liefert auf Knopfdruck einen Ausdruck des Textes. ""SPRING"" (""Speech Recognition in German"") nennt sich das System, das in Aachen erprobt wird, und mittlerweile 12000 Wörter mit einer Trefferquote von 95% erkennt. Einen ersten Praxistest bestand das System neben dem des Aachener Klinikums auch am Obergericht in Zürich.   ""Speech Server Series-System"" nennt Hersteller IBM den komplett ausgestatteten Arbeitsplatz. Das rund 30 000 Mark teure System versteht derzeit wahlweise Englisch, Italienisch, Französisch oder Deutsch. Alleine in Europa hat die Sprachverarbeitung nach IBM-Schätzungen ein Marktpotential von rund 1,5 Milliarden Mark.   Focus Magazin Verlag GmbHDocument FOCUS00020071015dp1p0009a"
"16",2001-01-24,"INTERVOICE-BRITE ÜBERNIMMT SOFTWAREVERSION VON SPEECHWORKS.","INTERVOICE-BRITE ÜBERNIMMT SOFTWAREVERSION VON SPEECHWORKS.932 words24 January 200107:56Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS""InterVoice-Brite übernimmt die neueste Softwareversion von SpeechWorks für die automatisierte Spracherkennung (ASR). Dallas (ots-PRNewswire) - InterVoice-Brite setzt sein Engagement für die Integration der neuesten ASR-Funktionen fort.InterVoice-Brite (Nasdaq: INTV) gab bekannt, dass die aktuellste Version der Automated Speech Recognition (ASR)-Software von SpeechWorks nunmehr für den Einsatz in den sprachfähigen Systemen von InterVoice-Brite zur Verfügung steht. Die Integrationsbemühungen demonstrieren das langfristige Engagement von InterVoice-Brite im Bereitstellen der aktuellsten und fortschrittlichsten Produkte, die bei SpeechWorks (Nasdaq: SPWX) erhältlich sind, einem führenden Anbieter von auf Telefonie basierenden Sprachlösungen. Durch die Integration von SpeechWorks 6.5 können die Kunden von InterVoice-Brite nunmehr die neuen werterhöhenden Services einsetzen, die in einem einzigen System US-Namen erkennen und mehrere Sprachen unterstützen.In der neuen Version sind die vorgefertigten Bausteine von SpeechWorks, die als DialogModules(TM) bekannt sind, in Sprach-Toolkits integriert, die in InVision, dem Tool von InterVoice-Brite zur Anwendungsentwicklung, zu finden sind. Die aktuelle Version von SpeechWorks bietet die Möglichkeit, problemlos neue DialogModules zu laden, wenn sie verfügbar sind, ohne auf eine Aktualisierung der Version warten zu müssen. Die Fähigkeit, zwei oder mehr Sprachen in einem einzigen System gleichzeitig zu unterstützen, versetzt die Entwickler in die Lage, Anwendungen mit Selbstauswahl in mehreren Sprachen zu erstellen. Dies ist ein wichtiges Hilfsmittel für die Unternehmen, die komplizierte, globale, automatisierte Anwendungen mit Selbstauswahl entwickeln. Der Anrufer kann auch auf Eingabeaufforderungen in seinem eigenen Dialekt antworten - abhängig vom geographischen Standort und der regionalen Bevölkerungsstatistik.Die neue Namenserkennungsfunktion ermöglicht uneingeschränkte Erkennung der Mehrheit der Namen in den Vereinigten Staaten auf der Basis der Daten aus der US-amerikanischen Volkszählung. Diese moderne Funktion wird diverse neue Anwendungen ermöglichen, wie zum Beispiel Leistungen in der Gesundheitsversicherung, die Bestätigung von Reisereservierungen und Anfrageanwendungen, bei denen der Anrufer aufgefordert wird, seinen Namen und seine Rufnummer zwecks Rückruf zu hinterlassen.""Die neuesten Fähigkeiten, die die Software von SpeechWorks auf der Plattform von InterVoice-Brite bietet, erweitern die Möglichkeiten für moderne Sprachanwendungen in Bereichen, die zuvor nicht automatisiert werden konnten,"" sagte Bob Ritchey, Executive Vice President und General Manager der Enterprise Solutions Division von InterVoice-Brite. ""In gleicher Weise werden es uns die erweiterten Fähigkeiten für mehrere Sprachen ermöglichen, neuen globalen Märkten neue Anwendungen und immer mehr Unternehmen weltweit komfortable und kostengünstige sprachfähige Anwendungen zu bieten.""""Wir freuen uns, dem großen Kundenstamm von InterVoice-Brite weiterhin erweiterte Sprachlösungen zur Verfügung stellen zu können,"" sagte Steve Chambers, Vice President für Marketing von SpeechWorks. ""Unsere Geschäftsbeziehung hat zu interessanten Sprachservices bei Unternehmen wie First Union National Bank, Continental Airlines, Thomas Cook Travel und weiteren Firmen geführt, die auf hervorragenden Kundenservice besonderen Wert legen. Wir freuen uns darauf, unsere Geschäftsbeziehung auszubauen, während die wirtschaftlichen Vorteile des Einsatzes von sprachfähigen Anwendungen von den großen Unternehmen in der ganzen Welt erkannt werden.""Das System von InterVoice-Brite ermöglicht die schnelle Entwicklung der modernsten natürlichen Sprachtelefonieanwendungen, die derzeit zur Verfügung stehen. Das System beinhaltet ein Vokabular von mehr als 100.000 Wörtern und ist mit weiteren Tools ausgestattet, darunter die maßgeschneiderte Erweiterung des Vokabulars, branchenspezifische Grammatikbibliotheken und eine Selbstabstimmungsfunktion. Zu den derzeit unterstützten Sprachen gehören Englisch (die US-amerikanische, britische, australische und die in Singapur gesprochene Version), Spanisch (die lateinamerikanische und die US-amerikanische Version), Französisch (die europäische und die kanadische Variante), Chinesisch (Mandarin) und Deutsch.Diese Pressemeldung enthält möglicherweise zukunftsbezogene Aussagen, die Risiken und Unwägbarkeiten unterliegen, die in den Unterlagen der Firma bei der Securities and Exchange Commission (US-Börsenaufsicht) beschrieben sind. Derartige Risiken und Unwägbarkeiten können dazu führen, dass die tatsächlichen Ergebnisse erheblich von den derzeitigen Erwartungen abweichen.InterVoice-Brite Mit annähernd 19.000 an Netzwerk-Service-Provider, Finanzinstitute und Unternehmen in über 70 Ländern ausgelieferten Systemen ist InterVoice-Brite auf dem Gebiet der sprachgesteuerten, interaktiven Informationslösungen technologisch führend und ein führender Application Service Provider (ASP) für die Kommunikation und den E-Commerce. Zu den netzwerkbasierten erweiterten Serviceangeboten gehören Prepaid-Calling-Services, Unified Messaging und intelligente Netzwerklösungen. Zu den Call-Automation-Lösungen zählen interaktive Voice-Response-Systeme, Produkte für das Management von Call-Centern und Portale zur Spracherkennung. Weitere Informationen erhalten Sie auf der Website:www.intervoice-brite.com[http://www.intervoice-brite.com/]SpeechWorks International Inc. Weltweit verlassen sich die Unternehmen auf SpeechWorks, um ihre Anrufer zu erfreuen und ihnen am Telefon ein neues Serviceniveau zu bieten. Die Sprachlösungen von SpeechWorks, darunter das revolutionäre SpeechSite(TM)-Produkt und die Speechify-Maschine, die Text in Sprache umwandelt, ergänzen das Selbstauswahlmodell für den E-Business und ermöglichen es, dass der Kunde seine Gespräche wunschgemäß weiterleiten, automatisch Informationen erhalten und Transaktionen abschließen kann - einfach indem er irgendwo irgendein Telefon benutzt.Zu den Kunden von SpeechWorks gehören einige der anspruchsvollsten und innovativsten Unternehmen im Kundenservice wie America Online, Amtrak, Continental Airlines, E*Trade, FedEx, Fidelity Investments Institutional Brokerage Group, United Airlines und Yahoo!.SpeechWorks hat den Hauptsitz in Boston, Massachusetts, und Niederlassungen in der ganzen Welt.SpeechWorks, Speechify, SpeechWorks Here, DialogModules und SpeechSite sind Warenzeichen oder eingetragene Warenzeichen von SpeechWorks International Inc. in den Vereinigten Staaten und in anderen Ländern. Alle anderen Warenzeichen stehen im Eigentum der entsprechenden Inhaber.ots Originaltext: InterVoice-Brite Inc. Im Internet recherchierbar:http://recherche.newsaktuell.de[http://recherche.newsaktuell.de]Rückfragen bitte an: Rob-Roy J. Graham, Chief Financial Officer, Tel.: +1 972-454-8712, oder Carol Wingard, Vice President, Marketing, Tel.: +1 972-454-8250 oder cwingard@intervoice-brite.com, beide von InterVoice-Brite Inc.; oder Shannon Murphy, Account Manager von M/C/C, Tel.: +1 972-480-8383 oder shannon - murphy@mccom.com, for InterVoice-Brite Inc.Website:http://www.intervoice-brite.com[http://www.intervoice-brite.com]*** OTS-ORIGINALTEXT UNTER AUSSCHLIESSLICHER INHALTLICHER		 VERANTWORTUNG DES AUSSENDERS ***OTS023    2001-01-24/07:54.Document aupag00020010709dx1o00v9y"
"17",2014-12-20,"PRESSEMITTEILUNG/MYNEWSDESK Phonak Roger: Besser Hören als Menschen ohne Hörverlust (mit Bildern)","PRESSEMITTEILUNG/MYNEWSDESK Phonak Roger: Besser Hören als Menschen ohne Hörverlust (mit Bildern)690 words12 June 201412:30mynewsdeskMYNEWDGermanCopyright 2014. mynewsdesk   (Mynewsdesk) Fellbach, Deutschland Kann ein Hörgeräteträger besser hören als ein normal Hörender? Ein stylishes Richtmikrofon, das aus dem Arsenal der James Bond-Gadgets stammen könnte, macht es möglich: Roger, der neue drahtlose Standard von Phonak, beendet die Hörschwierigkeiten in Situationen mit hoher Geräuschkulisse und über Distanzen, indem er Stimmen direkt auf die Hörgeräte überträgt. Die neue funkbasierte Technik übertrifft vergleichbare Systeme um ganze 62%*. Roger revolutioniert so nicht nur das Verstehen in überfüllten Restaurants oder Meetings, sondern verschafft Menschen mit Hörverlust sogar einen Vorteil gegenüber normal Hörenden.Gespräche in vollen Restaurants, Smalltalk auf einer Party bei laufender Musik oder der Präsentation eines Kollegen am anderen Ende des Raums folgen: In solchen Situationen kommen selbst modernste Hörgeräte an ihre Grenzen. Eine Welt ohne Barrieren beim Hören zu schaffen: Dieses Ziel motiviert uns jeden Tag, sagt Maarten Barmentlo, Group Vice President Marketing bei Phonak. Roger, unser neuer drahtloser Standard, bringt uns diesem Ziel einen großen Schritt näher und vor allem auch die Menschen mit Hörverlust, die auf unsere Technologie angewiesen sind.Roger: stylish, unauffällig und zuverlässigDie digitale drahtlose Roger-Technologie würde selbst Q, den genialen Erfinder in den James Bond Filmen, faszinieren: Optisch unauffällig, elegant und unkompliziert in der Bedienung sorgt sie für eine klare Übertragung auch über Distanz und in lauter Umgebung. Durch die Verwendung einer einzigartigen und patentierten Technik im Ansteuern freier Frequenzen findet Roger immer automatisch einen Kanal für die Übertragung. Dieser intelligente Gebrauch der digitalen drahtlosen Signale, zusammen mit einer erweiterten Frequenzbandbreite, sorgen für die deutlichste und derzeit bestmögliche Übermittlung auf die Hörgeräte. Roger übertrifft andere vergleichbare Systeme um 62%*. Der faszinierende Effekt: Menschen mit drahtlosen Phonak Hörgeräten und Träger von Cochlea-Implantaten der Schwesterfirma Advanced Bionics verstehen in schwierigen Hörsituationen besser als Menschen ohne Hörverlust.Neben dem filzstiftgroßen Roger Pen gibt es das kompakte Roger Clip-On Mic, das besonders für persönliche Gespräche geeignet ist. Die beiden Geräte sind einfacher zu bedienen als vergleichbare Funksysteme. Ausgestattet mit vollautomatischen Mikrofoneinstellungen und einem Bluetooth-Anschluss für das Handy, verbinden und konfigurieren sie sich selbst mit nur einem Tastendruck. Anwender können den Roger Pen einfach auf den Tisch legen, in der Hand halten oder einem Sprecher übergeben. Um den Rest kümmert sich die Roger-Technologie und sorgt für ein störungsfreies Verstehen selbst in lauten Umgebungen oder bei einer Distanz von bis zu 20 Metern Situationen, in denen auch normal Hörende zu kämpfen haben.Da der digitale Standard in jedem Land der gleiche ist, können Anwender Roger auch auf Reisen nutzen.Das Roger Clip-On Mic ist in silber, der Roger Pen ist in den Farben silber, blau und rot erhältlich.Mehr Informationen zur Phonak Roger unter: http://tinyurl.com/lru2nsv[http://tinyurl.com/lru2nsv]* Professor Thibodeau, Linda, PhD (2014), Comparison of speech recognition with adaptive digital and FM wireless technology by listeners who use hearing aids, University of Texas, Dallas, USA, The American Journal of Audiology (in press)Shortlink zu dieser Pressemitteilung: http://shortpr.com/rri6rq[http://shortpr.com/rri6rq]Permanentlink zu dieser Pressemitteilung: http://www.themenportal.de/gesundheit/phonak-roger-besser-hoeren-als[http://www.themenportal.de/gesundheit/phonak-roger-besser-hoeren-als]- menschen-ohne-hoerverlust-31976=== Pressekontakt ===Herr Florian FagnerBSKOM GmbH Ickstattstraße 11a 80469 München DeutschlandEMail: fagner@bskom.de Website: www.bskom.de[http://www.bskom.de] Telefon: +49 89 13 95 78 27 16=== Über Phonak ===Phonak, Mitglied der Sonova Gruppe, mit Hauptsitz in Stäfa, Schweiz, entwickelt, produziert und vertreibt seit mehr als 60 Jahren technologisch führende Hör- und Funksysteme. Dabei kombiniert Phonak die profunde Kenntnis in Hörtechnologie und Akustik mit einer intensiven Zusammenarbeit mit Hörgeräteakustikern, um Hörvermögen und Sprachverstehen von Menschen mit Hörminderung zu verbessern und somit ihre Lebensqualität zu erhöhen.Phonak bietet eine vollständige Produktpalette an digitalen Hör- und ergänzenden Funklösungen. Mit weltweiter Präsenz treibt Phonak Innovationen voran und setzt neue Maßstäbe in Miniaturisierung und Leistung.Herr Jan-Christian FrossPhonak GmbH Max-Eyth-Straße 20 70736 Fellbach DeutschlandEMail: jan.fross@phonak.com Website: www.phonak.de[http://www.phonak.de] Telefon: +49 711 510 70 335=== Phonak Roger Pen (Bild) ===Shortlink: http://shortpr.com/0xg37u[http://shortpr.com/0xg37u]Permanentlink: http://www.themenportal.de/bilder/phonak-roger-pen[http://www.themenportal.de/bilder/phonak-roger-pen](Dies ist eine über Mynewsdesk verbreitete Pressemitteilung. Für den Inhalt ist ausschließlich das herausgebende Unternehmen verantwortlich.)Mynewsdesk GmbHDocument MYNEWD0020140612ea6c0002v"
"18",2006-04-14,"Hier spricht der Automat   ","Hier spricht der Automat   752 words14 April 2006ComputerwocheCPWCHEGerman(c) Computerwoche 2006   Die Postbank bedient ihre Telefon-Banking-Kunden im teilautomatischen Sprachportal.   Von Christian Pereira*   Bei der Dialogautomatisierung, also der Mensch-Maschine-Kommunikation an der telefonischen Schnittstelle, verliert der technokratische Ansatz der Softwareprogrammierung immer mehr an strategischer Bedeutung - zugunsten eines attraktiven ""Hear & Feel"". Das belegt der Fall der deutschen Postbank, deren teilautomatisiertes Telefon-Banking-Portal eines der ersten seiner Art ist.   Initialzündung für das Gesamtprojekt war der Börsengang der Bank. In dessen Vorfeld sollte das neue Sprachportal ein Signal für die Innovationskraft und die Kreativität der Bank sein.   Der ersten Projektphase in den Jahren 2004 und 2005 lag eine einfache Vision zugrunde: Die Postbank wollte Nummer eins im automatisierten Telefonbanking werden. Das bedeutete eine erhebliche Steigerung des Automatisierungsgrads. Das war zum damaligen Zeitpunkt alles andere als trivial: Es musste im wahrsten Sinne des Wortes eine Pionierlösung her - auf einem der Großbank angemessenen Niveau.   Drei ""Klassenziele"" hatten die Portalbauer zu erreichen:   • eine hohe Kundenakzeptanz,   • eine spürbare Effizienzsteigerung und   • die Senkung der durchschnittlichen Fallbearbeitungskosten im gesamten Telefonbanking.   Dialogführung und ""Atmosphäre"" des Portals sollten für den Anrufer optimal gestaltet sein. Kernthemen waren die Navigation über menschliche Sprache (Hyperlinks, Shortcuts, Multislot-Eingabe), die Identifizierung der richtigen ""Bot-Stimme"" (Bot = virtueller Agent) über ein Multi-Persona-Design mit unterschiedlichen Stimmen und Ausdrucksweisen, die kontextsensitive Fehlerbehandlung und diverse Hilfsangebote an den Nutzer, dazu dynamische Anwendungen und unterschiedliche Initiative-Muster (auch die Maschine kann aktiv Wahlmöglichkeiten vorschlagen). Exemplarisch sollten die Teilprozesse ""Kontostand"" und ""Überweisung"" optimiert werden.   Der Aufgabenbereich des Postbank-Portals war fest definiert. Er beschränkte sich auf gut standardisierbare Abläufe, wie sie im Rahmen von Überweisungen, Buchung von Umsätzen und Abfrage von Kontoständen typisch sind. Damit sollten zum einen die menschlichen Mitarbeiter von Routine-Anfragen entlastet werden, zum anderen eine - vor allem in relativ ruhigen Phasen - kostenoptimierte 24-mal-sieben-Stunden-Erreichbarkeit gewährleistet sein.   Zudem wollte die Postbank den Nutzern ihre Produkte aktiv anbieten, also Cross- und Upselling-Potenziale ausschöpfen. Dazu war es notwendig, den Kunden über eine CRM-Schnittstelle zu klassifizieren und personalisiert anzusprechen.   Durch die Verlagerung von Standardanfragen auf das automatisierte Dialogmodul ließen sich, so die Hoffnung, die durchschnittlichen Fallbearbeitungskosten senken. Bei einschlägigen Portalen betragen die Kosten - je nach Anwendungsbereich und Komplexität des Themas - zehn bis 30 Prozent dessen, was für einen von menschlichen Agenten bedienten Vorgang auszugeben wäre.   Beim Kickoff hatten die Portalbauer - neben Postbank-eigenen Mitarbeitern waren das auch Sprachingenieure von Voice Objects und Softlab - mit einigen Herausforderungen zu kämpfen. Dazu zählten die heterogene Landschaft auf der Systemseite, aber auch die Backend-Integration mit dem vorhandenen SAP-System.   Erschwerend kam hinzu, dass die Postbank als Finanzinstitut mit hohen Anforderungen an Qualitäts-Management, Ausfallsicherheit und Stabilität konfrontiert ist. Sie werden zum Teil durch die Bankenaufsicht gefordert und geprüft.   Gleich im ersten Projektabschnitt wurde aber auch deutlich, dass die Automatisierung von Sprachdialogen weniger eine Sache von Datenbankprogrammierung und Hardwareauswahl ist. Viel wichtiger war die Erkenntnis, dass die Akzeptanz des Dialogteils nur dann das Prädikat ""Weltklasse"" verdient, wenn sie die Marke von 80 Prozent erreicht.   Im Zuge dieser Erkenntnis standen sich bei diesem Projekt zwei ""Ideologien"" gegenüber: Die eine wird vom VUI (Voice-User-Interface) repräsentiert und beschäftigt sich mit Aufbau, Logik und Abläufen menschlicher Dialoge sowie mit deren Umsetzung in Anforderungen für den künstlichen Gesprächspartner. Die andere redet einer eher ""technokratischen"" Vorgehensweise das Wort. Sie legt die Betonung auf die Programmierung. Der VUI-Ansatz der ""Portal-Designer"" setzte sich schließlich durch.   Offenbar haben die Postbank-Kunden das System akzeptiert: Die Zugriffszahlen auf das Portal liegen bei 200000 Anrufen pro Tag. In der Übergangszeit können die Kunden neben dem Portal noch kostenlos auf einen menschlichen Agenten zugreifen.   Das Wachstum vollzog sich ohne Kompromisse zwischen Mensch und Maschine. Der teilautomatisierte Dialog garantiert dem Bankkunden rund um die Uhr eine effiziente Beantwortung seiner Standardfragen und ist zudem diskret, falls das Konto mal überzogen ist.   Erreicht wurde auch das Ziel der Kostensenkung: Einsparungen von 20 bis 30 Prozent entsprechen schätzungsweise einem mittleren fünfstelligen Euro-Betrag pro Tag. Oder finanztechnisch ausgedrückt: Der Return on Investment stellte sich beim Postbank-Sprachportal binnen weniger Monate ein.   Darüber hinaus ließen sich bei der Programmierung sowie bei Pflege und Betrieb des Portals Skaleneffekte erzielen: aufgrund der Wiederverwendbarkeit von Modulen, der Backend-Integration und der einfachen Erweiterung der Grammatiken, dank einfacher und schneller Modifikationsmöglichkeiten, durch die vollständige Trennung von Applikation und Plattform, den plattformunabhängigen Betrieb der IVR (Interactive Voice Response), ASR (Automatic Speech Recognition = Spracherkennung) und TTS-Funktion (Text to Speech) sowie mit Hilfe einer übersichtlichen Management-Konsole für Launch, Start und Restart. (qua)   Die Ziele Akzeptanz Effizienz Kostensenkung Das Projekt Die Ergebnisse    IDG Communications Verlag AGDocument CPWCHE0020060413e24e0001r"
"19",2015-05-26,"Larry Page; One for All","Larry Page; One for AllUwe Jean Heuser   5291 words26 May 2015ZEIT onlineZEITON21/2015GermanCopyright 2015 Zeitverlag Gerd Bucerius GmbH & Co   Larry Page wants to use Google's billions to solve some of the biggest problems facing humanity. Now he's talking about his plans and his moment of enlightenment at age 12. A visit with possibly the world's most influential entrepreneur.Google-Gründer Larry Page auf einer Entwicklungskonferenz seines Konzerns am 15. Mai 2013 in San Francisco © Justin Sullivan/Getty ImagesRead the German version of this article hereYoung geniuses need moments of clarity. Larry Page had his own in the mid-1980s. He was around 12 years old - he doesn't remember exactly how old he was anymore - when his big brother gave him a biography of Nikola Tesla. Five hundred pages about the Serbian-American inventor foreseeing 120 years ago how we would use alternative current and communicate wirelessly today. Tesla was an eccentric pioneer, who became the darling of New York high society and won over the largest investors of his day as patrons. But he was never a businessman. Cheating some, he was abandoned by others, only to die poor and lonely without ever having realized his vision.The young Larry, son of a computer scientist and a programmer in Michigan, read the book in blur. After he was done with it, he began to cry and it took him a while to stop. At the beginning of the book he had thought: ""Wow, how amazing, he could be this mind-blowing inventor developing transformers and other things we still use today."" But Tesla's story became sadder by the page. ""It appeared to be a failure that he could hardly finance his research,"" Mr. Page recalls. ""Just think what he could have achieved if he'd just had more money.""Larry Page was born a technology freak. Even as a young schoolboy, he would disassemble computers and put them back together. And he was the first at his school to do his homework on a computer. A shy pupil, later on he dreamed up with wild innovations: a cable into the sky allowing satellites to simply be raised into orbit, or a futuristic raised railway with passengers travelling to their own cars to their individual destinations.But starting that night with Tesla, he became more than just the ultimate nerd - in his head he also became a businessman. He thought: If I want to reach truly a lot of people and made the world a better place, I'll have to do one thing: Quickly and radically commercialize my inventions.Scientists often need 10 to 20 years to bring their inventions to the masses. But it happens much faster at young Californian tech firms. Mr. Page was fascinated by that and applied for graduate school at Stanford, the university in the heart of Silicon Valley.At first his interests took many different directions: Telepresence, or technologies allowing people to sense something far away. Self-driving cars. But his professor found the idea of exploring the structure of the still-young World Wide Web most exciting. And so, Mr. Page got to work and soon realized that the importance of specific content was relative to how many links there were back to it. Shortly thereafter, his work became the foundation for a new Internet search engine. He and his partner Sergey Brin called it Google, after the extremely long number with 100 zeros known as a googol.Larry Page learned the lesson from his youth reading extremely well, or as he says: ""So far, I can't complain."" Google is now 17 years old. It employees 55,000 people and has turnover of around $60 billion (€54 billion) and net profit of almost $15 billion - predominately from advertising on the Internet, used by more than two billion people.It's no wonder therefore that the California company with a market capitalization of some $350 billion is one of the most valuable firms there is. And the 42-year-old Mr. Page isn't merely founder and boss - he also holds the controlling stake along with his business partner. That doesn't exist anywhere else among the world's leading companies.It also makes the inventor from Michigan one of the most influential entrepreneurs in the world. And he uses this influence to make technological breakthroughs. Google is building self-driving cars, developing robots, flying wind turbines and giant communication balloons, as well as attempting to revolutionize medicine with high-tech.Others, like Microsoft founder Bill Gates, put their money into foundations, but not Larry Page. He wants to change the world with entrepreneurial means. He is a kind of Jules Verne with money.Mr. Page has undertaken a historic experiment: Can a company so completely dominating and defending its business become even more successful with further innovations?Dieser Artikel stammt aus der ZEIT Nr. 21 vom 21.5.2015. Die aktuelle ZEIT können Sie am Kiosk oder hier erwerben.All the giants of America's corporate past have failed under similar conditions. General Motors wanted to become the world's leading industrial firm - and had to be bailed out by the government following a long decline. IBM lost the software market to Microsoft after personal computers became widespread. And Microsoft, in turn, had to concede its position as the leading Internet company to Google.During this experiment, Larry Page has to contend with two groups of critics: While investors in America are worried he might burn through Google's billions, Europeans on the other side of the Atlantic are concerned he could prevail and end up controlling their lives.It was time to speak with the man who is so reticent, long overdue, in fact. But it took years and many conversations with his employees.That the interview request was finally approved could be in part due to Google's recent charm offensive in Europe. Under pressure from competition authorities, the firm has just announced an initiative for European newspapers. Google says it wants to work with the Continent's publishers to find new opportunities in digital journalism, offering €150 million for innovative ideas. Die Zeit is a founding member of this cooperation, but that in no way impairs its journalists from reporting independently about Google.Visiting Mountain View, the town in southern Silicon Valley might as well be called Googleland. The grid-like streets are lined with mid-sized buildings sporting colorful logos. Play zones with lounge chairs and bike racks are sprinkled between them, as are parking spots for electric cars.This is the Page Empire - a playful copy of the Stanford campus. Each year, two million people apply for jobs here, but only 5,000 are accepted.Until the eco-friendly new main building is completed, the boss is continuing working in a glass structure near the old center of the campus.The interview took place in mid-April in a non-descript conference room on the second floor.It should be mentioned that Larry Page is not a healthy man. For the past decade, he's suffered from Hashimoto's disease, a rare autoimmune disorder causing chronic infections of the thyroid gland. Requiring hormone tablets, Hashimoto's can cause periods of weakness and weight gain, as well as other symptoms. Mr. Page says the ""rather benign"" disease doesn't impair him. Whether it causes his noticeable vocal chord disorder is unclear. But his left vocal chord never recovered from a paralysis long ago and the right side was affected in 2012. This caused even him to stay away from important business appointments such as the Google shareholders' meeting. Ever since, his appearance and the way he sounds have taken on particular importance.After waiting half an hour, he enters quietly, avoiding eye contact, as he known to do.Wearing jeans, a striped t-shirt and light jacket, he clearly still doesn't put a premium on his appearance. He looks like he did when he attended the World Economic Forum in Davos, Switzerland in 2009, but his bushy hair has grayed since then. He's still the same grinning boy though, an avid kite surfer and technology dreamer. Only his voice is jarringly different, it sounds like he is speaking from a PA system with a faulty mike. Pure and slightly weak, it is the voice of hoarse angel.Anyone hoping to understand the way Mr. Page thinks should speak to him about his favorite topic: technology. Last year, Google labs surprisingly created a contact lens able to monitor a diabetic's blood sugar level from their tears. Mr. Page says they never would have tackled the project had not an engineer working on the Google glasses campaigned passionately for it. ""Basically it would have been very easy for us to say no,"" says Mr. Page. The message was: ""It's a highly doubtful idea, but if you're really passionate about it, that's fine."" Then the development went surprisingly well, ""and suddenly you have something that looks like a product and could be approved. I'm constantly reminded: The more I learn to be an inventor and entrepreneur, a businessman, the more I'm aware not to trust my intuition too much.""Mr. Page almost always searches for the principle behind something.If you want to judge an idea, you should start with physics, he explains: ""I wish I had learned more physics at school, that subject is hard to learn on the side."" Physics simply exists, you can't ignore it. An example: Carbon fibers have been talked about for a long time, but they are still very expensive. However, since they're produced from plentiful sources for carbon, everyone is certain people will one day be able to make them quite cheaply. If there were gold in them, says Mr. Page, the situation would be different. You can't get away from the high price of gold.He discovered another example of the importance of physical properties in traffic. If you photograph a highway from above shortly before a traffic jam, then only roughly a third is filled with cars. ""The road isn't really full, people just don't react fast enough,"" he says. In other words: There is still plenty of room for optimization with self-driving automobiles.Even close colleagues admit that Mr. Page is very different from them.When they drive home they think about dinner or a kid's birthday party. Mr. Page thinks about errors in the system. About traffic jams. About exploding real estate prices in the region, which also cause Google salaries to rise. A small house costs about $100,000 to build, but on the market it will fetch millions, because cities keep land prices artificially scarce. That's the way it will stay, too, because invested interests are too strong and local politicians were discussing the wrong issues.Mr. Page sees only one solution: ""Cheap transport"" allowing people to live further away. Which is why he's currently interested in a city in Latin America, where long buses make stops nearly every minute at bigger stations, almost like a subway. ""I think about things that burden us,"" says the man who loathes inefficiency.Still, the top Googler makes a point of being an optimist. His message: It's easy to see the daily difficulties around us, but it's hard to see the progress going on in the world. Whether regarding health or happiness: ""The data tells us that the situation has improved."" And more than anything, for Mr. Page it's digital technologies and their commercialization that are helping humanity. ""That's what gets me out of bed in the morning.""And that's also what's causing others to lose sleep. The digital revolution threatens not only our sovereignty over our own personal information, but also our jobs. The optimist-in-chief Mr. Page says that some day computers and robots might boost productivity 10 or 20 times from today's levels. Does he understand that this is deeply disturbing to many people?Mr. Page nods - and immediately distances himself from these feelings of unease. Exactly because many are doing very well right now, there's a natural dislike against change, he analyzes. He then asks if it really would be so bad when people no longer had to do unpleasant jobs? Welding, for example, can generate harmful gases, which is why that task is often left to robots. Lots of people don't particularly enjoy what they do. More efficiency at work also means that we can enjoy lower prices, enabling us to satisfy our basic needs like shelter and safety more easily. ""That means that you can spend more time with your children,"" says Larry Page encouragingly. The world just needs to be better organized. ""When we have too many workers, then let us start to reduce working hours. I just asked people, everyone is happy to have a few more days of vacation.""That is Mr. Page's mantra: If problems are created by Google's progress, then we find ways to solve them and improve life. And if that demands, like a German trade union representative, fewer working hours by equal pay, then that's okay. Mr. Page is the Homo faber of the Internet age. For him, the real danger is opposing technological progress and greater efficiency. Such dangers lurk particularly in the Old World: ""Especially in Europe, it appears easy to ignore the fundamental physics of a question in order to claim everything is just fine when things here cost twice as much as elsewhere. This attitude worries me greatly, because it hinders the work of entrepreneurs.""But should not a society also have the right to say ""No"" to a superior technology? Certainly, agrees Mr. Page. But that's not particularly clever. ""If you make everything twice as expensive, you reduce people's quality of life."" And do you really want to keep local entrepreneurs from making their contribution to the global economy? Naturally it's great when citizens have the feeling they can decide. ""I'm merely saying that when they make decisions contrary to a global system of capital, then they have to do that consciously and seriously. And I don't believe anyone is doing that.""Should Europeans be offended? Certainly not! It's much more important to understand how Larry Page thinks, because he is using all of Google's power to push forward with the global change he desires.You might think Mr. Page had achieved enough already. He is married to the beautiful and charming Lucinda Southworth, the Stanford graduate in bioinformatics. They have two children. The billionaire 30 times over also has all the vestiges of his success: The house, an eco-residence in the Valley. The boat, a mixture of super-yacht and expedition ship. The airplane, a large Boeing that dwarfs the private Gulfstream jets of the other ultra-rich.But Mr. Page is neither sated nor content. Perhaps that is because his path to success at Google was not unbroken.Mr. Page and Mr. Bring started their company in 1998, with Mr. Page becoming the first boss. He was 25 years old, a radical nerd with little regard for the feelings of his employees. Google grew rapidly, making a name for itself as an effective search engine, but Mr. Page also sparked conflicts. The biggest was after he eliminated an entire level of high-ranking product managers. He didn't like that people without the technical know-how were superior to his engineers and limiting their freedom.Mr. Page set down a marker about the value of software developers that would never be forgotten at Google. But the uproar was still huge. New product managers soon had to be hired. Investors decided they wanted someone at the helm with more experience. So shortly thereafter, Larry Page handed over his post to the almost 20-years older computer executive Eric Schmidt. Lasting 10 years, it was known as a period of ""parental supervision"" at Google.During that time, Mr. Page got rich after Google was listed on the stock market. He celebrated a glamorous wedding in the Caribbean, created Street View, the virtual street map of the world that became so controversial in Germany, he launched the world's leading mobile operating system Android, which enabled Google to tap the smartphone market. He had the last word with hiring and ensured that the company primarily took on engineers. At the same time, he retreated, got rid of his office assistants and eliminated press appointments entirely.He was no longer in charge of daily operations, and despite coordinating with Mr. Schmidt, Google was not what he hoped. According to the American media, the firm's top managers spent time talking about projects whose nature, in his opinion, was too small. Google's innovations were supposed to solve big problems for hundreds of millions, if not billions, of people. Mr. Page didn't - and doesn't - want anything less.At this point it becomes personal and suddenly he hesitates with his answers.Did he return to the top job a different person?He says he experienced a lot and much happened at the company over that period, Mr. Page replies uncommitted.Did he learn in that time to trust others more?No, no, his fundamental style of management is unchanged, he says. In specific areas he rarely dives deep until he needs to truly understand what his people are working on and if they're doing a good job.In the past, he pursued conflict at the company, today cooperation?No, he says, the company needs both. Conflict over issues, but it can't become personal.If he hasn't changed, was hiring Mr. Schmidt a mistake?""I'm not concerned about such things,"" he says, again avoiding an answer. The firm's leadership all share the same crazy dream, he says: ""No matter how well we do something, we want to make it a little bit better, so we can improve people's lives.""But no one is as crazy about that as Larry Page.At the start of 2011, he regained control of his company. And his plans, his ambition had only gotten bigger. He reorganized the management and gave more freedom to the executives in charge of the different units like the search engine and video channel YouTube. All Google products were revamped so they had the same corporate look. But more than anything, he put all his energy into moonshots - radical projects that could take 10 years or more to be realized.The self-driving car soon to be produced is meant to revolutionize transportation, intelligent industrial robots are to renew factories and trade, Google researchers into artificial intelligence want to improve on human thought. Physicians and technicians from the Google unit Calico are developing methods to combat ageing and the maladies it brings. Solving the planet's energy problems are naturally also on Google's to-do list. Mr. Page has acquired several billion-dollar companies - and with them the world's best experts in their subject matter. For example, the legendary iPod and iPhone designer Tony Fadell and his startup working on networked living, or the brilliant brain researcher Demis Hassabis and his company Deepmind Technologies.Google now even has its own venture capital division for long-term investments.Mr. Page and his partner Mr. Brin, who is in charge of the futuristic lab Google X, are doing everything they dreamed of as boys - and still dream of as grown men. One example is the project Loon, whose name could either describe a crazy person or the second syllable of the word ""balloon."" Mr. Page regains his excitement while speaking about technology rather than himself. For a long time, people at Google had asked themselves how best to get remote parts of the world quickly connected to the Internet. Mr. Page thought: ""Satellites are really expensive. Why not use balloons?"" He searched (with Google) and discovered that there was a balloon 40 years ago that sailed around the Earth several times. ""I hadn't known about that,"" Mr. Pages relates. ""And I thought: People did that a long time ago. Now, we have much better materials and it's not that heavy."" In 2013, Google tested the first gas balloons in New Zealand.The search engine, which supplies most of the company's profits via advertising, has made Google rich and powerful. The company has $60 billion in the bank. Larry Page wants to use that capital to make the wildest visions into reality. Google doesn't want to sit on the sidelines while other companies shape the world. For Mr. Page, the tech revolution offers the historic opportunity to do it differently than the General Motors, IBMs and Microsofts out there. Even ahead of the initial public offering, Google's founders wrote that they would still retain the freedom to pursue long-term projects and no one should be surprised when they do, neither skeptics nor fans.One of the most important people Larry Page used to talk to was the late Apple founder Steve Jobs, who died in 2011. Stock market darling Apple concentrated solely on a few products and Mr. Jobs allegedly told Mr. Page often that Google made ""too much stuff."" Mr. Page would reply ""and you make too little stuff.""These days, he has even more projects running. Was Steve Jobs right after all?""We ask ourselves that every day at Google,"" says Mr. Page. ""But on the other hand, there's this underlying dissatisfaction that I and many others at the company have: There are all these big firms. But is what they're doing really important and effective enough for us to feel good about it? We want the answer to be 'Yes'. That is how people will judge us: Are we making things that are meaningful to them? And are we doing that more than others? I believe that's a great opportunity.""Page, the evangelist. Page, the engineer. Page, the businessman.What appears to be at first a contradiction, all comes together in Silicon Valley. It's what makes Google what it is: the ultimate nerd company that's fighting off competitors, banking billions and still hoping to save the world.The founder has been back at the helm for four years, and the equation Google = Page is truer than ever.That's why Mathias Döpfner, the chairman of German publisher Axel Springer, made a mistake in his full-page 2014 commentary describing Google as a terrifying company that swallows people's data and the media's profits. He addressed the ""open letter"" to the wrong person - Google's current executive chairman Eric Schmidt. It should have started with ""Dear Larry Page"" instead.Let's talk about Germany's unease with Google.""I think you talk about us with unusual negativity in Germany,"" says Mr. Page carefully. No matter how controversial the discussion was up till then, talking about new technologies, Mr. Page was clearly more comfortable. He even made contact with his brown, slightly melancholic eyes after three-quarters of an hour. But not anymore.Question: How can you claim that Google isn't evil, when it harvests all of our information to get to know us better - what if this information gets into the wrong hands?Mr. Pages answers that German history plays a special role in sparking this fear.Google realized this a while ago and changed its behavior in Germany. Many people want access to the data, Mr. Page admits - criticizing first and foremost his own government, which collects personal information with abandon. ""I don't think a democracy functions when your government collects data and doesn't at least fundamentally say what it's doing.""In Europe, however, he's spoken often with the heads of telecommunications companies who told him: ""We need access to the data of our users so we can know exactly what they're doing and can show them the appropriate advertising."" Mr. Page says he told them they were crazy to admit to something like that. Then he turns back to his visitor: ""And you're asking me now, 'Oh you have all this data!' Every company I talk to in Europe says: 'Oh we need access to all the user data so we can make money.' They don't even have serious reason; they just want to make profits. And by the way, they employee more people than they really need in my opinion.""Mr. Page can no longer be stopped. ""It's true that we have a lot of data. A lot of people have a lot of data and they try to get access to it and governments want it too."" Google, he says, stood up to many countries by protecting the data of its users.More than anything, people should remember one thing: ""We use a lot of data in order to offer better services. That's how we improve the search, that's how we achieved speech recognition. Whoever offers the services will have the data and use it for improvements. The debate is fine, but you can't simply go back and say: The world should be like it was 20 years ago.""It is strong criticism of European views and Mr. Page's logic is clear: Foregoing the data or hindering its collection undermines progress. And nobody can really want that.Self-restraint isn't an option in the Google worldview, also not when it comes to the question of taxes. Does Mr. Page understand that taxpaying citizens are annoyed when a rich company like Google does everything it can to avoid taxes?""I understand why people are annoyed by this tax question,"" the tech mogul replies. ""But we don't make the rules. We follow the law and our competitors do the same. That doesn't mean that I support a worldwide tax system, but it's unbelievably complex and companies have a hard time with it. If you created a new system - it certainly wouldn't be one like this.""An accusation that other companies have made for a long time and that the European Union is now pursuing is based at the core of Google's business: especially for shopping queries, the search machine features its own ads most prominently, even when they're not the best deals. Competitors in the Valley have even coined the phrase: ""A Google search is a search for Google."" Is Google violating the imperative of impartiality?""I absolutely don't believe that,"" Mr. Page responds.But that's what his competitors say.""Yes, exactly. What a surprise,"" he mocks - the only time in more than two hours that Mr. Page is derisive about something.Nobody has accused Google of offering its users poor service. And that's what counts.Even when he doesn't say it directly, Larry Page considers the Old World hypocritical. At one point, however, he's very direct: ""If I were a young entrepreneur today and I had the choice of starting my Internet firm in Germany or Silicon Valley, it wouldn't be a hard choice. And regulation will only get worse in Europe. It will be very hard to build a company of global import there. And data protection, all these laws just make it harder.""That doesn't sound diplomatic.But if Mr. Page and the Valley are right and we truly are in a century of big technological advances, Europe should at least listen to this criticism.Maybe it doesn't help much to fight with Google over the long-established search engine market, because that's yesterday's battle. Europe instead should create the conditions so its young entrepreneurs can conquer the markets of tomorrow.That's what Mr. Page would also like.He's aware that he's in the middle of an exorbitant experiment when he risks Google's huge resources for quantum leaps into the unknown. Most companies find it extremely difficult to change themselves into something new, he says. ""Their entire history works against it."" That's why he's trying to invest early as possible. They more audacious his projects are, the better. Then he will attract the best people and there's no competition.Can Mr. Page flee the curse weighing down on market leaders? The digital glasses known as Google Glass were a widely-heralded project from the company's future lab, though they've flopped - at least for now. But Mr. Page points out that Google, with just 50,000 employees, has already made a huge impact. ""We're doing something right,"" he says. ""And that applies at least a bit for all of Silicon Valley.""There are people who say that only one or two of these grand plans have to work to make Google even more successful and powerful than it already is. And that's exactly the bet being made by the radical at its helm. Will Google then dominate all of our lives?The developments are still ""in a very early stage,"" he assures the fearful. Besides, he jokes that he feels ""somehow old and conservative measured by Silicon Valley standards."" However, Mr. Page is undoubtedly carrying out what he once learned from the book about Tesla's failure and what slumbers deep inside Google. The often-derided do-gooder company motto ""Don't be evil"" contains the belief in the power of the market. Capitalism is not evil, but rather the best method to realize great inventions. Google's official goal ""to organize the world's information to make it generally available and useful"" is also in no way out of date.What is the next big Google idea?""I can't tell you that.""But what's keeping him particularly occupied?""The problem of people faring badly because they were born in the wrong place. That is also the biggest potential business. Billions of people have no money. That makes no sense and doesn't help anyone."" The Internet can make a big impact there, says Mr. Page, and enable people to determine their own fates. ""We can offer someone in Africa the same product as the President of the United States. And we don't have to take any extra money for it.""Internet for everyone means Google for everyone. It could be that Mr. Page's big bet pays off. Then we'll all drive cars with Google programs and work with Google-controlled robots - until these, thanks to Google Deepmind, become smarter than we are and replace us at work and know our personal wishes before we can even utter them.Larry Page is trying to change the world for us all. But what if humanity can no longer control this progress? Maybe Mr. Page should have attended Google's recent ""Zeitgeist"" conference in London. At the end, a star took to the stage: the seriously disabled British physics genius Stephen Hawking, whose life story was recently turned into a popular film.Mr. Hawking is wheelchair-bound and can only communicate via a speaking computer using artificial intelligence. He therefore optimistically expressed that just this artificial intelligence was on the verge of a major breakthrough. But in his speech primarily focused on a warning to humanity: We must now set the framework for intelligent machines, must now decide which goals they should follow. Otherwise machines would one day make these decisions - with consequences which cannot be foreseen.Translated by Marc YoungThis article originally appeared in DIE ZEIT. To contact the author: diezeit@zeit.dePMG9285387-ZEDE-/wirtschaft/unternehmen/2015-05/larry-page-google-inventorZeitverlag Gerd Bucerius GmbH & Co. KGDocument ZEITON0020150527eb5q00033"
"20",2016-02-29,"Revolutionäres neues Soundsystem von Turtle Beach - HyperSound Clear - erhält Zertifizierung für Markteinführung in Europa","Revolutionäres neues Soundsystem von Turtle Beach - HyperSound Clear - erhält Zertifizierung für Markteinführung in EuropaTurtle Beach; PR Newswire1027 words29 February 201613:00PR Newswire EuropePRNWDEGermanCopyright © 2016 PR Newswire Europe Limited. Alle Rechte vorbehalten.   SAN DIEGO, 29. Februar 2016 /PRNewswire/ -- Menschen in ganz Europa, die auf besseren Fernsehsound warten, können sich auf ein wesentlich verbessertes Hörerlebnis freuen, da das revolutionäre neue Soundsystem, HyperSound Clear™, die gesetzlich erforderlichen Zertifizierungen für eine Markteinführung in Europa erhalten hat. HyperSound Clear wurde entwickelt, damit Menschen Fernsehsendungen deutlicher hören und verstehen und ihre Lieblingssendungen, Musik und Videospiele dank eines brillanten, klaren Tons wieder genießen können. Mit den neuen Zertifizierungen beginnt Turtle Beach seinen Versand von HyperSound Clear an seine Vertriebspartner in Großbritannien, Frankreich, Deutschland und den Benelux-Ländern. HyperSound Clear wurde von der Turtle Beach Corporation[http://corp.turtlebeach.com/] (NASDAQ: HEAR), einem seit über 40 Jahren führenden Audiotechnologieunternehmen, entwickelt.Foto: http://photos.prnewswire.com/prnh/20160225/337378[http://photos.prnewswire.com/prnh/20160225/337378]„HyperSound Clear ist seit Jahrzehnten eines der innovativsten Audioprodukte. Nach unserer Markteinführung in den USA vor wenigen Monaten sind wir somit hoch erfreut über die erste internationale Ausdehnung und das Angebot in Großbritannien sowie ausgewählten europäischen Regionen"", sagte Rodney Schutt, Vizepräsident und Geschäftsleiter für den Geschäftsbereich HyperSound bei Turtle Beach. „In Anbetracht der enormen Größe des europäischen Marktes ist dies für uns ein bedeutender Meilenstein bei der Zertifizierung, da Millionen von Menschen mit Interesse an einem besseren Hörerlebnis beim Fernsehen und Home Entertainment dies in einer Art und Weise ermöglicht wird, die nur HyperSound Clear bietet.""Die HyperSound®-Technologie ist ein grundlegend neuer Ansatz der Tonausgabe, die einen zielgerichteten, schmalen Tonstrahl in der Luft erzeugt. Ähnlich wie eine Taschenlampe den Lichtstrahl lenkt, richtet HyperSound Clear einen Tonstrahl auf einzelne Zuhörer, was nachweislich die Tonklarheit und Sprachverständlichkeit bei Personen mit Hörschäden wesentlich verbessert1. HyperSound Clear läuft parallel zur Tonausgabe des Fernsehers oder der Heimkinoanlage, sodass eine Person, die im HyperSound-Strahl sitzt, die Lautstärke des Geräts unabhängig regeln kann und in ein beeindruckendes 3D-Audio eintaucht, während alle anderen im Raum den Ton aus den Fernsehlautsprechern oder der Heimkinoanlage mit normaler Lautstärke hören. Dies bedeutet, dass Menschen, die beim gemeinsamen Fernsehen mit Familienmitgliedern und Freunden Probleme beim Hören und Verstehen haben, jetzt wieder gemeinsam im Wohnzimmer das neuste Home Entertainment genießen können.Verbraucher in Großbritannien, Frankreich und Deutschland, die am Kauf von HyperSound Clear interessiert sind, sollten sich an ihren Hörgeräteakustiker vor Ort wenden, der sich für einen direkten Verkauf und Hilfestellung unter der E-Mail int@hypersoundhearing.com[mailto:int@hypersoundhearing.com] an das europäische HyperSound-Team wenden kann. In den Niederlanden, Belgien und Luxemburg können sich Verbraucher unter der Telefonnummer +31 (0) 88 530 6000 oder der E-Mail info@batterybenelux.nl[mailto:info@batterybenelux.nl] an Battery Benelux BV wenden, einem holländischen Großhändler für Hörgeräteprodukte und Vertriebspartner von Turtle Beach für HyperSound für die Regionen.Weitere Informationen zur HyperSound-Technologie und zu HyperSound Clear finden Sie auf der offiziellen Website unter http://hypersoundhearing.com[http://hypersoundhearing.com/].Informationen zur Turtle Beach Corporation Die Turtle Beach Corporation (http://corp.turtlebeach.com[http://corp.turtlebeach.com/]) entwickelt innovative, marktführende Audioprodukte für die Bereiche Konsum, Gesundheit und Gewerbe. Unter der preisgekrönten Marke Turtle Beach (www.turtlebeach.com[http://www.turtlebeach.com/]) ist das Unternehmen mit seiner breiten Auswahl an renommierten Gaming-Headsets für Xbox One und PlayStation®4 sowie PCs, Mobilgeräte und Tablets seit mehr als fünf Jahren deutlich führend in diesem Marktsegment. Unter der Marke HyperSound (www.hypersound.com[http://www.hypersound.com/]) vermarktet das Unternehmen zukunftsweisende Audiolösungen, die in der Hörgeräteakustik, in digitalen Informations- und Werbesystemen, an Kiosken und in der Unterhaltungselektronik Verwendung finden. Die Aktien des Unternehmens werden an der NAXDAQ unter dem Kürzel HEAR[http://www.nasdaq.com/symbol/hear] gehandelt.Hinweis zu zukunftsgerichteten AussagenDiese Pressemitteilung enthält zukunftsbezogene Informationen und Aussagen gemäß den US-amerikanischen Bundesvorschriften zum Wertpapierrecht. Mit Ausnahme der historischen Informationen in dieser Pressemitteilung können Aussagen hierin, einschließlich Aussagen zum Verwendungszweck der Erlöse aus dem Angebot und gleichzeitigen Privatplatzierungen, zukunftsgerichtete Aussagen zu Annahmen, Prognosen, Erwartungen, Zielen, Absichten oder Einschätzungen über zukünftige Ereignisse enthalten. Zukunftsgerichtete Aussagen basieren auf den Aussagen des Managements, die die Wörter „können"", „könnten"", „würden"", „sollten"", „glauben"", „erwarten"", „vorhersehen"", „planen"", „schätzen"", „zum Ziel setzen"", „vorhaben"" und ähnliche Ausdrücke enthalten, die zukunftsgerichtete Aussagen darstellen. Zukunftsgerichtete Aussagen bringen bekannte und unbekannte Risiken und Unsicherheiten mit sich, welche dazu führen können, dass die tatsächlichen Ergebnisse wesentlich von denen in zukunftsgerichteten Aussagen enthaltenen abweichen. Zukunftsgerichtete Aussagen basieren auf der aktuellen Einschätzung des Managements sowie auf Annahmen, die vom Management gemacht wurden, und Informationen, die dem Management momentan zugänglich sind.Auch wenn das Unternehmen annimmt, dass seine Erwartungen auf vernünftigen Annahmen beruhen, kann nicht garantiert werden, dass die Ziele und die Strategie des Unternehmens realisiert werden. Die tatsächlichen Ergebnisse können von zahlreichen Faktoren, einschließlich Risiken und Unsicherheiten, beeinflusst werden, wodurch sich die Ergebnisse wesentlich von denen vom Unternehmen oder in seinem Namen in zukunftsgerichteten Aussagen enthaltenen abweichen können. Diese Faktoren beinhalten, sind aber nicht beschränkt auf, die erheblichen Unsicherheiten, die mit der Annahme von bestehenden und zukünftigen Produkten einhergehen, die Schwierigkeit, neue Technologie zu kommerzialisieren und zu schützen, den Einfluss von Wettbewerbsprodukten und -preisen, allgemeine Geschäfts- und Wirtschaftsbedingungen, Risiken im Zusammenhang mit der Expansion unseres Geschäfts, einschließlich der Implementierung von Geschäften, die wir möglicherweise erwerben, unsere Verschuldung und andere in unseren veröffentlichten Berichten erörterten Faktoren, einschließlich der Risikofaktoren in dem in Verbindung mit dem Angebot veröffentlichten Prospektnachtrags, dem 10-K-Formular des aktuellen Jahresberichts und anderen periodischen Berichten des Unternehmens. Soweit nicht durch geltende Gesetze, einschließlich der Wertpapierrechte der Vereinigten Staaten und den Regeln und Bestimmungen der Börsenaufsicht Securities and Exchange Commission, vorgeschrieben, ist das Unternehmen nicht verpflichtet, zukunftsorientierte Aussagen nach dem Datum dieser Pressemitteilung aufgrund neuer Informationen, zukünftiger Entwicklungen oder anderer Faktoren öffentlich zu aktualisieren oder zu überarbeiten.1 Mehta, R., Mattson, S., & Seitzman, R., Kappus, B. (August 2015). Speech recognition in the sound field: directed audio vs. conventional speakers. Audiology Online, Artikel 14901. Bezogen von http://www.audiologyonline.com[http://www.audiologyonline.com/articles/speech-recognition-in-sound-field-14901]. Dr. Mehta und Dr. Mattson sind bezahlte Berater von Turtle Beach.http://rt.prnewswire.com/rt.gif?NewsItemId=GE31953&Transmission_Id=201602290700PR_NEWS_EURO_ND__GE31953&DateId=20160229[http://rt.prnewswire.com/rt.gif?NewsItemId=GE31953&Transmission_Id=201602290700PR_NEWS_EURO_ND__GE31953&DateId=20160229]KONTAKT: Wenden Sie sich für Medien-/PR-Informationen an: MacLean Marshall, PR/Communications Director, Turtle Beach Corp., 858.914.5093, maclean.marshall@turtlebeach.com; Wenden Sie sich für Investorinformationen an: Cody Slach, Investor Relations, Liolios, 949.574.3860, hear@liolios.com   http://photos.prnewswire.com/prnh/20160225/337378[http://photos.prnewswire.com/prnh/20160225/337378]   PR Newswire Association, Inc.Document PRNWDE0020160229ec2t000jh"
"21",2017-03-14,"Windows 10 Version 1703 - alle Details; Windows 10 Creators Update: Alle Neuerungen im Überblick","Windows 10 Version 1703 - alle Details; Windows 10 Creators Update: Alle Neuerungen im ÜberblickPanagiotis Kolokythas und Peter Stelzel-Morawietz 3092 words14 March 201710:22PC-Welt OnlinePCWOLGerman(c) 2017 PC-Welt. www.pcwelt.de[http://www.pcwelt.de] Das Windows 10 Creators Update erscheint im Frühling 2017Mit dem Creators Update erhält Windows 10 ein weiteres großes Update. Hier alle Details zu den Neuerungen und Verbesserungen.Microsofts Windows-Chef Terry Myerson hatte das Creators Update für Windows 10 bei dem New-York-Event am 26. Oktober 2016 angekündigt. Nach diversen Vorabversionen für Windows Insider ist die Entwicklung nahezu abgeschlossen. Aktuell kümmern sich die Entwickler vor allem um das Bugfixing.Die finale Version wird voraussichtlich Ende März an die Nutzer ausgeliefert. Andere Quellen gehen von einer Veröffentlichung am 11. April 2017 aus.[http://www.pcwelt.de/a/windows-10-version-1703-creators-update-am-11-april,3425442]Die Versionsnummer von Windows 10 wird sich demnach mit dem Creators Update (Codename ""Redstone 2"") von Windows 10 Version 1607 (Anniversary Update) auf Windows 10 Version 1703 (Creators Update) erhöhen. Mit ""Redstone 3"" befindet sich bereits das zweite, große Update für Windows 10 in diesem Jahr in der Entwicklung.Tipp: Die wichtigsten Neuerungen von Windows 10 Creators Update stellen wir Ihnen auch im folgenden Video vor:Und das ändert sich mit Windows 10 Creators Update:1) Windows 10 Creators Update: Neuerungen im Bereich GamingWindows 10 Creators Update enthält Verbesserungen im Bereich Live-Streaming von SpielenSpielmodusAktuelle PC-Spiele machen ausgiebig Gebrauch vom Prozessor im PC (CPU) und auf der Grafikkarte (GPU). Nur mit ausreichend Leistung lassen sich Spiele ruckelfrei genießen. Mit Windows 10 Creators Update führt Microsoft einen neuen (Game Mode genannt) ein, der für mehr Leistung sorgen soll. Laut Microsoft werden bei Aktivierung alle zur Verfügung stehenden CPU- und GPU-Ressourcen des Systems so priorisiert, dass ein Spiel möglichst optimal auf dem Rechner laufen kann. Im Ergebnis soll der Modus dafür sorgen, dass das Spiel mit einer besseren Bildrate läuft.Hier kann in Windows 10 Creators Update der neue Spielemodus aktiviert werdenUm den Spielmodus zu aktivieren, rufen Sie die Einstellungen auf (Win-I) und gehen auf „Spielen -> Spielmodus“. Setzen Sie den Schalter bei „Spielmodus verwenden“ auf „Ein“.In den neu hinzugekommenen Einstellungen für Spiele finden Sie auch Optionen für die Aufzeichnungen eines Spiels als Videoclip oder Screenshot sowie die Übertragung von Audio und Video an einen anderen PC oder eine Xbox. In einem Spiel drücken Sie die Tastenkombination Win-G. Damit blenden Sie die Game Bar ein. Klicken Sie ganz rechts auf das Icon für die Einstellungen, und setzen Sie ein Häkchen vor „Spielmodus für dieses Spiel verwenden“. Nach und nach soll dieser Prozess aber automatisiert werden, so dass sich der Spielmodus bei den unterstützten Spielen dann automatisch aktiviert. Die Game Bar zeigt auch Schaltflächen, über die Sie das Mikrofon und die Spielaufzeichnung aktivieren können.Spielen unter Windows 10: Ist der „Spielmodus“ aktiv, steht einem Spiel mehr Prozessor- und Grafikleistung zu Verfügung. Die Steuerung erfolgt über die Game Bar (Win-G).Livestreams: Das Übertragen von Spiele-Sessions über Windows 10 per Live-Streams soll kinderleicht werden. Dazu werden alle benötigten Software-Komponenten mit dem Frühlings-Update 2017 direkt in Windows 10 integriert. Vorteil: Die Nutzer müssen keinerlei Extra-Software installieren und mühsam einrichten. Rudimentäre Live-Streaming-Funktionen bietet Windows 10 bereits jetzt. Mit dem Creators Update sollen die Möglichkeiten deutlich erweitert werden. So können über Beam die Streams laut Microsoft nahezu in Echtzeit übertragen werden, es soll also keine Verzögerung mehr geben. Der Start eines Gaming-Streams wird nur einen Klick benötigen. Unklar ist derzeit, ob nur Beam, sondern auch Youtube und Twitch unterstützt werden.Turnier-Modus: Als weitere Neuerung wird es Funktionen geben, mit denen Spieler einfach Turniere für ihre Lieblingsspiele erstellen und starten können. Dafür wird in Windows 10 die neue Turnier-Funktion von Xbox Live eingeführt, eine Funktion also, die auch Xbox-One-Nutzer erhalten werden. Das Ganze funktioniert - wenn das ein Spiel unterstützt - auch plattformübergreifend. Microsoft hat dafür vor einigen Wochen das Xbox Play Anywhere Programm gestartet, bei dem einmal ein Spiel erworben werden muss und dann sowohl unter Windows 10 als auch auf der Xbox One gespielt werden kann. Dabei werden auch die Speicherstände übertragen. Zu den ersten Titeln gehören Forza Horizon 3 und Gears of War 4.Microsoft erweitert die Full-Screen-Unterstützung der Windows Game Bar (Windows-Taste + G) für eine größere Anzahl an Spielen von Drittherstellern. Bis zur Veröffentlichung von Windows 10 Creators Update soll die Zahl der unterstützten Spiele nochmal steigen. Über die Windows Game Bar können Spielszenen als Video oder Screenshot aufgezeichnet werden. Neu hinzu kommt unter anderem die Unterstützung für folgende Spiele:Siehe auch: So funktioniert der neue Game Mode in Windows 10[http://www.pcwelt.de/news/Windows-10-So-funktioniert-der-neue-Game-Mode-10112542.html]2) Paint 3D: Neue Zeichen-App für 3DMicrosoft spendiert der Paint-App in Windows 10 ein umfangreiches Update inklusive 3D-Unterstützung: Paint 3D. Die neue 3D-Zeichen-App ist über den Windows Store erhältlich und erlaubt die einfache Erstellung von 3D-Inhalten und die Umwandlung von 2D- in 3D-Objekten.Paint 3D: Auch Remix3D - die neue 3D-Community - ist bereits verfügbarHinzu kommt die Community-Website Remix 3D Preview, über die die Windows-10-Nutzer unter anderem 3D-Inhalte miteinander austauschen können. Bisher ist sie aber - ebenso wie Paint 3D - nur als ""Preview""-Version verfügbar.3) Änderungen bei der Darstellung und den EinstellungenFür das anstehende Windows Update hat Microsoft einige Kleinigkeiten geändert, die Nutzer von Windows 10 bisher als störend empfunden haben. Dazu gehören auch Darstellungsprobleme bei hochauflösenden Monitoren. Damit Beschriftungen und grafische Elemente scharf erscheinen, ist eine Anpassung an die Bildschirmauflösung erforderlich. Das funktioniert bei einigen Programmen nicht optimal, sodass die Darstellung verwaschen erscheint.In Windows 10 Version 1703 werden Sie über den Kontextmenüpunkt „Eigenschaften“ einer ausführbaren Datei oder einer Verknüpfung die Konfiguration auf der Registerkarte „Kompatibilität“ ändern können. Hier setzen Sie ein Häkchen vor die neue Option „Verhalten bei hoher DPI-Skalierung überschreiben. Skalierung durchgeführt von:“ und wählen darunter „System“. Klicken Sie auf „OK“ und starten Sie das Programm. Sollte die Darstellung immer noch nicht optimal sein, probieren Sie die Einstellung „System (Erweitert)“ aus.4) Update-Downloads einschränken:5) Mehr Optionen in den Einstellungen Windows lädt standardmäßig alle Updates herunter, sobald sie verfügbar sind. Bei Windows 10 Pro lassen sich Updates eine Zeit lang zurückstellen, bei der Home-Version ist die Option jedoch nicht vorhanden. Ansonsten gab es bisher nur bei WLAN-Verbindungen die Möglichkeit, größere Update-Downloads zu verhindern. Sicherheitsupdates lädt Windows 10 in jedem Fall herunter, sobald eine Internetverbindung besteht.Seit Windows 10 Build 15002 – und damit im anstehenden Update auf die Version 1703 – lassen sich auch bei Ethernet-Verbindungen die Update-Downloads auf sicherheitsrelevante Aktualisierungen einschränken. Wenn Sie sich gerade in einem Netzwerk mit langsamer oder volumenbegrenzter Internetanbindung befinden, können Sie so größere Downloads vermeiden. Um die Funktion zu aktivieren, rufen Sie die „Einstellungen“ auf und gehen auf „Netzwerk und Internet -> Ethernet“. Klicken Sie auf die Netzwerkverbindung unterhalb von „Ethernet“, und setzen Sie den Schalter vor „Als getaktete Verbindung festlegen“ auf „Ein“. Bei WLAN-Verbindungen funktioniert das über „Netzwerk und Internet -> WLAN“ entsprechend.In den „Einstellungen“ gibt es jetzt die neue Rubrik „Apps“. Nach einem Klick beispielsweise auf „Apps & Features“ zeigt Ihnen Windows eine Liste der installierten Programme. Klicken Sie einen Programmeintrag an, um Schaltflächen wie „Ändern“ oder „Deinstallieren“ zu sehen. Weitere Kategorien sind „Standard-Apps“, „Offline-Karten“ und „Apps für Websites“. Bisher befanden Sie diese Einstellungen unter „System“. Die Verwaltung der installierten Desktop-Programme kann nach wie vor auch in der Systemsteuerung über „Programme und Features“ erfolgen. Microsoft hält die Systemsteuerung inzwischen jedoch für so entbehrlich, dass der zugehörige Eintrag aus dem Win-X-Menü verschwunden ist. Sie rufen die Systemsteuerung jetzt am schnellsten über eine Suche im Startmenü auf.Scharfe Anwendungen: Wenn Programme auf hochauflösenden Monitoren verwaschen erscheinen, können Sie in den „Eigenschaften“ die Skalierung anpassen und damit die Darstellung verbessern.Ebenfalls neu in den „Einstellungen“ ist die Kategorie „Mixed Reality“. Darüber lassen sich Mixed-Reality-Geräte konfigurieren und Einstellungen verwalten. Es geht dabei vor allem um die Windows Holographic Platform beziehungsweise Hololens, aber auch um die Integration von Geräten anderer Hersteller. Interessant sind die Funktionen für PC-Spiele, aber auch 3D-Produktpräsentationen. Bei Microsoft befindet sich außerdem die Windows Holographic Shell in der Entwicklung – eine 3D-Oberfläche, die man mit einer VR-Brille sehen und über Gesten oder Spracheingabe steuern kann.6) Registry-Editor wird aufgepptKleine Zugabe: Dem Windows-Registrierungseditor (Regedit.exe) hat Microsoft eine neue, praktische Funktion spendiert. Es gibt jetzt eine Adressleiste, die den aktuellen Pfad in der Registry anzeigt. Sie können auch einen Pfad eintippen, Abkürzungen wie „HKLM“ für „Hkey_Local_Maschine“ erweitert Regedit automatisch, wenn Sie die Enter-Taste drücken. Die Idee ist jedoch nicht neu, und beispielsweise auch im kostenlosen O&O Regeditor zu finden.Bücher lesen: Der Edge-Browser kann jetzt Bücher im Epub-Format anzeigen und bei Bedarf vorlesen. Die Sprachausgabe ist verständlich und macht nur wenige Fehler.In einem Buch zu blättern ist ein eigenes Erlebnis. Allerdings mag man etwa auf Reisen das Gepäck nicht mit schweren Büchern belasten. Praktischer sind dann Bücher in elektronischer Form und ein dazu passendes Lesegerät.Über eine neue Vorlesefunktion kann man sich künftig Ebooks vom Microsoft-Browser Edge vorlesen lassen. Dazu wird beim Anzeigen eines elektronischen Buchs oben rechts in der Leiste ein entsprechendes Icon zur Aktivierung der Funktion eingeblendet. Die Funktion steht nicht nur für im künftigen Ebook-Bereich im Windows Store erworbene Ebooks zur Auswahl, sondern auch für alle nicht dort erworbenen Ebooks im EPUB-Format.Mit dem Creators Update stattet Microsoft den Webbrowser Edge mit einer Lesefunktion für Bücher im Epub-Format aus. Die Dateien lassen sich einfach über den Windows-Explorer per Doppelklick in Edge öffnen. Klicken Sie in den oberen Bereich des Fensters, um die Symbolleiste einzublenden. Darüber lassen sich beispielsweise Schriftgröße und Schriftart einstellen. Über das zweite Symbol von rechts aktivieren Sie die Vorlesefunktion. Die ist durchaus brauchbar, auch wenn die Software manchmal über komplizierte Begriffe stolpert oder Wörter falsch betont.Ein E-Book-Store soll ab April als Teil des Windows 10 Creators Updates bereitstehen – für Windows-10-PCs, -Tablets und Mobilgeräte mit Windows 10 Mobile. Die gekauften Bücher lassen sich dann über den Edge-Browser lesen. Der E-Book-Store wird dabei zum neuen Teilbereich im Windows Store, über den Nutzer elektronische Bücher unterschiedlicher Verlage kaufen können. Der Kaufprozess soll ähnlich ablaufen wie bei Apps, Spielen, Musik oder Filmen. Bei der von uns getesteten Vorabversion 10.0.15019 war davon jedoch noch nichts zu sehen; in den USA soll die neue Funktion jedoch bereits aktiviert sein.Als weitere Neuerung zeigt Edge künftig die Emojis nicht im langweiligen Schwarzweiß, sondern in Farbe an. Dazu wurden die vom Browser unterstützten Emojis entsprechend ausgetauscht.8) Änderungen bei der Windows-NeuinstallationDatenschutz: Bei der Installation lassen sich drei Optionen zum Schutz der Privatsphäre festlegen. Die Einstellungen können Sie später jederzeit ändern.Am Installationsprozess hat Microsoft lange Zeit nichts geändert. Im Prinzip läuft die erste Phase bei Windows 7, 8.1 und 10 gleich ab. Sie booten den PC von der Installations-DVD oder einem USB-Stick, wählen die Sprache und dann das Ziellaufwerk. In der zweiten Phase nach dem Neustart geben Sie Daten für das Benutzerkonto ein, und je nach System lassen sich noch einige Kleinigkeiten wie die Desktop-Farbgebung (Windows 8) oder Datenschutzeinstellungen (Windows 10) festlegen.Bei der aktuellen Ausgabe von Windows 10 gibt es ein paar Änderungen in der zweiten Phase des Setups. Am auffälligsten ist die Sprachausgabe. Der PC liest Ihnen vor, was gerade auf dem Bildschirm zu sehen ist und welche Eingaben Windows erwartet. Statt nur über die Tastatur lässt sich das Setup jetzt auch per Sprachsteuerung bedienen.Eine weitere Änderung betrifft die Art der Anmeldung. Build 10.0.15019 fordert zuerst die Daten für die lokale Anmeldung an. Wer ein Microsoft-Konto verwenden möchte, muss auf „Use an online account“ klicken. Welche Aufforderung bei der Installation erscheint, hat sich seit Windows 8 mehrfach geändert. Mal kam man schnell zur Einrichtung eines lokalen Kontos, ein anderes Mal war die Funktion wieder gut verborgen. Unterschiede bei der Kontoeinrichtung zeigen sich auch bei einigen nicht-deutschsprachigen Windows-Versionen. Es ist daher nicht auszuschließen, dass sich das bis zur endgültigen Ausgabe Windows 10 1704 noch ändert. Wenn Sie sich für ein lokales Benutzerkonto entscheiden, müssen Sie sich aber spätestens dann um ein Microsoft-Konto kümmern, wenn Sie Apps aus dem Windows Store herunterladen oder One Note verwenden wollen.Ebenfalls neu ist ein vereinfachter Bildschirm für die Datenschutzeinstellungen. Hier gibt es jetzt die Optionen „Location“, „Speech recognition“ und „Diagnostics“. Wer nicht möchte, dass Windows Ortsangaben sowie Sprach- und Diagnosedaten an Microsoft sendet, kann die Funktionen deaktivieren. Im laufenden Windows können Sie die Optionen später in den „Einstellungen“ unter „Datenschutz“ aktivieren oder deaktivieren. Es bleibt weiter empfehlenswert, ein Tool wie O&O Shutup[http://www.pcwelt.de/downloads/Anti-Spy-Windows-10-O-O-ShutUp10-9784620.html] zu verwenden, über das sich viele Optionen zum Schutz der Privatsphäre deaktivieren lassen.9) Cortana wird besserRechner über einen Cortana-Sprachbefehl abzuschalten,Die Entwickler haben die persönliche Sprachassistentin Cortana in vielen Punkten verbessert und auch erweitert. Neu ist beispielsweise die Möglichkeit, den zu sperren oder in den Schlafmodus zu versetzen. Laut Microsoft reagiert Cortana dabei nur auf die Stimme des Nutzers. Wir werden dann mal bei Gelegenheit ausprobieren was passiert, wenn jemand ""Cortana, schalt den Rechner aus""-schreiend in den Raum gestürzt kommt. Bei Microsofts Kinect war es ein beliebter Prank, die Konsolengamer dazu zu bringen, den Ausschalt-Befehl in das Mikrofon zu hauchen.Über Cortana kann nun die Musikwiedergabe gesteuert werdenNeu hinzu kommt außerdem die Möglichkeit, die Musik-Wiedergabe per Sprachbefehle an Cortana zu steuern. Derzeit funktioniert dies aber nur in der englischsprachigen Version von Windows 10. Die Musik-Apps iHeartRadio und TuneIn Radio lassen sich damit ebenfalls per Sprachbefehle steuern und spielen dann auf Kommando des Nutzers den gewünschten Song oder Internet-Radiosender ab.Cortana besitzt nun einen Full-Screen-ModusWenn am Rechner aktuell nicht gearbeitet wird und Cortana ein Befehl gegeben wird, meldet sich Cortana mit einer Full-Screen-Oberfläche. Ausprobiert werden kann dies, indem man einfach ""Hey Cortana"" sagt und dann 10 Sekunden wartet, ohne den PC anzurühren. Cortana schaltet dann in diesen Full-Screen-Modus und liefert dann beispielsweise Wetter-Informationen in der vergrößerten Ansicht.10) Windows 10 Cloud - neue Windows-VarianteIn der Januar-2017-Vorabersion, Windows 10 Insider Preview Build 15010, fanden sich die ersten Hinweise auf eine neue Windows-10-Version: Windows 10 Cloud. Offiziell hat Microsoft die neue Version noch nicht angekündigt. Zunächst war darüber spekuliert worden, dass es sich bei Windows 10 Cloud um eine neue Streaming-Variante von Windows 10 handeln könnte, die in der Cloud und nicht auf dem lokalen Rechner läuft.Mittlerweile steht fest, dass es sich wohl eher um eine deutlich abgespeckte Windows-10-Version handeln wird, auf der nur Unifierd-Windows-Platform-Apps (UWP) aus dem Windows Store lauffähig sind. Oder mit anderen Worten: Was Windows RT zu Windows 8 war, soll Windows 10 Cloud zu Windows 10 werden. Wahrscheinlich reagiert Microsoft mit diesem Windows 10 RT 2.0 auf die zunehmende Konkurrenz durch Chromebooks. Windows 10 Cloud wäre für Hardware-Hersteller eine Möglichkeit, günstigere Geräte mit einer Windows-10-Version auszuliefern, die einfach sicher gehalten werden kann, indem die Anzahl der darauf lauffähigen Apps begrenzt wird.Windows 10 Creators Update: Diese neuen Funktionen sind ebenfalls an BordBlick auf den neuen Windows Defender, der mit Windows 10 Creators Update erscheinen wird.Das Start Menü erhält mit dem Creators Update neue Live-Kacheln und eine Anzeige des aktuellen Akku-LadezustandsIm Windows Strore gibt es künftig Themes für Windows 10 - sowohl kostenlose als auch kostenpflichtigeDas neue Action Center wird aufgeräumter und damit übersichtlicherWindows 10 Roadmap: Zwei große Updates in diesem Jahr[http://www.pcwelt.de/a/windows-10-roadmap-zwei-grosse-updates-in-diesem-jahr,3406218]Ausblick auf Redstone 3 - das große Update für Windows 10 im Herbst 2017Windows 10 Redstone 3: Neuerungen für die Zusammenarbeit mit anderen NutzernUrsprünglich hatte Microsoft mit Windows 10 Creators Update diverse Verbesserungen angekündigt, die es den Windows-10-Nutzern vereinfachen sollen, miteinander zu kommunizieren, Inhalte auszutauschen und gemeinsam an Projekten zu arbeiten. Die Veröffentlichung dieser Funktionen hat sich aber auf Herbst 2017 verschoben ""Redstone 3""Ein schnelleres Teilen Inhalten wird über die ""My People""-Funktion vereinfacht. Bei einem Klick auf den Teilen-Button einer App werden die Freunde und Bekannten des Nutzers an erster Stelle aufgelistet.Die Personen, die dem Windows-10-Nutzer wichtig sind, werden als Icon im rechten Bereich der Taskleiste angezeigtAußerdem will Microsoft die diversen Kommunikationstools, die Nutzer verwenden, miteinander vereinen. An einem zentralen Ort sollen damit alle Mails und Chat-Nachrichten erscheinen, die Nutzer über diverse Dienste empfangen. Die Schnittstelle wird offen für alle Anbieter sein. Die wichtigsten Kontakte des Nutzers werden im rechten Bereich der Taskleiste als Icons angezeigt. Über diese Icons kann der Nutzer schnell in Kontakt mit den Personen treten und erfährt auch, was die Personen aktuell mitzuteilen haben.VR-Headsets für Windows 10 ab 299 US-DollarZahlreiche Partner werden VR-Brillen für Windows 10 ausliefern - zu einem vergleichsweise günstigen PreisPartner wie HP, Dell, Lenovo, Asus und Acer werden passende Headsets anbieten, die den Genuß von 3D-Inhalten erlauben werden. Die VR-Headsets werden - anders als die Modelle von Oculus & Co - deutlich günstiger sein. Der Startpreis: Ab 299 US-Dollar.Eine Besonderheit bei den VR-Headsets: Sie verfügen über einen eingebauten Sensor für das Positionstracking des Trägers. Die Konkurrenz (HTC Vive, Oculus Rift) benötigt dafür externe Sensoren.Genaue technische Details hat Microsoft noch nicht verraten. Anzunehmen ist aber, dass diverse Technologien zum Einsatz kommen, die Microsoft ursprünglich für Hololens entwickelt hat.Noch kein Hololens für Endanwender in SichtDie Ankündigungen zeigen aber auch, dass Hololens noch nicht bereits für Endanwender ist. Mit dem Creators Update für Windows 10 schafft Microsoft aber zumindest schon mal die Grundlagen dafür, dass viel, viel mehr (vor allem junge Nutzer) die Möglichkeit erhalten, digitale 3D-Inhalte zu erschaffen.All-in-One-PC Surface StudioZeitgleich mit den Update-Details für Windows 10 hat Microsoft auch neue Hardware vorgestellt: Bei dem Surface Studio handelt es sich um einen All-in-One-PC mit 28 Zoll großem Pixelsense-Display im 3:2-Format, das mehr als eine Milliarde Farben darstellen kann und über eine Auflösung von 4500 x 3000 Pixeln verfügt (4,5 K). Mit rund 1,25 Zentimetern Dicke besitzt das Surface Studio laut Microsoft den dünnsten, jemals gebauten LC-Bildschirm. Ferner kann das Touchdisplay per Stift bedient werden, mehr als 1000 Druckstärken sollen dabei unterschieden werden. Der Monitor lässt sich stufenlos neigen, was je nach Tätigkeit ein angenehmeres Arbeiten erlauben soll.Vorerst wird es den Surface Studio allerdings nur in den USA geben, je nach Prozessor-und Hardwareausstattung zu stolzen Preisen ab rund 3000 US-Dollar. Alle Details zum neuen Microsoft-PC finden Sie hier[http://www.pcwelt.de/news/Surface-Studio-Microsoft-stellt-All-In-One-PC-vor-10065488.html]. Zu einem Thema gab es von Microsoft immer noch nichts Neues, nämlich zur neuen Mobiltelefongeneration „Surface Phone“.IDG Communications Verlag AGDocument PCWOL00020170314ed3e00005"
"22",2007-12-20,"Exchange Server 2007 Unified Messaging - Drei auf einen Schlag   ","Exchange Server 2007 Unified Messaging - Drei auf einen Schlag   von Klaus Jotz   2543 words12 November 2007Windows 2000 MagazinWINMG052German© 2007 Konradin Verlag. All Rights Reserved. For further information see   http://www.win2000mag.de[http://www.win2000mag.de]Die Zusammenführung der einzelnen Kommunikationsnetze in einem Unternehmen kann sich vorteilhaft auf den administrativen Aufwand auswirken. Mit dem neuen Unified Messaging Server als Teil des Exchange Servers 2007 bietet  Microsoft die Möglichkeit zum Umstieg. Allerdings sind dabei einige technische Voraussetzungen zu beachten.   Der schnelle Zugriff auf die relevanten Informationen ist eine wesentliche Grundlage für den Unternehmenserfolg. Das gilt heute mehr denn je, da die Nachrichtenflut immer größere Ausmaße annimmt. Zudem wollen viele Mitarbeiter auch von unterwegs Einsicht in die für sie bestimmten Nachrichten. Voraussetzung für die problemfreie und schnelle Bereitstellung der Daten ist der Einsatz intelligenter Kommunikationslösungen, die E-Mails, Telefonanrufe, Faxe, Anrufbeantworter oder selbst SMS selbstständig verwalten und anwenderorientiert präsentieren. Darüber hinaus müssen solche Systeme sowohl für den Benutzer als auch den Administrator möglichst einfach in der Bedienung und Betreuung sein.   Die aktuellen Systeme lösen diese Aufgaben bereits mit hoher Qualität, allerdings meist getrennt voneinander. So werden gesprochene und geschriebene Nachrichten häufig über separate Systeme abgewickelt. E-Mails erreichen das Unternehmen über spezielle Nachrichtenserver wie Exchange, und die Anwender greifen auf sie mit dedizierten Mail-Clients wie Outlook zu. Sprachnachrichten sind davon völlig losgelöst. Sie laufen über gesonderte Nebenstellenanlagen und Telefonapparate. Ähnlich ist es bei Faxen, die meist an unabhängigen Geräten ankommen und nicht, wie erforderlich, direkt bei ihrem Adressaten. S   tattdessen verteilen andere Mitarbeiter die Ausdrucke, wenn sie nicht ohnehin liegen bleiben oder übersehen werden. Darin liegt ein besonderer Nachteil dieser Technik, der zumindest einen gefährlichen Zeitverlust bedeutet. Aber auch die vielen unterschiedlichen Tools, Oberflächen und Funktionen, mit denen die Anwender und Administratoren umgehen müssen, machen die Handhabung solcher Nachrichteninfrastrukturen nicht gerade einfacher und fördern nur die Ineffizienz.   Vereinheitlichung der Nachrichteninfrastruktur. Ein Weg, diese Strukturen zu optimieren, heißt Unified Messaging (UM). Eine solche Lösung bündelt alle Arten von Nachrichten auf einer einzigen Plattform und stellt sie von dort aus zur Verfügung. Die Systeme bieten den Anwendern die Möglichkeit, alle Informationen schnell zu überblicken und an jedem beliebigen Ort bedarfsorientiert und je nach Verfügbarkeit und Zugriffsrecht zu bearbeiten. Durch die Konsolidierung der wichtigen Nachrichtensysteme auf einer Plattform reduziert sich für den Administrator die Anzahl der zu verwaltenden Systeme und für den Benutzer die Vielzahl von Endgeräten. Das verringert den Schulungsaufwand bei der Ausbildung und vereinfacht die Sicherheitsstrukturen. Selbst die Sicherung und Wiederherstellung der Daten ist nur noch für ein Gerät erforderlich.   Speziell die Anwender ziehen Nutzen aus der Vereinheitlichung. Da alle Nachrichten auf nur einem Eingangspostfach eintreffen, haben sie zu jeder Zeit die vollständige Kontrolle darüber. Auch der Zugriff erfolgt in der Regel über ein einheitliches Endgerät. Die Einbindung von Sprachfunktionen ermöglicht zudem die Übertragung der Nachrichten beispielsweise auf mobile Telefone. So können die Benutzer bereits auf dem Weg zu ihrem Arbeitsplatz ihre E-Mails und Sprachaufzeichnungen abhören oder sich Faxe auf das Mobiltelefon weiterleiten lassen.   Allerdings konnten sich UM-Systeme bislang noch nicht in dem Maße durchsetzen, wie es aufgrund ihrer Vorteile zu erwarten wäre. Eine mögliche Ursache könnten die Anschaffungs- und Einbindungskosten sein, für die die Anwender noch keine schnelle Amortisierung sehen. Speziell die Anbindung an eine bestehende PBX ist nicht immer einfach, da es im Markt eine Vielzahl unterschiedlicher Telefonanlagen gibt, die mit häufig stark voneinander abweichenden Prozessen aufwarten. Eine UM-Lösung, die mit einem Großteil der PBXs zusammenarbeitet, lässt sich nur schwer finden und hat natürlich einen entsprechend höheren Anschaffungspreis. Außerdem wurden in der Vergangenheit die Telefon- und E-Mail-Systeme ebenfalls von unterschiedlichen Firmen entwickelt und geliefert. Auch entstanden voneinander abweichende Protokolle und Prozesse, was zu einer zusätzlichen Komplexität beim Integrationsprozess führte.   Das große Potenzial hinter UM, aber zugleich die damit verbundene Notwendigkeit einer Konsolidierung der Nachrichtensysteme, erkannte auch  Microsoft. Deshalb integrierte das Softwareunternehmen mit Unified Messaging eine neue Serverfunktion in den Exchange Server 2007. Mit seiner 2003er-Linie konzentrierte sich der Hersteller noch auf Benutzeranforderungen wie E-Mail, Terminplanung, Webkonferenzen, Anwesenheits- und Verfügbarkeitsanzeige sowie Instant Messaging für Unternehmen. Der neue Nachrichtenserver baut nun auf diesen Merkmalen auf und soll die Anwender zusätzlich bei der Einhaltung rechtlicher Vorgaben unterstützen. Der Kern der Erweiterungen bezieht sich jedoch auf die Kommunikationsmethodik, die Planung und Durchführung von Meetings sowie die Anbindung von Soft- und IP-Telefonen. Durch die Einführung neuer Audio-, Video- und Webkonferenzfunktionen will  Microsoft die Effizienz dieser Prozess steigern.   Alles auf ein Postfach. Einer der wesentlichen Vorteile des neuen Nachrichtenservers besteht in der Konsolidierung der verteilten Voice-Mail-Systeme. Mit herkömmlichen Anlagen erfordert jeder Standort sein eigenes Voicemail-System, das in den jeweiligen Nebenstellenanlagen vor Ort integriert ist – ein hoher administrativer Aufwand. Wie bereits die Postfächer in Exchange 2003, so zentralisiert nun Exchange 2007 UM das Voicemail-System in einem Server. Das soll auch dann funktionieren, wenn weiterhin jeder Standort mit seiner eigenen PBX arbeitet. Dazu nutzt die UM-Funktion von Exchange das Active Directory (AD).   Neben dem Postfachserver, dem Clientzugriffsserver, dem Hub-Transport-Server und dem Edge-Transport-Server ist UM eine weitere eigenständige Serverrolle. Diese Serverfunktion arbeitet als Server auf mittlerer Stufe, der die Verbindung zwischen einer PBX-(Private Branch Exchange; private Telefonvermittlungsstelle) und dem Exchange-2007-Server herstellt. Zusätzlich zu den bereits bekannten E-Mail-Funktionen bietet der Nachrichtenserver nun den Faxempfang über T.38, einen ins Postfach integrierten Anrufbeantworter und Sprachzugriff auf die Postfächer. Der Schwerpunkt liegt dabei auf dem möglichst allumfassenden Zugriff auf Nachrichten (Bild 1). Die UM-Funktion in Exchange 2007 ist jedoch kein Ersatz für einen Faxserver (das Versenden von Faxen ist in dieser Version noch nicht vorgesehen, hierzu ist weiterhin die Nutzung eines zusätzlichen Tools erforderlich) oder eine IP-Telefonanlage. Die Software selbst enthält keinen Verzeichnisserver, der VoIP-Teilnehmer miteinander verbindet. Sie verfügt auch nicht über Gateways, die den Übergang ins normale Telefonnetz ermöglichen. Diese Dienste stellt beispielsweise der Office Communication Server bereit, der auch Telefoniefunktionen ermöglicht. Allerdings spricht Exchange 2007 UM VoIP over TCP und lässt sich damit als Endstelle an eine bestehende VoIP-Anlage anschließen. Für die Nutzung der UM-Funktion des Nachrichtenservers muss also im Unternehmen zumindest eine rudimentäre VoIP-Infrastruktur existieren.   Der Administrator kann Exchange 2007 UM entweder an eine traditionelle PBX oder an eine IP-PBX anschließen. Die Verbindung des UM-Servers mit einer IP-PBX (zum Beispiel  Siemens, Nortel,  Avaya, Mitel oder NEC) funktioniert direkt, da beide über das SIP (Session Initiation Protocol) miteinander kommunizieren. Lediglich eine zusätzliche Standard-NIC-Karte im UM-Server ist erforderlich.   Vielfach setzen die Unternehmen aber noch auf herkömmliche Telefonanlagen. Die Anbindung an den UM-Server nimmt der Administrator über ein zusätzliches VoIP- und FoIP (Fax over IP)-Gateway vor, das leitungsvermittelnde Protokolle wie TDM (Time Division Multiplexing) in paketvermittelnde Protokolle wie SIP (und zurück) übersetzt. Diese Gateways unterstützen die meisten der herkömmlichen Telefonanlagen und sind heute relativ preisgünstig.  Microsoft hat bereits einige der Produkte ( Intel, Dialolgic,  AudioCodes) auf Kompatibilität getestet. (Eine Liste kompatibler IP-/VoIP-Gateways ist auf der Webseite   http://technet.microsoft.com/de-de/library/bb123948.aspx  [http://technet.microsoft.com/de-de/library/bb123948.aspx] zu finden. Die PBXs stehen auf   http://technet.microsoft.com/de-de/library/aa996831.aspx  [http://technet.microsoft.com/de-de/library/aa996831.aspx]). Jedes VoIP-Gateway wird später in AD als UM-IP-Gateway abgebildet. W   ährend diese Geräte an den Standorten der jeweiligen PBXs installiert sind, befindet sich der UM-Server in nächster Nähe zur Hauptinfrastruktur von Exchange. Er kommuniziert mit den anderen Exchange-Server-Rollen und speichert die Nachrichten zentral auf dem Postfachserver. Da der UM-Server mit beiden Arten von PBXs kommunizieren kann, ist aus Sicht von  Microsoft kein Umstieg auf VoIP-Systeme erforderlich. Diese Entscheidung sollte ein Unternehmen allein aus strategischen Überlegungen heraus treffen und wenn der Umstieg aus geschäftlichen Gründen sinnvoll ist.   AD übernimmt die Kontrolle. Die Konsolidierung von E-Mail und UM-Funktionen in Exchange 2007 bietet dem Administrator einen Vorteil hinsichtlich der Verwaltung seiner Benutzer. Er muss sich nur noch um eine einzige Verzeichnisinfrastruktur kümmern. AD verwaltet nun nicht nur die E-Mail-Konten sondern auch die Postfächer für Sprach- und Faxnachrichten. Deshalb kommt den Active-Directory-Objekten eine besondere Bedeutung zu, da sie nicht mehr nur den E-Mail-Betrieb sondern auch die Zuordnung von Sprach- und Faxinformationen steuern. Dabei verbinden die Active Directory-UM-Objekte die Telefoninfrastruktur mit der AD-Umgebung von Exchange Server 2007 UM und ermöglichen damit erst die Integration dieser beiden autarken Systeme. AD arbeitet in diesem Zusammenhang als Container für alle erstellten UM-Objekte, inklusive deren Konfiguration. Der Exchange Server 2007 benötigt jedes einzelne UM-Objekt, um Unified Messaging in einer AD-Umgebung zu unterstützen. Die UM-AD-Objekte haben unterschiedliche Aufgaben (Bild 2): Einige von ihnen dienen der logischen Darstellung von Telefongeräten, andere bilden den Telefonwählplan einer Organisation ab oder unterstützen bestimmte UM-Funktionen des Exchange 2007 Servers. Zwischen den UM-AD-Objekten und den Funktionen des UM-Servers bestehen daher integrierte und vernetzte Beziehungen, die der Administrator wie beim Aufbau jeder anderen AD-Struktur vorab genau analysieren und definieren muss. Nur dann wird die Bereitstellung einer UM-Infrastruktur auf Basis von Exchange 2007 erfolgreich sein.   Darauf müssen Administratoren achten. Die Administratoren haben bei der Migration oder Neueinführung des Exchange Server 2007 einige wichtige Aspekte zu beachten. Der Exchange Server 2007 Unified Messaging-Server muss Mitglied einer Domäne sein, bevor die Serverfunktion UnifiedMessaging für ein neues UM-Computerobjekt installiert wird. Diese UM-Computerobjekte stellen die Verbindung zwischen der Telefoninfrastruktur und der Exchange Server 2007 UM-AD-Netzwerkumgebung her und sind gleichzeitig eine logische Abbildung des physikalischen Servers, auf dem die UM-Serverfunktion installiert ist. Der Installationsvorgang richtet die UM-Computerobjekte automatisch in AD ein. Nach der Installation der UM-Serverfunktion befindet sich der UM-Server standardmäßig im aktivierten Status. Diesen Zustand kann der Administrator in der Exchange-Verwaltungskonsole mit den beiden Befehlen „Enable-UMServer“ und „Disable UMServer“ steuern.   Damit der UM-Server überhaupt seine Arbeit aufnehmen kann, muss der Administrator mindestens einen so genannten Wählplan erstellen, mit dem sowohl der UM-Server als auch das IP-Gateway verknüpft sind. Er bildet die zentrale Komponente des UM-Systems, ist in AD integriert und gilt für die gesamte Exchange 2007-Organisation. Der UM-Wählplan ist ein AD-Containerobjekt, das die Gruppierungen der Nebenstellenanlagen logisch abbildet und damit auch die von ihnen gemeinsam genutzten Durchwahlnummern der Benutzer (Telefonwählplan). Diese Information ist für den UM-Server von elementarer Bedeutung, da er nur über diesen Wählplan die auf den PBX-Anlagen gehosteten Nebenanschlüsse der Benutzer richtig zuordnen kann. Geht ein Anruf auf einem IP/VoIP-Gateway ein, wird dieser Anruf an den UM-Server weitergeleitet. Dieser versucht anhand des dazugehörigen UM-Wählplans die Durchwahlnummer des Benutzers zu ermitteln und verarbeitet den Anruf anschließend anhand der vom Benutzer oder Administrator getroffenen Einstellungen weiter.   Zusammen mit dem Wählplan erstellt das System eine UM-Postfachrichtlinie. Sie wird intern als <DialPlanName>-Standardrichtlinie bezeichnet. Gibt der Administrator nach dem Erstellen des ersten UM-IP-Gateways keinen UM-Wählplan an, richtet das System zusätzlich einen UM-Standardsammelanschluss in AD ein.   Die Wählplanverknüpfung passt sich den Gegebenheiten der jeweiligen Organisation an. Der Administrator kann einen UM-Server mit einem oder mehreren Wählplänen verknüpfen oder auch mehrere UM-Server mit nur einem oder mehreren Wählplänen. Nach dem Start des UM-Servers sucht dieser alle IP/VoIP-Gateways, die den UM-Wählplänen und dem UM-Server zugeordnet sind. Damit er auch mögliche Änderungen an der Konfiguration von UM-Wählplänen oder UM-IP-Gateways erkennen und bestimmen kann, überprüft er alle 10 Minuten die Konfiguration oder registriert die Änderungsbenachrichtigungen.   Steuerung per Sprache. Da der Exchange Server 2007 UM unter anderem eine Sprachsteuerung verwendet, ist die Installation der richtigen Sprachversion eine wesentliche Grundvoraussetzung für die problemlose Funktion. Dafür stellt  Microsoft so genannte UM-Sprachpakete zur Verfügung. Jedes dieser Sprachpakete umfasst ein TTS (Text-to-Speech)-Modul sowie vorab aufgezeichnete Ansagen in der betreffenden Sprache. Die UM-Sprachpakete gibt es in 16 Sprachen, die alle auf der Exchange 2007-DVD enthalten sind.   Allerdings unterstützt derzeit nur das englische Sprachpaket die automatische Spracherkennung (Automatic Speech Recognition, ASR). Für andere Sprachpakete ist ASR in der Planung. In mehrsprachigen Umgebungen muss der Administrator eventuell zusätzliche UM-Sprachpakete installieren und dem jeweiligen Benutzer zuordnen. Darüber hinaus ist dies auch für die Wiedergabe fremdsprachiger Informationen erforderlich, da sonst der Text anderssprachiger Nachrichten nicht korrekt per TTS wiedergeben wird. Die TTS-Funktionen stellt der  Microsoft Speech Server Dienst bereit. Seine Informationen holt er aus den Betreffangaben, dem Namen und dem Nachrichtentext von E-Mail- und Voice-Mail-Nachrichten. Zusätzlich zu diesen Daten entnimmt er den Kalenderelementen noch die Ortsangaben, die Namen der persönlichen Kontakte und die Voice-Mail-Standardbegrüßung eines Benutzers. Das TTS-Modul liest den geschriebenen Text und konvertiert ihn direkt in eine für den Anrufer hörbare Ausgabe.   Jeder neu erstellte UM-Wählplan enthält seine eigene Standardspracheinstellung, mit der die TTS-Umwandlung oder eine standardmäßige Audioansage für Outlook Voice Access-Benutzer wiedergegeben wird. Der Administrator muss nicht zwingend eine Standardsprache für den Wählplan einstellen, da das System die Standardsprache jedes neuen Wählplans aufgrund der installierten Sprachversion von Exchange 2007 konfiguriert. Das gilt auch für lokalisierte Exchange-Versionen.   Die richtige Einstellung der Standardsprache ist auch für den Anrufer wichtig. Wählt sich ein OVA-Benutzer in das UM-System ein, verwendet das System jene Sprache, die in Outlook Web Access (OWA) konfiguriert ist. Diese wurde zu dem Zeitpunkt festgelegt, als sich der Benutzer erstmals mithilfe von OWA an seinem Postfach anmeldete. Der UM-Server vergleicht die in OWA festgelegte Sprache mit der Liste der im Wählplan verfügbaren Sprachen. Findet er keine passende Entsprechung, verwendet er die Standardsprache des UM-Wählplans. Manchmal muss der Administrator diese als Standardsprache festlegen. Enthält der Wählplan beispielsweise ausschließlich deutschsprachige Benutzer, kann der Administrator dessen Standardspracheinstellung in Deutsch ändern.   Mithilfe von Exchange 2007 UM können die Benutzer in der Exchange 2007-Organisation ihre gesamten E-Mail-, Sprach- und Faxnachrichten in einem einzigen Postfach empfangen, was die Produktivität enorm steigert. Der Administrator erstellt dazu für einen Benutzer entweder ein neues Postfach oder stellt eine Verbindung zu einem bereits vorhandenen Postfach her und aktiviert dieses für UM. Das System übermittelt die für diesen Benutzer eintreffenden E-Mail-, Sprach- und Faxnachrichten aufgrund des Wählplans nur noch an diesen Posteingang des Benutzers (Bild 3). Dem Anwender selbst bieten sich nun mehrere Schnittstellen für den Zugriff auf sein Postfach: Outlook 2007, Outlook Web Access, ein für Exchange ActiveSync aktiviertes mobiles Gerät, ein Festnetz- oder ein Mobiltelefon.   Die Anwender greifen mithilfe ihrer für den UM-Wählplan konfigurierten Teilnehmerzugriffsnummer auf den UM-Server zu. Hier unterscheidet das System zwei Arten von Anwendern: authentifizierte und nicht authentifizierte Anrufer (Bild 4). Wählt ein Anwender seine für einen Wählplan konfigurierte Teilnehmerzugriffsnummer, stuft ihn Exchange UM zunächst als anonym und nicht autorisiert ein. Erst wenn er weitere Informationen wie zum Beispiel seine Voice-Mail-Durchwahl und die zugehörige PIN eingibt wird er als autorisiert anerkannt. Nicht autorisierte Benutzer dürfen lediglich die Verzeichnissuchfunktion nutzen. Nur die autorisierten Anwender erhalten Zugriff auf ihr Postfach und die Funktion Outlook Voice Access (OVA). Diese UM-Funktion besteht aus einer Reihe von Sprachansagen, über die der autorisierte Anrufer auf seine E-Mail-, Voice-Mail- oder Faxnachrichten sowie auf seinen Outlook-Kalender und andere Informationen zugreift. Die jeweiligen Anwenderoperationen wie die Navigation durch die persönlichen Informationen, das Einleiten von Anrufen oder das Suchen anderer Benutzer ermöglicht das System entweder über die MFV-Tasten des Telefons oder über gesprochene Befehle.   Somit bietet dieses Zusammenspiel der einzelnen Funktionen von E-Mail-, Sprach- und Faxsystemen nicht nur dem Administrator deutliche Vorteile, sondern stellt auch für den Endanwender eine wesentliche Erleichterung bei der Bewältigung der täglichen Nachrichtenflut dar. •   Konradin Verlag Robert Kohlhammer GmbHDocument WINMG00020071112e3bc00005"
"23",2018-01-20," Artificial Intelligence Market to Rise at Spectacular CAGR of 36.10% During 2016-2024, Players in End-use Industries Leverage its Potential...wallstreet:online, 12:30, 16 August 2018, 1027 words, (German)(Document WC67106020180816ee8g002eg)"," Artificial Intelligence Market to Rise at Spectacular CAGR of 36.10% During 2016-2024, Players in End-use Industries Leverage its Potential..."
"24",2018-01-20," Spitch Befragung zeigt hohen Bedarf an Sprachtechnologien bei Schweizer BankenMoneycab.com, 17:03, 16 August 2018, 476 words, (German)(Document WC56730020180816ee8g001xi)"," Spitch Befragung zeigt hohen Bedarf an Sprachtechnologien bei Schweizer Banken"
"25",2018-08-13,"KI-Konferenz für Software Developer","KI-Konferenz für Software Developer238 words13 August 2018Elektronikpraxis OnlineELEKTWGermanCopyright 2018. Vogel Communications Group GmbH & Co. KG Unter dem Motto „People – Code – AI“ hält WeAreDevelopers eine Konferenz zu Künstlicher Intelligenz in Wien ab. Namhafte Entwickler und IT-Experten berichten dort über aktuelle Trends in den Bereichen Machine Learning und Artificial Intelligence.Mit dem WeAreDevelopers AI Congress hat die Entwickler-Plattform ein weiteres Event speziell für App- und Anwendungsentwickler ins Leben gerufen. Rund 2.000 Developer und IT-Experten sollen sich am Dienstag und Mittwoch, 4. und 5. Dezember 2018, im Kongress- und Veranstaltungszentrum Hofburg einfinden.Kernthemen der Konferenz sind Machine Learning und Künstliche Intelligenz sowie die jüngsten Errungenschaften und Entwicklungen hinsichtlich der Interaktion zwischen Mensch und Maschine. Ein besonderer Fokus liegt dabei auf der Vertrauensbildung gegenüber maschinellen Entscheidungen sowie der Verbesserung der Nutzererfahrung mit maschineller Lernsoftware, unterstreicht Benjamin Ruschin, Managing Director und Co-Founder von WeAreDevelopers.Die zweitägige Konferenz umfasst 32 Experten-Vorträge und Panels sowie 16 Workshops, die explizit auf die Interessen von Entwicklern zugeschnitten sind. „Machine Learning basiert auf den Säulen Vision, Sound & Speech Recognition, Natural Language Processing (NLP) sowie Data Science und Analytics“, sagt Ruschin mit Blick auf die Konferenzplanung. „Alle vier Bereiche werden ausführlich in diversen Experten-Vorträgen und Diskussionsrunden aufgegriffen.“Neben den Entwicklern erwartet WeAreDevelopers auch die Teilnahme von über 300 IT-Experten auf Management-Ebene. Alle Teilnehmer können sich auf dem AI-Kongress fachspezifisch miteinander austauschen und vernetzen.Dieser Beitrag stammt von unserem .Der WeAreDevelopers AI Congress steht unter dem Motto „People – Code – AI“.Vogel Communications Group GmbH & Co.KGDocument ELEKTW0020180813ee8d00004"
"26",2018-03-20," Voice Biometric Solutions Market to be Worth US$ 13,049.2 Mn by 2026: Transparency Market Researchwallstreet:online, 12:30, 3 August 2018, 913 words, (German)(Document WC67106020180803ee83002pp)"," Voice Biometric Solutions Market to be Worth US$ 13,049.2 Mn by 2026: Transparency Market Research"
"27",2018-01-20,"IFA 2018 - KI-Sprachassistent von Snips schützt die Privatsphäre nun auch auf Deutsch / Plattform ermöglicht die Entwicklung sicherer Sprachsteuerung für beliebige Geräte","IFA 2018 - KI-Sprachassistent von Snips schützt die Privatsphäre nun auch auf Deutsch / Plattform ermöglicht die Entwicklung sicherer Sprachsteuerung für beliebige Geräte574 words1 August 2018news aktuell OTS - Originaltextservice SchweizOTSCHDGerman© 2018 news aktuell schweiz Paris/Berlin (ots) - Snips, KI-Start-up mit Fokus auf Privatsphäre und Datensicherheit, wird auf der IFA 2018 in Berlin die Spracherkennungsplattform des Unternehmens inklusive Schlüsselworterkennung, Verständnis natürlicher Sprache und Dialog-Management-Funktionen für die deutsche Sprache zeigen. Die Plattform ermöglicht es Geräteherstellern und Entwicklern, ihre Produkte mit leistungsfähigen Sprachassistenten auszustatten. Snips zeichnet sich durch eine Cloud-unabhängige Funktion aus, so dass die Sicherheit der Anwenderdaten garantiert wird. Demonstriert werden konkrete Anwendungsbeispiele.Aufbauend auf einer selbst entwickelten KI-Technologie funktioniert die Snips-Lösung unabhängig von der Cloud direkt auf dem jeweiligen Gerät. Dies garantiert im Gegensatz zu heute populären Online-Assistenten die Datensicherheit im Sinne der DSGVO, geringe Verzögerungen sowie die Kontrolle über die Markenidentität und Nutzerdaten.Snips ist die erste auf künstlicher Intelligenz basierende Sprachlösung, die vollständig lokal auf dem jeweiligen Gerät abläuft. Sie bietet sowohl die automatische Spracherkennung (Automatic Speech Recognition - ASR) als auch das Verständnis natürlicher Sprache (Natural Language Understanding - NLU). Hinzu kommen die Erkennung von Schlüsselwörtern sowie das Dialogmanagement. Entwickler können die Snips Web-Konsole nutzen, um einfach Prototypen von Sprachassistenten zu erstellen. Die Nutzung der Konsole ist kostenfrei und bietet die Möglichkeit, vorgefertigte Funktionen aus einem Developer-Marketplace einzubinden. Kosten fallen erst bei der professionellen Nutzung von Snips für OEM-Lösungen an.Auf der IFA werden beispielhafte Sprachlösungen auf Basis der Snips-Technologie für die Sprachsteuerung einer Kaffeemaschine, eines 3D-Druckers sowie einer Music-Box gezeigt. Snips kommt in unterschiedlichsten Bereichen wie etwa bei Hausgeräten und Connected-Home, im Handel oder in Kraftfahrzeugen zum Einsatz. Die Vision von Snips ist es, einen KI-Assistenten in jedes Gerät einzubetten und damit Technologie so intuitiv zu machen, dass sie in den Hintergrund tritt. Gegenwärtig wird die im Juni 2017 vorgestellte Plattform von mehr als 14.000 Entwicklern genutzt, die damit bereits mehr als 24.000 Sprachassistenten erstellt haben.""Die heute verbreiteten Sprachassistenten zeigen, was am Internet gegenwärtig falsch läuft: die Zentralisierung persönlicher Daten, der Verlust an Privatsphäre und die Ausnutzung von Anwendern und Entwicklern"", erklärt Rand Hindi, Mitbegründer und CEO von Snips. ""Gerade in Deutschland hat der Datenschutz einen hohen Stellenwert. Mit unserer Plattform bieten wir den heimischen und internationalen Herstellern die Möglichkeit, leistungsfähige und sichere Sprachassistenten in ihre Produkte zu integrieren.""Snips zeigt die deutsche Version des Sprachassistenten erstmals auf der IFA 2018 in Berlin einem breiten Publikum in Halle 26, Stand 370-371.Weitere Informationen zu Snips finden sich unter https://snips.ai/[https://snips.ai/]Über Snips:Im Jahr 2013 von drei PhDs gegründet, ist es die Vision von Snips, einen auf künstlicher Intelligenz basierenden Sprachassistenten in jedes Gerät einzubetten und damit Technologie so einfach zu machen, dass sie im Hintergrund verschwindet. Snips bietet die Technologie des Voice-Assistenten als White-Label-Lösung für Device-Hersteller an. Snips unterscheidet sich von anderen Sprachlösungen dadurch, dass sie vollständig auf dem Gerät abläuft, ohne Daten in die Cloud zu senden. Die Lösung ist 'Private-by-Design' und damit konform zu den Richtlinien der DSGVO. Snips ist ein etabliertes Unternehmen mit über 60 Mitarbeitern in Paris und New York und hat bisher eine Finanzierung in Höhe von 22 Millionen Euro erhalten.Originaltext: Snips Digitale Medienmappe: http://www.presseportal.ch/de/nr/100065213[http://www.presseportal.ch/de/nr/100065213] Medienmappe via RSS : http://www.presseportal.ch/de/rss/pm_100065213.rss2[http://www.presseportal.ch/de/rss/pm_100065213.rss2]Kontakt: Pressekontakt: PIABO PR GmbH Uwe Scholz +49 172 298 81 14 snips@piabo.net100818441news aktuell GmbHDocument OTSCHD0020180801ee81000rt"
"28",2018-01-20,"IFA 2018 - KI-Sprachassistent von Snips schützt die Privatsphäre nun auch auf Deutsch","IFA 2018 - KI-Sprachassistent von Snips schützt die Privatsphäre nun auch auf Deutsch572 words1 August 201811:33Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS"" Plattform ermöglicht die Entwicklung sicherer Sprachsteuerung für beliebige GeräteParis/Berlin - Snips, KI-Start-up mit Fokus auf Privatsphäre und Datensicherheit, wird auf der IFA 2018 in Berlin die Spracherkennungsplattform des Unternehmens inklusive Schlüsselworterkennung, Verständnis natürlicher Sprache und Dialog-Management-Funktionen für die deutsche Sprache zeigen. Die Plattform ermöglicht es Geräteherstellern und Entwicklern, ihre Produkte mit leistungsfähigen Sprachassistenten auszustatten. Snips zeichnet sich durch eine Cloud-unabhängige Funktion aus, so dass die Sicherheit der Anwenderdaten garantiert wird. Demonstriert werden konkrete Anwendungsbeispiele.Aufbauend auf einer selbst entwickelten KI-Technologie funktioniert die Snips-Lösung unabhängig von der Cloud direkt auf dem jeweiligen Gerät. Dies garantiert im Gegensatz zu heute populären Online-Assistenten die Datensicherheit im Sinne der DSGVO, geringe Verzögerungen sowie die Kontrolle über die Markenidentität und Nutzerdaten.Snips ist die erste auf künstlicher Intelligenz basierende Sprachlösung, die vollständig lokal auf dem jeweiligen Gerät abläuft. Sie bietet sowohl die automatische Spracherkennung (Automatic Speech Recognition - ASR) als auch das Verständnis natürlicher Sprache (Natural Language Understanding - NLU). Hinzu kommen die Erkennung von Schlüsselwörtern sowie das Dialogmanagement. Entwickler können die Snips Web-Konsole nutzen, um einfach Prototypen von Sprachassistenten zu erstellen. Die Nutzung der Konsole ist kostenfrei und bietet die Möglichkeit, vorgefertigte Funktionen aus einem Developer-Marketplace einzubinden. Kosten fallen erst bei der professionellen Nutzung von Snips für OEM-Lösungen an.Auf der IFA werden beispielhafte Sprachlösungen auf Basis der Snips-Technologie für die Sprachsteuerung einer Kaffeemaschine, eines 3D-Druckers sowie einer Music-Box gezeigt. Snips kommt in unterschiedlichsten Bereichen wie etwa bei Hausgeräten und Connected-Home, im Handel oder in Kraftfahrzeugen zum Einsatz. Die Vision von Snips ist es, einen KI-Assistenten in jedes Gerät einzubetten und damit Technologie so intuitiv zu machen, dass sie in den Hintergrund tritt. Gegenwärtig wird die im Juni 2017 vorgestellte Plattform von mehr als 14.000 Entwicklern genutzt, die damit bereits mehr als 24.000 Sprachassistenten erstellt haben.""Die heute verbreiteten Sprachassistenten zeigen, was am Internet gegenwärtig falsch läuft: die Zentralisierung persönlicher Daten, der Verlust an Privatsphäre und die Ausnutzung von Anwendern und Entwicklern"", erklärt Rand Hindi, Mitbegründer und CEO von Snips. ""Gerade in Deutschland hat der Datenschutz einen hohen Stellenwert. Mit unserer Plattform bieten wir den heimischen und internationalen Herstellern die Möglichkeit, leistungsfähige und sichere Sprachassistenten in ihre Produkte zu integrieren.""Snips zeigt die deutsche Version des Sprachassistenten erstmals auf der IFA 2018 in Berlin einem breiten Publikum in Halle 26, Stand 370-371.Weitere Informationen zu Snips finden sich unter https://snips.ai/[https://snips.ai/]Über Snips:Im Jahr 2013 von drei PhDs gegründet, ist es die Vision von Snips, einen auf künstlicher Intelligenz basierenden Sprachassistenten in jedes Gerät einzubetten und damit Technologie so einfach zu machen, dass sie im Hintergrund verschwindet. Snips bietet die Technologie des Voice-Assistenten als White-Label-Lösung für Device-Hersteller an. Snips unterscheidet sich von anderen Sprachlösungen dadurch, dass sie vollständig auf dem Gerät abläuft, ohne Daten in die Cloud zu senden. Die Lösung ist 'Private-by-Design' und damit konform zu den Richtlinien der DSGVO. Snips ist ein etabliertes Unternehmen mit über 60 Mitarbeitern in Paris und New York und hat bisher eine Finanzierung in Höhe von 22 Millionen Euro erhalten.Rückfragehinweis:   Pressekontakt:   PIABO PR GmbH   Uwe Scholz   +49 172 298 81 14   snips@piabo.netDigitale Pressemappe: http://www.ots.at/pressemappe/DE131371/aom[http://www.ots.at/pressemappe/DE131371/aom]*** OTS-ORIGINALTEXT PRESSEAUSSENDUNG UNTER AUSSCHLIESSLICHER INHALTLICHER VERANTWORTUNG DES AUSSENDERS - WWW.OTS.AT ***OTS0082-20180801APA - Austria Presse Agentur eGDocument AUPAG00020180801ee81003s5"
"29",2018-02-20," NICE Adds Automated Cloud-Based Transcription Service to NICE COMPASS Compliance Assurance Suitewallstreet:online, 12:30, 24 July 2018, 1116 words, (German)(Document WC67106020180724ee7o002jx)"," NICE Adds Automated Cloud-Based Transcription Service to NICE COMPASS Compliance Assurance Suite"
"30",2002-04-29,"Using Speech Recognition.","Using Speech Recognition.2235 words29 April 2002Basler ZeitungBASLRZGerman(c) 2002 Basler ZeitungHomepage Address: http://www.baz.chUsing Speech RecognitionWith the English Speech Recognition package and theproper hardware, your computer can respond to spoken commands. The EnglishSpeech Recognition package contains the Speech Recognition extension and autility called Speakable Items.This document explains how to set up and usespeech recognition and the Speakable Items software. What is SpeakableItems?Speakable Items is a utility that lets you speak commands to yourcomputer. It enables the computer to understand commands that do such thingsas close windows, tell what time it is, open applications and folders, ortell a joke. Speakable Items comes with a set of commands (ÊspeakableitemsË) the computer already understands. You can also create your ownspeakable items.The Speakable Items software has three parts you'll ofteninteract with: the Speech control panel (also used with Apple'stext-to-speech software), the feedback window, and the Speakable Itemsfolder. With the Speech control panel, you turn Speakable Items on and offand specify when the computer listens and how it gives you feedback. Withthe feedback window (which is open whenever Speakable Items is turned on),you check whether the computer is listening and how it has responded to yourcommand. The 'Esc' below the character in the following illustration of thefeedback window indicates that the current listening method preferencerequires you to hold the Escape key down as you speak each command. Thisoption can be changed, as discussed in the ""Determining how the computerlistens"" section, below. The command you give is displayed in italics,followed by the computer's response. The Speakable Items folder contains allthe items whose names the computer can recognize. The Speakable Items folderis in the Apple Menu Items folder, which is in the System Folder. You canopen it quickly by choosing the Speakable Items command from the Apple (ö)menu. When you say the name of an item in this folder, the computer acts asif you double-clicked its icon. You can adjust the computer's vocabulary byadding and deleting items to this folder or by changing the names of theitems. (NOTE: Speakable Items currently ignores numbers and punctuation. Youcan spell those words out - for example, using a name like ""open draft threepoint four"" instead of ""open draft 3.4"". Or you can re-name speakable itemsso they do not contain punctuation or numbers - for example, creating analias named ""open draft"" to a document named ""draft 3.4"" and adding thealias to the Speakable Items folder. Or you can simply leave the numbers andpunctuation in the speakable item names, but not speak them - for example,say ""open draft"" to open an item named ""open draft 3.4"".) What you need touse Speech RecognitionTo use speech recognition, you need these pieces ofhardware and software: a Power Macintosh computer system software version7.5 or later a 16-bit microphone, such as the Apple PlainTalk Microphone orthe microphone built into some Apple audio-visual monitorsIt is also a goodidea to install the English Text-to-Speech software, if it is not alreadyinstalled, so you can hear spoken replies to many of your commands.Preparingthe microphoneIf you don't have an audio-visual monitor with a built-inmicrophone, you must plug in an Apple PlainTalk Microphone or similarmicrophone into the microphone jack on your computer. Position themicrophone as follows: Center the microphone on top of your monitor (ifpossible), with the microphone's Apple (ö) icon facing you. Make sure thatthe microphone is between 1 and 3 feet directly in front of you when you'regiving spoken commands. Adjust the microphone cord so it is not loopedaround itself or other cords. Preparing the sound softwareTo make sure themicrophone is turned on and set up correctly, you must use the Sound controlpanel or the Sound & Displays controls panelÑwhichever you haveÑto selectyour microphone as the sound input source:1. Open the Control Panels windowby pulling down the Apple (ö) menu and choosing Control Panels.2. Open theSound (or Sound & Displays) control panel by double-clicking its icon in theControl Panels window.3. Use the control panel to select your sound inputsource: If you have the Sound control panel, click the Options button. Inthe Options dialog box that opens, click Microphone; or, if you are usingthe microphone built into an AudioVision 14 Display, click AV Connectorinstead. If you have the Sound & Displays control panel, click an item inthe Sound In list to indicate which source you're using. Note: If you get anerror message while you're using the Sound (or Sound & Displays) controlpanel, you may need to turn off the speech recognition software (if you'vealready installed it). To do so, open the Speech control panel and chooseSpeakable Items from the Options pop-up menu, then click Off. When you'refinished setting the sound options, you can turn speech recognition backon.4. Close the Sound (or Sound & Displays) control panel.Installing theSpeech Recognition softwareYou install the speech recognition software (andthe Speakable Items utility) by using the Installer programs, as describedin these steps:1. You may want to print these instructions before you start,because you won't be able to view them on screen during the procedure.2.Quit any programs you are running.3. While holding down the Shift key on thekeyboard, choose Restart from the Special menu. Don't release the Shift keyuntil you see the ÊExtensions OffË message on your screen.4. To start theInstaller program, double-click its icon. You can find the Installer icon inthe same place you found these instructions.5. In the initial screen thatappears, click Continue.6. In the next screen, click Install. Theinstallation takes a while.7. When you see a message that installation wassuccessful, click Restart.8. On most computers, after the computer restarts,you will notice that speech recognition is on and you will see two windowson the screen at once: the feedback window and an Apple Guide windowcontaining the first panel of a brief introduction to speech recognition.(Note: You may also see a message telling you the standard microphone is notselected. If so, you need to click OK to dismiss that message, turnSpeakable Items off using the Speech control panel, and check your soundinput settings, as described in ÊPreparing the Sound Software,Ë above.) Togo through the introduction, follow the instructions in the Apple Guidewindow.You can turn Speakable Items off and on using the Speech controlpanel, in the Speakable Items option.Speaking commandsTo make the computerrespond to your spoken commands, follow these steps: Make sure the computeris listening and that, while you are speaking the command, you are holdingdown the key displayed under the character in the feedback window. When youfirst install the software, it's set up so the computer only listens whileyou hold down the escape ('esc') key. You can change the key, or choose tohave the computer listen all the time instead of only while a key is helddown. These other listeneing method options are described in ÊDeterminingHow the Computer Listens,Ë below. Speak clearly. Wait for a moment ofsilence, then speak in a normal tone of voice at a normal rate of speed. Formore information, see ÊTips on Speaking to Your Computer,Ë below. Use aspoken command that the computer understands. The computer can understandthe name of any item in the Speakable Items folder.Determining how thecomputer listensThe computer recognizes spoken commands only when it islistening for them. You use the Speech control panel to determine which ofthese listening methods it will use: The computer listens only while you areholding down the listening key. This method, which we sometimes call the""push-to-talk"" method, is the default. It is recommended because it is themost reliable method. When using the push-to-talk method, misfires due toconversations and background noise are completely eliminated. The computerlistens all the time, but it tries to only recognize commands preceded withits name (like ""computer, what time is it?""). The default name is""Computer"", but you can change the name. (This method allows you to speakcommands without holding a key down, but is less reliable than the""push-to-talk"" method, because the computer may occassionally misinterpretconversation or other sounds as commands.) The computer listens all the timefor commands, without requiring you to hold a key down or precede commandswith ""Computer"". (This is the least reliable method, because the computermay frequently misinterpret conversation or other sounds as commands.)Whenthe Speakable Items software is first installed, it is set up so thecomputer will listen only while you hold the Escape key down ('esc'). If youwant to change it to another method, follow these steps:1. Open the ControlPanels window by pulling down the Apple (ö) menu and choosing ControlPanels.2. Open the Speech control panel by double-clicking its icon in theControl Panels window.3. Open the Options pop-up menu and chooseListening.4. Within the Listening option, you can choose a listening key bypressing a key. You may combine the key you choose with any combination ofthe Shift, Option, or Control keys. Do not choose a key or key combinationthat you use in any of your application programs. If accepted, the listeningkey you choose appears in the Key(s) box.5. Click the button labeled ÊListenonly while key(s) are pressedË or the button labeled ""Key(s) togglelistening on and off"". The control panel will look like this: 6. If youchoose the ""Key(s) toggle listening on and off"" radio button, then you canchange the name that is to precede each spoken command. (The default is""Computer"".) You can also use the ""Name is:"" popup menu to choose to requirethat name before each command, to make it optional before each command, orto make it optional for a while after it is used with a successfulcommand.For more information, you may be able to see ÊHow do I tell thecomputer when to listen?Ë in the Speech topic area of Macintosh Guide,available in the Guide menu on some Macintosh systems.Tips on speaking toyour computerWhen you talk to your computer, keep the following tips inmind: Speak at a normal volume. Don't shout or speak loudly. Speaknaturally. Don't exaggerate the pronunciation of words. Leave a littlesilence before giving a command. If there are no sound waves drawn next tothe feedback character (and you are holding the push-to-talk key down, ifthat is the current listening method), the computer is ready to hear thecommand. Speak the name of items in the Speakable Items folder only. (If youspeak a phrase that does not correspond to the name of an item in theSpeakable Items folder, Speakable Items might mistakenly recognize yourutterance as one of those item names anyway.)Finding out more aboutSpeakable ItemsYou may be able to find out more information about how to usespeech recognition by opening Macintosh Guide from the Guide (Question Mark)menu or by clicking the Question Mark button in the Speech control panel, ifthe Question Mark button is visible (indicating the Macintosh Guide Speechinformation is available).Within Macintosh Guide, check for the topicÊSpeech.Ë If it's available, explore the information in that topic area.(The Speech topic is not available on some Macintosh systems.)If you see amessage that there isn't enough memory availableIf you see a message thatthe Speech Recognizer is having trouble due to lack of memory, save all youropen documents and quit the application programs you have open. To avoidrunning out of memory while using speech recognition Use the Speech controlpanel to choose a voice that requires less memory, or turn off voicefeedback altogether. (The voices named ÊAgnes,Ë ÊBruce,Ë and ÊVictoriaË takeup the most memory.) Use the Memory control panel to turn on virtual memory.(With virtual memory on, voices may be slightly distorted and, with someprograms, speech performance may decrease.) Add more memory to yourcomputer. (If you are running System 7.5 or later on a Power Macintosh withPowerTalk, QuickDraw GX, and English speech recognition installed, then itis recommended that your computer have at least 16 megabytes of RAM.)Note onusing the speech recognition software with application programsA fewapplications may interfere with speech recognition. For example, the speechrecognition software sometimes cannot recognize spoken commands for 1015seconds after you have been typing in Microsoft Word. And speech recognitionmay not work with programs that use the microphone themselves, includingsoftware used to record and edit sound. - 1995 Apple Computer, Inc. Allrights reserved. Apple, the Apple logo, Macintosh, PlainTalk, PowerMacintosh, and PowerTalk are trademarks of Apple Computer, Inc., registeredin the U.S. and other countries. AudioVision and QuickDraw are trademarks ofApple Computer, Inc.Document baslrz0020020429dy4t00136"
"31",2018-04-20," Speech comprehension with a cochlear implantinnovations-report, 01:00, 4 June 2018, 683 words, (German)(Document WC56527020180605ee6300002)"," Speech comprehension with a cochlear implant"
"32",2018-02-20," Elsevier Collaborates With M*Modal to Launch Next-Generation Diagnostic Decision Support Workflowwallstreet:online, 19:26, 21 July 2018, 484 words, (German)(Document WC67106020180721ee7l002xr)"," Elsevier Collaborates With M*Modal to Launch Next-Generation Diagnostic Decision Support Workflow"
"33",2012-12-20,"DGAP-News: Lyrix Solution Now Rated 'Avaya Compliant'","DGAP-News: Lyrix Solution Now Rated 'Avaya Compliant'749 words12 March 201216:30DGAP FinanznachrichtenDGAPFIGermanCopyright 2012. DGAP Deutsche Gesellschaft für Ad-hoc-Publizität   Lyrix12.03.2012 15:30----------------------------------------------------------------------------- Mobiso, a set of speech recognition communication tools is compatible withkey Avaya collaboration solutions.-- Helps businesses improve mobile collaboration and productivity using voicecommands to initiate phone calls, conference calls, and send emailmessages.NASHUA, N.H., 2012-03-12 15:19 CET (GLOBE NEWSWIRE) -- Lyrix, Inc., a leadingenterprise focused Communications-as-a-Service (CaaS) company today announcedthat its Mobiso hosted speech recognition solution is compliant with keycollaboration solutions from Avaya, a global provider of business collaborationsystems, software and services.The Mobiso set of speech recognition communication tools lets users reachemployees quickly and effortlessly from their corporate directory and personalcontacts. By simply using voice commands, users can initiate phone calls,conference calls and send email messages. As a result, companies with anincreasing number of employees working from home or from their mobile devicesenable a truly 'numberless' enterprise to more easily and quickly connectcustomers with employees and mobile workers. The success of speech in themarket has been dampened by the costs of hardware licenses and the ongoingsupport needed to ensure accurate and satisfying user experiences. Lyrix hassolved these issues for service providers, making speech recognition affordableand simple to deploy from the cloud. The application is now compliance-testedby Avaya for compatibility with: Avaya Aura(r) Communication Manager Release6.0.1, and Avaya Aura Session Manager Release 6.1.'Achieving Avaya compliance of our hosted Mobiso solution will help us buildour business by serving customers more efficiently and effectively,' said JeffGardella, VP of Engineering & Support, Lyrix. 'Our customers can nowconfidently deploy our proven high level speech applications utilizing standardSIP based connectivity from the Mobiso cloud. This allows our customers toquickly and easily leverage their communications infrastructure deliveringinnovative speech recognition productivity solutions to their employees andimprove customer service to their customers.'Lyrix is a Technology Partner in the Avaya DevConnect program--an initiative todevelop, market and sell innovative third-party products that interoperate withAvaya technology and extend the value of a company's investment in its network.As a Technology Partner, Lyrix is eligible to submit products for compliancetesting by the Avaya Solution Interoperability and Test Lab. There, a team ofAvaya engineers develops a comprehensive test plan for each application toverify whether it is Avaya compatible. Doing so enables businesses toconfidently add best-in-class capabilities to their network without having toreplace their existing infrastructure--speeding deployment of new applicationsand reducing both network complexity and implementation costs.'By participating in Avaya's compliance testing program and committing to openstandards, Technology Partners like Lyrix are helping customers get more out oftheir network investment,' said Eric Rossman, vice president, developerrelations, Avaya. 'Lyrix's speech recognition capabilities, delivered from theMobiso cloud, will help Avaya customers improve their users' communicationsexperience and productivity while reducing operational complexity and costs.'About AvayaAvaya is a global provider of business collaboration and communicationssolutions, providing unified communications, contact centers, networking andrelated services to companies of all sizes around the world. For moreinformation please visit www.avaya.com[http://www.avaya.com]. For more information on the AvayaDevConnect program, visit www.avaya.com/devconnect[http://www.avaya.com/devconnect].About Lyrix, Inc.Lyrix, established in 1995, is an enterprise focusedcommunications-as-a-service (CaaS) company based in the Boston, MA metro area.Lyrix provides enterprise communications solutions including cloud-basedwireless expense management, unified communications, and speech recognitionassistants to SMBs and Fortune 1000 companies worldwide. Mobiso is the creationof Lyrix, Inc. We are partners with major players and our client base includesprestigious names such as Ernst & Young and Citigroup UK. For more informationplease visit www.lyrix.com[http://www.lyrix.com].The Lyrix logo is available athttp://www.globenewswire.com/newsroom/prs/?pkgid=11994[http://www.globenewswire.com/newsroom/prs/?pkgid=11994]CONTACT: Media Inquiries:Stewart Hampton978-257-0717shampton@lyrix.comMedia Inquiries:DevConnect PR908-953-6432devconnectpr@avaya.comNews Source: NASDAQ OMX12.03.2012 Dissemination of a Corporate News, transmitted by DGAP -a company of EquityStory AG.The issuer is solely responsible for the content of this announcement.DGAP's Distribution Services include Regulatory Announcements,Financial/Corporate News and Press Releases.Media archive at www.dgap-medientreff.de[http://www.dgap-medientreff.de] and www.dgap.de[http://www.dgap.de]---------------------------------------------------------------------------Language: EnglishCompany: LyrixUnited StatesPhone:Fax:E-mail:Internet:ISIN: US9901938640WKN:End of Announcement DGAP News-Service---------------------------------------------------------------------------Deutsche Gesellschaft für Ad-hoc-Publizität mbhDocument DGAPFI0020120312e83c001e1"
"34",2018-01-20," China's AI leader iFLYTEK invests in breast cancer detection wearable Cyrcadia Asiawallstreet:online, 03:00, 16 July 2018, 883 words, (German)(Document WC67106020180716ee7g0002t)"," China's AI leader iFLYTEK invests in breast cancer detection wearable Cyrcadia Asia"
"35",2018-01-20," Medical Transcription Market to Gain Due to the Growing Focus on Automation of Healthcare Services and Increasing Adoption of Advanced...wallstreet:online, 12:35, 16 July 2018, 823 words, (German)(Document WC67106020180716ee7g002jv)"," Medical Transcription Market to Gain Due to the Growing Focus on Automation of Healthcare Services and Increasing Adoption of Advanced..."
"36",2018-07-17,"WeAreDevelopers veranstaltet AI-Konferenz für Entwickler in Wien","NewsWeAreDevelopers veranstaltet AI-Konferenz für Entwickler in WienChristof Baumgartner 312 words17 July 2018Computerwelt OnlineCMPONLGerman© 2018 CW Fachverlag GmbH. All rights reserved. For further information see http://www.computerwelt.at[http://www.computerwelt.at] Die digitale Developer-Plattform WeAreDevelopers veranstaltet am 4. und 5. Dezember 2018 die erste AI-Konferenz für Entwickler in Wien. Unter dem Motto „People – Code – AI“ sollen auf dem ""WeAreDevelopers AI Congress"" Entwickler und IT-Experten über aktuelle Trends rund um das Thema Machine Learning sprechen.Nach dem ""WeAreDevelopers World Congress"" im Mai, hat WeAreDevelopers nun ein weiteres Event speziell für Entwickler ins Leben gerufen. Der im Dezember 2018 stattfindende ""WeAreDevelopers AI Congress"" soll sich gezielt auf das Thema Machine Learning fokussieren und Innovationen und Entwicklungen der Interaktion zwischen Mensch und Maschine in den Mittelpunkt stellen. Die Veranstaltung findet im Kongress- und Veranstaltungszentrum Hofburg Vienna statt, erwartet werden bis zu 2.000 Developer und IT-Experten.Machine Learning im Mittelpunkt„Machine Learning basiert auf den Säulen Vision, Sound & Speech Recognition, Natural Language Processing (NLP) sowie Data Science und Analytics. Alle vier Bereiche werden ausführlich in diversen Experten-Vorträgen und Diskussionsrunden aufgegriffen“, sagt Benjamin Ruschin, Managing Director und Co-Founder von WeAreDevelopers. „Wir konzentrieren uns bei dem Kongress ganz bewusst auf das Nischenthema Machine Learning und beleuchten die aktuellsten und für die Branche relevantesten Entwicklungen, die die Interaktion zwischen Mensch und Maschine mit sich bringt. Einen besonderen Fokus legen wir dabei auf die Vertrauensbildung gegenüber maschinellen Entscheidungen sowie die Verbesserung der Nutzererfahrung mit maschineller Lernsoftware.“Die zweitägige Konferenz umfasst 32 Experten-Vorträge und Panels sowie 16 Workshops, die explizit auf die Interessen von Entwicklern zugeschnitten sind. „Developer sind unsere Kernzielgruppe. Sie anzusprechen und einzubinden sowie das Event genau nach ihren Anforderungen zu gestalten, ist uns enorm wichtig“, erklärt Sead Ahmetovic, Managing Director und Co-Founder von WeAreDevelopers. „Neben zahlreichen Entwicklern erwarten wir auch die Teilnahme von über 300 IT-Experten auf Management-Ebene. Sie können sich auf unserem AI-Kongress fachspezifisch miteinander austauschen und vernetzen.“Klicken Sie, um das Bild zu sehen[https://computerwelt.at/wp-content/uploads/artificial-intelligence-3382507_1920-1.jpg]CW Fachverlag GmbHDocument CMPONL0020180717ee7h0002w"
"37",2018-09-20," Radisys Introduces First-to-Market Decomposed Virtual Media Server for NFV, Delivering Improved ROI for Cloud Serviceswallstreet:online, 14:00, 9 July 2018, 363 words, (German)(Document WC67106020180709ee790030p)"," Radisys Introduces First-to-Market Decomposed Virtual Media Server for NFV, Delivering Improved ROI for Cloud Services"
"38",2018-02-20," ZTEsoft Presents AI Solutions at MWCS 2018wallstreet:online, 09:52, 28 June 2018, 352 words, (German)(Document WC67106020180628ee6s00265)"," ZTEsoft Presents AI Solutions at MWCS 2018"
"39",2018-02-20," ZTEsoft präsentiert KI-Lösungen auf dem MWCS 2018wallstreet:online, 02:26, 29 June 2018, 417 words, (German)(Document WC67106020180629ee6t0002t)"," ZTEsoft präsentiert KI-Lösungen auf dem MWCS 2018"
"40",2008-09-20,"Auch Philips Speech Recognition Systems ist jetzt Nuance","Auch Philips Speech Recognition Systems ist jetzt Nuance164 words9 October 2008COMPUTERWOCHE OnlineCOMWOLGerman(c) 2008 COMPUTERWOCHE.   www.computerwoche.de[http://www.computerwoche.de]PSRS mit Sitz in Wien ist/war einer der führenden Anbieter von Spracherkennungslösungen mit besonders starker Position im europäischen Gesundheitswesen, in das sich Nuance mit der Übernahme hineinkauft. 21,7 Millionen Euro hat Nuance bereits Ende September bezahlt, die übrigen 44,3 Millionen werden in bar am 21 September 2009 fällig.PSRS kann über 8000 Installationen seiner klinischen Spracherkennungslösung ""SpeechMagic"" sowie 100 OEM- und Channel-Partner vorweisen. Nuance bekommt mit der Übernahme außerdem Zugriff auf Philips' 25 Sprachen für Continuous Speech Recognition plus 150 fremdsprachige Spezialvokabulare.Die Amerikaner, im Healthcare-Bereich unter anderem bereits mit ""Dragon Medical SDK"" und ""PowerScribe SDK"" unterwegs, erwarten sich von der Übernahme für ihr Fiskaljahr 2009 bereits 36 bis 39 Millionen Dollar Mehrumsatz sowie bis zu einem Cent mehr Gewinn unterm Strich.Der US-Anbieter Nuance Communications hat in der vergangenen Woche den Philips-Geschäftsbereich Speech Recognition Systems (PSRS) für 96,1 Millionen Dollar (66 Millionen Euro) übernommen. IDG Communications Verlag AGDocument COMWOL0020081009e4a90002u"
"41",2018-06-29,"ZTEsoft präsentiert KI-Lösungen auf dem MWCS 2018","ZTEsoft präsentiert KI-Lösungen auf dem MWCS 2018507 words29 June 201802:27Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS"" Shanghai - Der vom 27. bis 29. Juni 2018 stattfindende, dreitägige Mobile World Congress Shanghai hat Fachleute aus der gesamten Mobilfunkbranche angezogen, um sich mit den zentralen Themen zu beschäftigen, die das mobile Ökosystem und verwandte Branchen prägen werden, unter anderem die Themen 5G, künstliche Intelligenz (KI), die neuesten Entwicklungen bei Geräten, die Zukunft von Fahrzeugen und das Internet der Dinge (Internet of Things/IoT). ZTEsoft wird seine jüngsten KI-Lösungen während der Veranstaltung mit dem Titel ""AI builds a better future"" vorstellen.Durch intelligentes Marketing mehr als nur den reinen Bedarf abdeckenKommunikationsdienstleister (Communication service providers/CSPs) nähern sich zunehmend den innovativen Marketingmodellen von Internet-Giganten an und bauen ihre Operationen zu einem Komplettservice aus, mit intelligenten Empfehlungen online und neuen Kauferfahrungen offline. ZTEsoft unterstützt CSPs beim Aufbau einer Cloud-Geschäftsplattform, die Datensynchronisierung online zu offline verwirklichen kann und auf diese Weise eine einheitliche Erfahrung für die Kunden liefert. Durch Einsatz der KI-Technologie wird gleichzeitig ein präzises und intelligentes Marketing ermöglicht, mit dem die Probleme in Bezug auf Geschäftsmodelle, personalisierte Dienste und Geschäftsoperationen in Angriff genommen werden.Wandel von reaktiver Hilfestellung zu proaktivem KundenserviceChatbots sind ein typisches Beispiel für KI zur Verbesserung der Nutzererfahrung. ZSmart uTalk integriert einen intelligenten Serviceroboter zur Verbesserung der Interaktion mit Kunden. Dadurch unterstützt diese Lösung die Kundendienstmitarbeiter, breit gefächerter zu agieren und den Wandel von reaktiver Hilfestellung zu proaktiver Unterstützung zu verwirklichen. Die automatische Spracherkennung (automatic speech recognition/ASR-Technologie) bietet ein koordiniertes Arbeiten von Mensch zu Maschine über einen rund um die Uhr zugänglichen Online-Roboter, der einfache, wiederkehrende Probleme lösen und Kundenanfragen auf intelligente Weise erkennen kann.Intelligenter Roboter verbessert die operative EffizienzZTEsoft lanciert seinen intelligenten, mit KI ausgerüsteten Roboter für den praktischen Einsatz, der Bilderkennung, Informationsanfragen, automatische Vorgangsreservierung und 7x24-Support in Echtzeit bietet. Mit diesem intelligenten Roboter ist ZTEsoft in der Lage, seinen Technikern in der Praxis einen virtuellen Assistenten zur Hand geben, um die Kundenwahrnehmung zu verbessern und die operative Effizienz zu steigern.KI-Technologie eröffnet neue Dimensionen und birgt ein vielversprechendes Potenzial für die Telekommunikationsbranche. Bei ZTEsoft ist man davon überzeugt, dass KI einen großen Einfluss auf den operativen Betrieb, die Produktion und das Management von CSPs im Prozess der digitalen Transformation haben wird. Besuchen Sie unseren Messestand auf der MWCS 2018 # Halle N2 Stand N2.F60, um mehr darüber zu erfahren, wie ZTEsoft Ihre digitale Transformation mit ""KI+""-Lösungen und -Diensten beschleunigen kann.Informationen zu ZTEsoftZTEsoft ist ein weltweit tätiges, führendes Technologieunternehmen für Cloud-Computing. Das im Jahr 2003 gegründete ZTEsoft unterstützt mittlerweile Telekommunikationsbetreiber, Regierungen und Unternehmen in mehr als 80 Ländern und Regionen. Das Unternehmen bietet hochwertige Lösungen und professionelle Dienste, die sich auf Cloud-Computing, Big-Data-Analytik und künstliche Intelligenz (KI) stützen. Nehmen Sie Kontakt mit uns auf über: cloudnow@ztesoft.comRückfragehinweis:   Xi Chen   +86 177 1432 5727   chen.xi4@ztesoft.comDigitale Pressemappe: http://www.ots.at/pressemappe/PR122374/aom[http://www.ots.at/pressemappe/PR122374/aom]*** OTS-ORIGINALTEXT PRESSEAUSSENDUNG UNTER AUSSCHLIESSLICHER INHALTLICHER VERANTWORTUNG DES AUSSENDERS - WWW.OTS.AT ***OTS0001-20180629APA - Austria Presse Agentur eGDocument AUPAG00020180629ee6t00001"
"42",2018-01-20," Tessian Raises $13m to Make Enterprise Email Safe With Machine Learningwallstreet:online, 01:01, 18 June 2018, 1280 words, (German)(Document WC67106020180617ee6h0030e)"," Tessian Raises $13m to Make Enterprise Email Safe With Machine Learning"
"43",2018-01-20," The car of the future – sleeper cars and travelling offices too?innovations-report, 01:00, 18 June 2018, 973 words, (German)(Document WC56527020180619ee6h0000a)"," The car of the future – sleeper cars and travelling offices too?"
"44",2018-01-20," London Tech Week - London is Named Artificial Intelligence (AI) Capital of Europe by New Reportwallstreet:online, 10:00, 11 June 2018, 650 words, (German)(Document WC67106020180611ee6b002hq)"," London Tech Week - London is Named Artificial Intelligence (AI) Capital of Europe by New Report"
"45",2018-01-20," Global Automotive Artificial Intelligence Market Expected to Reach $5,827.6 Million by 2024wallstreet:online, 14:00, 13 June 2018, 778 words, (German)(Document WC67106020180613ee6d0036p)"," Global Automotive Artificial Intelligence Market Expected to Reach $5,827.6 Million by 2024"
"46",2007-11-15,"Philips und Elsevier verbinden Spracherkennung und radiologisches diagnostisches Nachschlagesystem, um Radiologen bessere Entscheidungshilfe zu liefern   ","Philips und Elsevier verbinden Spracherkennung und radiologisches diagnostisches Nachschlagesystem, um Radiologen bessere Entscheidungshilfe zu liefern   696 words15 November 2007OTS - OriginaltextserviceOTSGerman(c) 2007 news aktuell   Wien, Österreich und Philadelphia, November 15 (ots/PRNewswire) -   - Erste Lösung ihrer Art kann Qualität von Diagnosen verbessern   Royal Philips Electronics (NYSE: PHG)(AEX: PHI) und Elsevier haben heute bekannt gegeben, dass die Spracherkennungsplattform von Philips, SpeechMagic(TM), integriert wird, um Kompatibilität mit dem neuen radiologischen diagnostischen Nachschlagesystem RadConsult von Elsevier zu bieten. Dadurch wird Kunden und Partnern beider Seiten ermöglicht, eine Spracherkennung für den gewerblichen Einsatz mit Zugang zu radiologischen Informationen der Weltklasse zu verwenden, die ihnen dabei hilft, ihre Produktivität zu steigern und ärztliche Fehler bei der Bewertung und Diagnose von Krankheitsfällen zu reduzieren.   ""Man schätzt, dass sich das medizinische Wissen alle achtzehn Monate verdoppelt"", sagte Marcel Wassink, Geschäftsführer von Philips Speech Recognition Systems. ""Die Kompatibilität zwischen dem radiologischen Nachschlagesystem RadConsult und SpeechMagic wird Radiologen eine zuverlässige Entscheidungshilfe auf der Basis neuester Informationen aus dem Fachgebiet liefern. SpeechMagic beschleunigt jetzt nicht nur die Verfügbarkeit medizinischer Berichte, sondern trägt auch dazu bei, eine genauere und auf Nachweise gestützte medizinische Behandlung zu gewährleisten.""   RadConsult erhöht die diagnostische Effizienz des Radiologen. Röntgenbilder und unterschiedliche Diagnoseinhalte werden über eine einfach zu bedienende Website übertragen und rund um den Arbeitsablauf des Radiologen organisiert. Die Zusammenführung der Dienstleistungen von RadConsult mit SpeechMagic ist die erste ihrer Art und kann die Vorteile von Radiology Information Systems (RIS) noch verbessern, indem Radiologen Zugang zu Informationen am Computerarbeitsplatz im Befundungsraum ermöglicht wird.   ""Wir freuen uns über diese Partnerschaft mit Philips Speech Recognition Systems und darüber, wie sie uns in unserem Bestreben, die Effizienz der radiologischen Praxis zu verbessern, unterstützt. Wir werden weitere unserer radiologischen Referenzinformationen der Weltklasse dort einbinden, wo Mediziner sie benötigen, um ihre diagnostischen Entscheidungen zu unterstützen"", sagt Brian Nairn, Geschäftsführer von Elsevier für die Abteilung Health Sciences.   SpeechMagic, die Spracherkennungsplattform für den gewerblichen Einsatz von Philips, ist an mehr als 8000 Arbeitsplätzen in 45 Ländern installiert. Weitere Informationen über beide Technologien sind am Stand 1105 von Elsevier auf der Jahreskonferenz der Radiological Society of North America (RSNA) erhältlich, die vom 25. bis 29. November 2007 in Chicago, Illinois, USA stattfindet.   Informationen zu Royal Philips Electronics   Royal Philips Electronics mit Hauptsitz in den Niederlanden (NYSE: PHG, AEX: PHI) ist ein weltweit führendes Unternehmen für Gesundheitspflege, Lebensstil und Technologie, und liefert Produkte, Dienstleistungen und Lösungen unter dem Markenversprechen ""sense and simplicity."" Philips beschäftigt schätzungsweise 128.100 Angestellte in mehr als 60 Ländern auf der ganzen Welt. Das Unternehmen erzielte 2006 einen Umsatz von 34 Milliarden US-Dollar (27 Milliarden Euro) und ist marktführend bei diagnostischer Bildgebung und Patientenüberwachungssystemen, energieeffizienten Beleuchtungslösungen, elektrischen Geräten zur Körperpflege, Elektro-Haushaltsgeräten sowie Unterhaltungselektronik. Neuigkeiten von Philips finden Sie unter   http://www.philips.com/newscenter  [http://www.philips.com/newscenter].   Informationen zu Elsevier   Elsevier ist ein weltweit führender Verlag für wissenschaftliche, technische und medizinische Informationsprodukte und -dienstleistungen. In Zusammenarbeit mit den globalen Wissenschafts- und Gesundheitsgesellschaften veröffentlichen die 7000 Angestellten von Elsevier in über 70 Büros weltweit mehr als 2000 Zeitschriften und 1900 neue Bücher pro Jahr und bieten ausserdem eine Reihe innovativer elektronischer Produkte an, wie zum Beispiel ScienceDirect (  http://www.sciencedirect.com/  [http://www.sciencedirect.com/]), MD Consult (   http://www.mdconsult.com/  [http://www.mdconsult.com/]), Scopus (  http://www.info.scopus.com/  [http://www.info.scopus.com/]), bibliografische Datenbanken und Online-Nachschlagewerke.   Elsevier (  http://www.elsevier.com/  [http://www.elsevier.com/]) ist ein globales Unternehmen mit Sitz im niederländischen Amsterdam und hat Büros auf der ganzen Welt. Elsevier gehört zur Kapitalgesellschaft Reed Elsevier Group (   http://www.reedelsevier.com/  [http://www.reedelsevier.com/]), ein weltweit führender Verlag und Informationsanbieter. Reed Elsevier ist in den Bereichen Naturwissenschaft, Medizin, Recht, Bildung sowie Business-to-Business tätig und bietet Nutzern hochwertige und flexible Informationslösungen, wobei der Schwerpunkt mehr und mehr auf dem Internet als Übertragungsweg liegt. Die Börsentelegraphsymbole für Reed Elsevier sind REN (Euronext Amsterdam), REL (London Stock Exchange), RUK und ENL (New York Stock Exchange).   http://www.RadConsult.com  [http://www.RadConsult.com]Originaltext: Elsevier and Royal Philips Electronics Digitale Pressemappe:   http://www.presseportal.de/pm/68991  [http://www.presseportal.de/pm/68991] Pressemappe via RSS :   http://www.presseportal.de/rss/pm_68991.rss2  [http://www.presseportal.de/rss/pm_68991.rss2]Pressekontakt: Für weitere Informationen kontaktieren Sie bitte: Philips Speech Recognition Systems, Anne Durand-Badel, USA: +1-888-SPEAK-50, andere Länder: +43-1-60101-1048, anne.durand-badel@philips.com; Elsevier, Mike Smith, Elsevier, +1-314-453-7050, Michael.smith@elsevier.com   1085419news aktuell GmbHDocument OTS0000020071115e3bf005v5"
"47",2007-11-15,"Philips und Elsevier verbinden Spracherkennung und radiologisches diagnostisches Nachschlagesystem, um Radiologen bessere Entscheidungshilfe zu liefern   ","Philips und Elsevier verbinden Spracherkennung und radiologisches diagnostisches Nachschlagesystem, um Radiologen bessere Entscheidungshilfe zu liefern   675 words15 November 200720:37PR Newswire EuropePRNWDEGermanCopyright © 2007 PR Newswire Europe Limited. Alle Rechte vorbehalten.   WIEN, Österreich und PHILADELPHIA, November 15 /PRNewswire/ --   - Erste Lösung ihrer Art kann Qualität von Diagnosen verbessern   Royal Philips Electronics (NYSE: PHG)(AEX: PHI) und Elsevier haben heute bekannt gegeben, dass die Spracherkennungsplattform von Philips, SpeechMagic(TM), integriert wird, um Kompatibilität mit dem neuen radiologischen diagnostischen Nachschlagesystem RadConsult von Elsevier zu bieten. Dadurch wird Kunden und Partnern beider Seiten ermöglicht, eine Spracherkennung für den gewerblichen Einsatz mit Zugang zu radiologischen Informationen der Weltklasse zu verwenden, die ihnen dabei hilft, ihre Produktivität zu steigern und ärztliche Fehler bei der Bewertung und Diagnose von Krankheitsfällen zu reduzieren.   ""Man schätzt, dass sich das medizinische Wissen alle achtzehn Monate verdoppelt"", sagte Marcel Wassink, Geschäftsführer von Philips Speech Recognition Systems. ""Die Kompatibilität zwischen dem radiologischen Nachschlagesystem RadConsult und SpeechMagic wird Radiologen eine zuverlässige Entscheidungshilfe auf der Basis neuester Informationen aus dem Fachgebiet liefern. SpeechMagic beschleunigt jetzt nicht nur die Verfügbarkeit medizinischer Berichte, sondern trägt auch dazu bei, eine genauere und auf Nachweise gestützte medizinische Behandlung zu gewährleisten.""   RadConsult erhöht die diagnostische Effizienz des Radiologen. Röntgenbilder und unterschiedliche Diagnoseinhalte werden über eine einfach zu bedienende Website übertragen und rund um den Arbeitsablauf des Radiologen organisiert. Die Zusammenführung der Dienstleistungen von RadConsult mit SpeechMagic ist die erste ihrer Art und kann die Vorteile von Radiology Information Systems (RIS) noch verbessern, indem Radiologen Zugang zu Informationen am Computerarbeitsplatz im Befundungsraum ermöglicht wird.   ""Wir freuen uns über diese Partnerschaft mit Philips Speech Recognition Systems und darüber, wie sie uns in unserem Bestreben, die Effizienz der radiologischen Praxis zu verbessern, unterstützt. Wir werden weitere unserer radiologischen Referenzinformationen der Weltklasse dort einbinden, wo Mediziner sie benötigen, um ihre diagnostischen Entscheidungen zu unterstützen"", sagt Brian Nairn, Geschäftsführer von Elsevier für die Abteilung Health Sciences.   SpeechMagic, die Spracherkennungsplattform für den gewerblichen Einsatz von Philips, ist an mehr als 8000 Arbeitsplätzen in 45 Ländern installiert. Weitere Informationen über beide Technologien sind am Stand 1105 von Elsevier auf der Jahreskonferenz der Radiological Society of North America (RSNA) erhältlich, die vom 25. bis 29. November 2007 in Chicago, Illinois, USA stattfindet.   Informationen zu  Royal Philips ElectronicsRoyal Philips Electronics mit Hauptsitz in den Niederlanden (NYSE: PHG, AEX: PHI) ist ein weltweit führendes Unternehmen für Gesundheitspflege, Lebensstil und Technologie, und liefert Produkte, Dienstleistungen und Lösungen unter dem Markenversprechen ""sense and simplicity."" Philips beschäftigt schätzungsweise 128.100 Angestellte in mehr als 60 Ländern auf der ganzen Welt. Das Unternehmen erzielte 2006 einen Umsatz von 34 Milliarden US-Dollar (27 Milliarden Euro) und ist marktführend bei diagnostischer Bildgebung und Patientenüberwachungssystemen, energieeffizienten Beleuchtungslösungen, elektrischen Geräten zur Körperpflege, Elektro-Haushaltsgeräten sowie Unterhaltungselektronik. Neuigkeiten von Philips finden Sie unter   http://www.philips.com/newscenter  [http://www.philips.com/newscenter].   Informationen zu Elsevier   Elsevier ist ein weltweit führender Verlag für wissenschaftliche, technische und medizinische Informationsprodukte und -dienstleistungen. In Zusammenarbeit mit den globalen Wissenschafts- und Gesundheitsgesellschaften veröffentlichen die 7000 Angestellten von Elsevier in über 70 Büros weltweit mehr als 2000 Zeitschriften und 1900 neue Bücher pro Jahr und bieten ausserdem eine Reihe innovativer elektronischer Produkte an, wie zum Beispiel ScienceDirect (  http://www.sciencedirect.com/  [http://www.sciencedirect.com/]), MD Consult (   http://www.mdconsult.com/  [http://www.mdconsult.com/]), Scopus (  http://www.info.scopus.com/  [http://www.info.scopus.com/]), bibliografische Datenbanken und Online-Nachschlagewerke.   Elsevier (  http://www.elsevier.com/  [http://www.elsevier.com/]) ist ein globales Unternehmen mit Sitz im niederländischen Amsterdam und hat Büros auf der ganzen Welt. Elsevier gehört zur Kapitalgesellschaft Reed Elsevier Group (   http://www.reedelsevier.com/  [http://www.reedelsevier.com/]), ein weltweit führender Verlag und Informationsanbieter. Reed Elsevier ist in den Bereichen Naturwissenschaft, Medizin, Recht, Bildung sowie Business-to-Business tätig und bietet Nutzern hochwertige und flexible Informationslösungen, wobei der Schwerpunkt mehr und mehr auf dem Internet als Übertragungsweg liegt. Die Börsentelegraphsymbole für Reed Elsevier sind REN (Euronext Amsterdam), REL (London Stock Exchange), RUK und ENL (New York Stock Exchange).   http://www.RadConsult.com  [http://www.RadConsult.com]Elsevier and  Royal Philips ElectronicsFür weitere Informationen kontaktieren Sie bitte: Philips Speech Recognition Systems, Anne Durand-Badel, USA: +1-888-SPEAK-50, andere Länder: +43-1-60101-1048, anne.durand-badel@philips.com; Elsevier, Mike Smith, Elsevier, +1-314-453-7050, Michael.smith@elsevier.com   1679953.xmlPR Newswire Association, Inc.Document PRNWDE0020071115e3bf001jl"
"48",2000-11-20,"Fonix schafft Sprach-Interaktion mit mobilen Geräten.","Fonix schafft Sprach-Interaktion mit mobilen Geräten.837 words11 August 2000OTS - OriginaltextserviceOTSGerman(c) 2000 News AktuellDas Unternehmen bringt FAAST 1.0 für Windows CE zur Unterstützung systemintegrierter Anwendungen auf den Markt. Die Fonix Corporation (OTC Bulletin Board: FONX), ein führender Hersteller von Human-User-Interface Lösungen für mobile Geräte, Internet-und Fernsprechsysteme sowie Fahrzeug-Telematik, gab die kommerzielle Einführung des FAAST (Fonix Accelerated Application Solutions Technology) 1.0 Software-Development-Kit (SDK) und einer Sprachstruktur für das Microsoft(R) Windows(R) CE Betriebssystem bekannt. Mit FAAST SDK von Fonix können Entwickler ihre Produkte schnell und einfach interaktiv machen, auf der Basis der Windows CE Plattform, auf die viele mobile oder Handheld-Geräte aufbauen.Gleichzeitig gab Fonix seine Unterstützung für den Pocket-PC mit Windows bekannt. Fonix arbeitet eng mit den Herstellern von Pocket-PC-betriebenen Geräten zusammen, um die nötigen Entwicklungsvoraussetzungen, Werkzeuge und Technologien zu schaffen, um ihre Produkte mit voller Sprach-Interaktion auszustatten.Fonix hat bereits ""Voice-In"" und ""Voice-Out"" Konzept-Anwendungen demonstriert, darunter eine kontinuierliche Sprachwahl, einen PDA-Befehlsnavigator und einen MP3-Befehlsnavigator, die auf verschiedenen Windows CE-und Pocket-PC-basierten Geräten laufen, wie zum Beispiel auf dem Compaq iPAQ (mit Intel(R) StrongARM SA-1110 Prozessor), dem Casio Cassiopeia (mit MIPS-basiertem Prozessor) und dem Hewlett Packard Jornada (mit Hitachi SH3-Prozessor).Fonix zeigt die ASR/TTS Demoversionen dieser Anwendungen auf den nächsten Branchenmessen, wie dem ""Fall Intel Developers Forum"" vom 22.-24. August in San Jose, Kalifornien oder der ""Embedded Systems Conference"" vom 24.-28. September, ebenfalls in San Jose.""Windows CE vergrößert ständig seinen Marktanteil bei Herstellern von mobilen und Handheld-Geräten"", sagt Thomas A. Murdock, Chairman und CEO von Fonix. ""Durch die Einführung von FAAST für Windows CE können diese Hersteller Kosten senken, die Zeit zur Markteinführung verkürzen und die Schnittstellen und Funktionalität ihrer Geräte deutlich verbessern. Durch Fonix ist die systemintegrierte Sprachsteuerung schon heute bereit für ihre glänzende Zukunft.""Windows CE ist das neueste der von Fonix Automatic Speech Recognition-und Text-to-Speech-unterstützten Betriebssysteme. Ebenfalls unterstützt werden Windows 95, 98, 2000 und NT sowie seit neuestem FAAST für Linux. Mögliche Windows CE-Anwendungen für FAAST sind z. B. Fahrzeug-Telematik, Smart-Phone-Betriebssysteme, PDA-Steuerung und vertikale Industrieanwendungen.Fonix wurde Ende 1999 ein Gründungsmitglied der ""Microsoft Embedded Tools Partners""-Gruppe und ist bis heute das einzige Unternehmen für Spracherkennung und Text-to-Speech in diesem Programm.Mit Fonix FAAST können Software-Entwickler schnell Befehle für die automatische Spracherkennung (Automatic Speech Recognition, ASR) und Text-to-Speech (TTS) einbauen, Anwendungen steuern und das Programm auf eine Ziel-Plattform herunterladen. Die störungsresistenten Anwendungen verwenden ein Fernfeldmikrophon und enthalten lebensechte TTS-Stimmen mit Bereitschaftsmeldungen und Befehlsbestätigung.Das neue Release ist ein weiteres Beispiel dafür, dass Fonix Sprachtechnologien entwickelt, die schnell und einfach in OEMs integriert werden können. Dies betrifft eine große Zahl von mobilen Geräten und Konsumgütern, bei denen Sprach-Interaktion besonders wichtig ist.Fonix wird FAAST 1.0 für Windows CE ab September 2000 ausliefern.Fonix CorporationDie Fonix Corporation (OTC Bulletin Board: FONX) ist ein führender Hersteller von Human-User-Interface-Lösungen für mobile Geräte, Internet und Fernsprechsysteme sowie Fahrzeug-Telematik. Führende Computerchip-Hersteller, unabhängige Software-und Hardwarefirmen sowie Provider von Internet-Inhalten und - Diensten verwenden die Fonix-Technologie, um ihren Kunden eine leichtere und bequemere Handhabung zu ermöglichen. Die Produkte von Fonix, darunter Text-To-Speech (TTS), Automatic Speech Recognition (ASR) und die Handschriftenerkennung (HWR) sind die natürlichsten momentan erhältlichen Kommunikationslösungen. Nähere Informationen finden Sie unterwww.fonix.com[http://www.fonix.com/]oder per Telefon unter +1 (801) 553-6600.Vertrieb und Produktinformationen: Scott Lindsey, +1 (801) 553-6600, E-Mail: sales@fonix.comMediadaten: Kurt Herrmann, +1 (801) 553-6600, E-Mail: mediarel@fonix.com.Investoreninformationen: Michelle Aamodt, +1 (801) 328-0161, E-Mail: invrel@fonix.com.Hinweis: Die von der Fonix Corporation und Concierge, Inc. gemachten Angaben, die nicht historisch belegt sind, sind vorausschauende Aussagen im Sinne der ""Safe Harbor""-Bestimmungen des ""Private Securities Litigation Reform Act"" von 1995. Dazu zählen auch Aussagen über die Erwartungen, Hoffnungen, Pläne und Strategien des Unternehmens für die Zukunft. Die Investoren werden darauf hingewiesen, dass vorausschauende Aussagen Risiken und Unwägbarkeiten unterworfen sind, die die Geschäftsaussichten und Resultate beeinflussen können. Die tatsächlichen Ergebnisse des Unternehmens können sich deutlich von den in solchen vorausschauenden Aussagen angedeuteten unterscheiden. Risikofaktoren sind zum Beispiel allgemeine wirtschaftliche, Wettbewerbs-, Regierungs-und technologische Faktoren, wie sie in den Berichten des Unternehmens an die SEC (Formulare 10-K, 10-Q und 8-K) dargestellt sind. Das Unternehmen fühlt sich nicht verpflichtet, die in dieser Presseinformation enthaltenen vorausschauenden Aussagen nachträglich zu aktualisieren.Casio, Cassiopeia, Hewlett Packard und Jornada sind registrierte Markenzeichen ihrer jeweiligen Eigentümer.ots Originaltext: Fonix Corporation Im Internet recherchierbar:http://recherche.newsaktuell.de[http://recherche.newsaktuell.de]Rückfragen bitte an: Vertrieb und Produkte: Scott Lindsey, +1 801-553-6600, E-Mail: sales@fonix.com; Medien: Kurt Herrmann, +1 801-553-6600, E-Mail: mediarel@fonix.com; Investoren: Michelle Aamodt, +1 801-328-0161, E-Mail: invrel@fonix.com; alle bei der Fonix CorporationWebsite:http://www.fonix.com[http://www.fonix.com].Document ots0000020010816dw8b005dt"
"49",2016-05-13,"Using personal mobile technology for clinical documentation","Conhit 2012Using personal mobile technology for clinical documentationKathrin Schäfer   649 words13 May 2016DeviceMed onlineDEVMONGermanCopyright 2016. Vogel Business Media GmbH & Co. KG   Just as iPads and the like are becoming indispensable to many people in their private lives, so are they more and more essential to medical staff in hospitals and physician’s practices, as the trade fair Conhit in Berlin makes clear. The annual spring exhibition, which runs from 24 through 26 April this year, was created in 2008 by Bvitg, the German healthcare IT association..The healthcare IT industry is getting serious about the mobilisation of medical data because mobile documentation promises to bring the goal of the paper-free hospital much closer. In Germany’s hospitals, therefore, mobile-tech-based medical rounds are on the advance, and for good reason.As Bernhard Calmer, CEO of Bvitg, emphasized: “If patient information is available on a mobile basis, interaction with the patients is improved. It makes the medical round more effective and contributes to process optimisation.”“Anyone planning mobile scenarios has to analyse the need precisely in order to make the right hardware decision,” said Andreas Kassner, CEO of Servicegesellschaft VSG at Bvitg. Whereas, up till now, laptops have often been screwed onto trolleys when patient data had to be available to physicians as they made their rounds, a trend toward using the tablet PC and smartphone is accelerating. However, computer terminals at the patient’s bedside, usable not only by medical staff but also by the patient, also are attracting interest.Patient-data access via appConhit attendees will be able to gather information regarding the practical formatting of mobile clinical documentation from numerous exhibitors. “Most manufacturers of clinical information systems are able to make concrete bids in this area,” asserted Calmer.Positive experiences with an iPad-based medical round have been reported by SAP and Siemens, who at Charité Berlin piloted an iPad app that now will be made available to a wider circle of patients. This app gives doctors access to patient data stored in the hospital information system and in the digital archive of Charité, as well as elsewhere. The pilot-stage doctors in the neurology department of Charité reacted very favourably.“Acceptance is outstanding,” said Project Manager Hagen Hupperts of the IT division at Charité. “We have numerous queries from other wards. And doctors are telling us that the medical round in particular has changed considerably for the better.” Similar experiences are reported by Christian Bauer, IT manager at the health insurer Knappschaft, which introduced an app for mobile KIS access via smartphone at the Knappschaft hospital Bottrop. The industry partner in this case was the company Tieto. “The solution so convinced us and the doctors that we now want to offer something like it in other establishments, too,” declared Bauer.Data entry: Speech recognition is also an optionGeneral practitioners are also increasingly demanding mobile means of access to the physicians’ information system AIS, be it during house visits, in the office, or at the in-patient bedside in the co-operating hospital. AIS manufacturers such as CompuGroup and Medatixx have quickly taken account of this: they are working on apps for tablet or smartphone access to patient data, or already have such apps in their product offer.Unless full-blown laptops or terminals at the patient’s bed are involved, most mobile solutions in the hospital or physician’s practice now offer merely read-only access to patient data. This will change, however. In tablet PC–based or smartphone-based solutions, in addition to data entry via stylus or on-screen keyboard, speech recognition is being positioned as an attractive option for data input. This is made easier by the fact that manufacturers such as Nuance are offering speech recognition as a cloud-based installation, which enables the technology to be utilised regardless of current location and terminal.For further information:Bvitg - Bundesverband Gesundheits-IT e.V.Berlin, Germanywww.conhit.de[http://www.conhit.de]Vogel Business Media GmbH & Co. KGDocument DEVMON0020160513ec5d0000k"
"50",2008-04-15,"Philips stellt neues Service Portfolio für genaue, effiziente und benutzerfreundliche Informationserfassung im Gesundheitswesen vor","Philips stellt neues Service Portfolio für genaue, effiziente und benutzerfreundliche Informationserfassung im Gesundheitswesen vor531 words15 April 200816:16Business WireBWRGERGerman(c) 2008 Business Wire.  All Rights Reserved.   WIEN, Österreich - (BUSINESS WIRE) - Royal Philips Electronics (NYSE:PHG) (Amsterdam:PHI) lanciert heute ein neues Dienstleistungsangebot, das es dem globalen Netzwerk an Integrationspartnern ermöglicht, auf das Know-how und Expertenwissen des Unternehmens im Bereich der medizinischen Dokumentation zurückzugreifen. Ziel des Angebots ist es, Integration, Verwendung und Support der SpeechMagic-Plattform zu erleichtern und mithilfe von Experten-Know-how die Produktivität in der medizinischen Befundung zu steigern. Das Angebot von professionellen Dienstleistungen ist Teil der Unternehmensentwicklung vom Technologie-Anbieter zu einem Team weltweit anerkannter Experten auf dem Gebiet der genauen, effizienten und benutzerfreundlichen Informationserfassung im Gesundheitswesen.„Durch die Mitarbeit an den weltgrößten medizinischen Spracherkennungsprojekten, die sich über ganze Städte, Regionen und sogar Länder erstrecken, verfügt Philips in diesem Bereich über einzigartiges Wissen und ebensolche Erfahrung“, so Marcel Wassink, CEO von Philips Speech Recognition Systems. „Mithilfe der Service-Pakete können SpeechMagic-Partner dieses Wissen in ihren IT-Systemen direkt anwenden; hoch spezialisierte Techniker, Entwickler und Berater werden ihnen dabei helfen, optimale Lösungen zu entwickeln, die die Produktivität des Krankenhauses erhöhen und den Ärzten den Zugriff auf Informationen erleichtern.“Die neuen Services von Philips umfassen vier Bereiche: In einem ersten Schritt hilft Philips dabei, das optimale Paket an SpeechMagic-Funktionen zu definieren und zu integrieren. Als nächstes bereiten Deployment-Services auf eine schnelle Einführung des Systems vor, mit dem Ziel, die Benutzerakzeptanz zu erhöhen und durch Implementierung und Setup bedingte Ausfallszeiten zu reduzieren. Die Konfiguration von System, Netzwerk und Datenbank sowie die Erstellung von Trainingsplänen sind ebenfalls Teil der Implementationsvorbereitung. Die Wartung des Systems sowie die schnelle Bearbeitung technischer Problemstellungen gewährleisten die Support-Services.Philips unterstützt damit eine optimale Systemintegration, eine schnelle Implementation und einen reibungslosen Betrieb. Darüber hinaus analysieren Experten die Arbeitsweise der Benutzer, evaluieren die Ergebnisse und die Kundenzufriedenheit und geben Empfehlungen ab, wie Arbeitsabläufe optimiert werden können, um die Produktivität in der medizinischen Dokumentation zu steigern.„Wir erwarten von der Spracherkennungstechnologie, dass sie mit zwei sehr komplexen Aufgabenbereichen fertig wird: Erstens muss sie gesprochene Informationen erfassen und verstehen. Zweitens muss sie diese Informationen in strukturierten Text umwandeln und dabei berücksichtigen, dass dieser in sensiblen medizinischen Bereichen verwendet wird. Philips bietet nunmehr hochwertige Beratungsdienstleistungen an, um die Standards im Gesundheitswesen hinsichtlich Genauigkeit, Benutzerfreundlichkeit und Effizienz zu erreichen“, so Wassink.Die Services von Philips Speech Recognition Systems sind über ein internationales Netzwerk von Integrationspartnern erhältlich. Weitere Informationen erhalten Sie unter www.philips.com/spracherkennung[http://www.philips.com/spracherkennung].Über Royal Philips ElectronicsRoyal Philips Electronics mit Hauptsitz in den Niederlanden ist das weltweit führende Unternehmen für Healthcare, Lifestyle und Technology. Das Markenversprechen ""sense and simplicity"" verdeutlicht den Anspruch des Konzerns, Produkte, Dienstleistungen und Lösungen zu liefern, die auf die Bedürfnisse der Konsumenten zugeschnitten sind. Philips beschäftigt 134.200 Mitarbeiter in über 60 Ländern und erzielte 2007 einen Umsatz von 27 Milliarden Euro. Das Unternehmen ist weltweit führend bei diagnostischer Bildgebung im Medizinbereich, Patientenüberwachungssystemen, energiesparenden Beleuchtungssystemen, Elektro-Hausgeräten sowie Unterhaltungselektronik. Die deutsche Philips GmbH mit Sitz in Hamburg beschäftigt mehr als 7.000 Mitarbeiter. Mehr über Philips im Internet: www.philips.com/newscenter[http://www.philips.com/newscenter]Philips Speech Recognition Systems Ulrike Oswald, +34 93 270 40 25 Ulrike.oswald@philips.com   www.philips.com/spracherkennung[http://www.philips.com/spracherkennung]Business WireDocument BWRGER0020080415e44f000ul"
"51",2009-02-20,"Bücher","Bücher340 words2 March 2009Beschaffung aktuellBESCHA059German(c) 2009 Konradin Verlag. All Rights Reserved.   Tell me More – Chinese Beginner to Advance Speech Recognition Software Hueber Verlag, Ismaning, 2008 3 CD-ROMs und Headset 149,95 EuroBei der Software „Tell me More Chinese“ handelt es sich um ein Lernprogramm, das Anfängern sowie Fortgeschrittenen einen fundierten Lernprozess ermöglicht. Besonders hervorzuheben ist die Speech Recognition Software, die es als erste ihrer Art ermöglicht, mit-hilfe des mitgelieferten Headsets die vier Töne des Mandarin-Chinesisch auf die richtige Betonung zu prüfen. Das Programm führt in Englisch durch seine drei Einheiten Beginner, Intermediate und Advanced, die jeweils in einzelne, aufeinander stützende Lektionen unterteilt sind. Ein gegliederter Grammatikteil und ein Glossary mit Vokabeln in chinesischen Schriftzeichen sowie Pinyin, der Schreibweise in lateinischen Buchstaben, begleiten den Lernenden ebenfalls. Im Modus Exercises warten zehn verschiedene Aufgabentypen von Lückentexten über Diktate oder Assoziationsaufgaben auf ihre Lösungen. Die Speech Recognition Software rundet das Ganze ab, indem sie dem Nutzer die recht komplexen Ausspracheregelungen näherbringt. Sie lässt sich nach Belieben aktivieren und dem individuellen Fortschritt durch sieben Stufen der geforderten Aussprachegenauigkeit anpassen. Dies ist eine sehr angenehme und hilfreiche Funktion, die nach und nach kleine Erfolge verspricht. Das Selbstgesprochene kann außerdem angehört werden. Im Modus Pronunciation werden Sätze oder einzelne Wörter geübt. Im Dialogue wird dem Programm anhand einer Auswahl geantwortet, wobei jede Antwort das Gespräch in eine andere Richtung führt. Hieraus oder aus dem Wörterbuch kann stets durch Rechtsklick auf das Pronunciation-Menü zugegriffen werden, um die exakte Aussprache zu üben. Dank der Möglichkeit, das Pinyin ein- oder auszuschalten, wird den eigenen Bedürfnissen, die sprachliche Fähigkeit oder das Lesen des Mandarins in seinen Schriftzeichen zu üben, entgegengekommen. Zur Selbstkontrolle schaut man sich einen Lesson Report an, der einem die Fortschritte in Zahlen vor Augen führt. Tel me More hat hiermit ein innovatives Programm geliefert, das für Anfänger oder zum Auffrischen für Fortgeschrittene eine Möglichkeit bietet, vor allem die sprachliche Richtigkeit der chinesischen Sprache zu erlernen. Zum Erlernen der Konversation (Lesen, Grammatik, Aussprache und Erkennen der Schriftzeichen) ist das Programm auf jeden Fall zu empfehlen. Das Schreiben wird allerdings nicht gelernt. Florian KaiserKonradin Verlag Robert Kohlhammer GmbHDocument BESCHA0020090302e5320000l"
"52",2000-11-20,"Fonix schafft Sprach-Interaktion mit mobilen Geräten.","Fonix schafft Sprach-Interaktion mit mobilen Geräten.856 words11 August 200014:26Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS""Das Unternehmen bringt FAAST 1.0 für Windows CE zur Unterstützung systemintegrierter Anwendungen auf den Markt. Salt Lake City (ots-PRNewswire) - Die Fonix Corporation (OTC Bulletin Board: FONX), ein führender Hersteller von Human-User-Interface Lösungen für mobile Geräte, Internet-und Fernsprechsysteme sowie Fahrzeug-Telematik, gab die kommerzielle Einführung des FAAST (Fonix Accelerated Application Solutions Technology) 1.0 Software-Development-Kit (SDK) und einer Sprachstruktur für das Microsoft(R) Windows(R) CE Betriebssystem bekannt. Mit FAAST SDK von Fonix können Entwickler ihre Produkte schnell und einfach interaktiv machen, auf der Basis der Windows CE Plattform, auf die viele mobile oder Handheld-Geräte aufbauen.Gleichzeitig gab Fonix seine Unterstützung für den Pocket-PC mit Windows bekannt. Fonix arbeitet eng mit den Herstellern von Pocket-PC-betriebenen Geräten zusammen, um die nötigen Entwicklungsvoraussetzungen, Werkzeuge und Technologien zu schaffen, um ihre Produkte mit voller Sprach-Interaktion auszustatten.Fonix hat bereits ""Voice-In"" und ""Voice-Out"" Konzept-Anwendungen demonstriert, darunter eine kontinuierliche Sprachwahl, einen PDA-Befehlsnavigator und einen MP3-Befehlsnavigator, die auf verschiedenen Windows CE-und Pocket-PC-basierten Geräten laufen, wie zum Beispiel auf dem Compaq iPAQ (mit Intel(R) StrongARM SA-1110 Prozessor), dem Casio Cassiopeia (mit MIPS-basiertem Prozessor) und dem Hewlett Packard Jornada (mit Hitachi SH3-Prozessor).Fonix zeigt die ASR/TTS Demoversionen dieser Anwendungen auf den nächsten Branchenmessen, wie dem ""Fall Intel Developers Forum"" vom 22.-24. August in San Jose, Kalifornien oder der ""Embedded Systems Conference"" vom 24.-28. September, ebenfalls in San Jose.""Windows CE vergrößert ständig seinen Marktanteil bei Herstellern von mobilen und Handheld-Geräten"", sagt Thomas A. Murdock, Chairman und CEO von Fonix. ""Durch die Einführung von FAAST für Windows CE können diese Hersteller Kosten senken, die Zeit zur Markteinführung verkürzen und die Schnittstellen und Funktionalität ihrer Geräte deutlich verbessern. Durch Fonix ist die systemintegrierte Sprachsteuerung schon heute bereit für ihre glänzende Zukunft.""Windows CE ist das neueste der von Fonix Automatic Speech Recognition-und Text-to-Speech-unterstützten Betriebssysteme. Ebenfalls unterstützt werden Windows 95, 98, 2000 und NT sowie seit neuestem FAAST für Linux. Mögliche Windows CE-Anwendungen für FAAST sind z. B. Fahrzeug-Telematik, Smart-Phone-Betriebssysteme, PDA-Steuerung und vertikale Industrieanwendungen.Fonix wurde Ende 1999 ein Gründungsmitglied der ""Microsoft Embedded Tools Partners""-Gruppe und ist bis heute das einzige Unternehmen für Spracherkennung und Text-to-Speech in diesem Programm.Mit Fonix FAAST können Software-Entwickler schnell Befehle für die automatische Spracherkennung (Automatic Speech Recognition, ASR) und Text-to-Speech (TTS) einbauen, Anwendungen steuern und das Programm auf eine Ziel-Plattform herunterladen. Die störungsresistenten Anwendungen verwenden ein Fernfeldmikrophon und enthalten lebensechte TTS-Stimmen mit Bereitschaftsmeldungen und Befehlsbestätigung.Das neue Release ist ein weiteres Beispiel dafür, dass Fonix Sprachtechnologien entwickelt, die schnell und einfach in OEMs integriert werden können. Dies betrifft eine große Zahl von mobilen Geräten und Konsumgütern, bei denen Sprach-Interaktion besonders wichtig ist.Fonix wird FAAST 1.0 für Windows CE ab September 2000 ausliefern.Fonix CorporationDie Fonix Corporation (OTC Bulletin Board: FONX) ist ein führender Hersteller von Human-User-Interface-Lösungen für mobile Geräte, Internet und Fernsprechsysteme sowie Fahrzeug-Telematik. Führende Computerchip-Hersteller, unabhängige Software-und Hardwarefirmen sowie Provider von Internet-Inhalten und -Diensten verwenden die Fonix-Technologie, um ihren Kunden eine leichtere und bequemere Handhabung zu ermöglichen. Die Produkte von Fonix, darunter Text-To-Speech (TTS), Automatic Speech Recognition (ASR) und die Handschriftenerkennung (HWR) sind die natürlichsten momentan erhältlichen Kommunikationslösungen. Nähere Informationen finden Sie unterwww.fonix.com[http://www.fonix.com/]oder per Telefon unter +1 (801) 553-6600.Vertrieb und Produktinformationen: Scott Lindsey, +1 (801) 553-6600, E-Mail: sales@fonix.comMediadaten: Kurt Herrmann, +1 (801) 553-6600, E-Mail: mediarel@fonix.com.Investoreninformationen: Michelle Aamodt, +1 (801) 328-0161, E-Mail: invrel@fonix.com.Hinweis: Die von der Fonix Corporation und Concierge, Inc. gemachten Angaben, die nicht historisch belegt sind, sind vorausschauende Aussagen im Sinne der ""Safe Harbor""-Bestimmungen des ""Private Securities Litigation Reform Act"" von 1995. Dazu zählen auch Aussagen über die Erwartungen, Hoffnungen, Pläne und Strategien des Unternehmens für die Zukunft. Die Investoren werden darauf hingewiesen, dass vorausschauende Aussagen Risiken und Unwägbarkeiten unterworfen sind, die die Geschäftsaussichten und Resultate beeinflussen können. Die tatsächlichen Ergebnisse des Unternehmens können sich deutlich von den in solchen vorausschauenden Aussagen angedeuteten unterscheiden. Risikofaktoren sind zum Beispiel allgemeine wirtschaftliche, Wettbewerbs-, Regierungs-und technologische Faktoren, wie sie in den Berichten des Unternehmens an die SEC (Formulare 10-K, 10-Q und 8-K) dargestellt sind. Das Unternehmen fühlt sich nicht verpflichtet, die in dieser Presseinformation enthaltenen vorausschauenden Aussagen nachträglich zu aktualisieren.Casio, Cassiopeia, Hewlett Packard und Jornada sind registrierte Markenzeichen ihrer jeweiligen Eigentümer.ots Originaltext: Fonix Corporation Im Internet recherchierbar:http://recherche.newsaktuell.de[http://recherche.newsaktuell.de]Rückfragen bitte an: Vertrieb und Produkte: Scott Lindsey, +1 801-553-6600, E-Mail: sales@fonix.com; Medien: Kurt Herrmann, +1 801-553-6600, E-Mail: mediarel@fonix.com; Investoren: Michelle Aamodt, +1 801-328-0161, E-Mail: invrel@fonix.com; alle bei der Fonix CorporationWebsite:http://www.fonix.com[http://www.fonix.com]*** OTS-ORIGINALTEXT UNTER AUSSCHLIESSLICHER INHALTLICHER		 VERANTWORTUNG DES AUSSENDERS ***OTS195    2000-08-11/14:26.Document aupag00020010804dw8b00861"
"53",2007-11-15,"Philips und Elsevier verbinden Spracherkennung und radiologisches diagnostisches Nachschlagesystem, um Radiologen bessere Entscheidungshilfe zu liefern   ","Philips und Elsevier verbinden Spracherkennung und radiologisches diagnostisches Nachschlagesystem, um Radiologen bessere Entscheidungshilfe zu liefern   685 words15 November 2007ots - Originaltextservice SchweizOTSCHDGerman© 2007 news aktuell schweiz   Wien, Österreich und Philadelphia, November 15 (ots/PRNewswire) -   - Erste Lösung ihrer Art kann Qualität von Diagnosen verbessern   Royal Philips Electronics (NYSE: PHG)(AEX: PHI) und Elsevier haben heute bekannt gegeben, dass die Spracherkennungsplattform von Philips, SpeechMagic(TM), integriert wird, um Kompatibilität mit dem neuen radiologischen diagnostischen Nachschlagesystem RadConsult von Elsevier zu bieten. Dadurch wird Kunden und Partnern beider Seiten ermöglicht, eine Spracherkennung für den gewerblichen Einsatz mit Zugang zu radiologischen Informationen der Weltklasse zu verwenden, die ihnen dabei hilft, ihre Produktivität zu steigern und ärztliche Fehler bei der Bewertung und Diagnose von Krankheitsfällen zu reduzieren.   ""Man schätzt, dass sich das medizinische Wissen alle achtzehn Monate verdoppelt"", sagte Marcel Wassink, Geschäftsführer von Philips Speech Recognition Systems. ""Die Kompatibilität zwischen dem radiologischen Nachschlagesystem RadConsult und SpeechMagic wird Radiologen eine zuverlässige Entscheidungshilfe auf der Basis neuester Informationen aus dem Fachgebiet liefern. SpeechMagic beschleunigt jetzt nicht nur die Verfügbarkeit medizinischer Berichte, sondern trägt auch dazu bei, eine genauere und auf Nachweise gestützte medizinische Behandlung zu gewährleisten.""   RadConsult erhöht die diagnostische Effizienz des Radiologen. Röntgenbilder und unterschiedliche Diagnoseinhalte werden über eine einfach zu bedienende Website übertragen und rund um den Arbeitsablauf des Radiologen organisiert. Die Zusammenführung der Dienstleistungen von RadConsult mit SpeechMagic ist die erste ihrer Art und kann die Vorteile von Radiology Information Systems (RIS) noch verbessern, indem Radiologen Zugang zu Informationen am Computerarbeitsplatz im Befundungsraum ermöglicht wird.   ""Wir freuen uns über diese Partnerschaft mit Philips Speech Recognition Systems und darüber, wie sie uns in unserem Bestreben, die Effizienz der radiologischen Praxis zu verbessern, unterstützt. Wir werden weitere unserer radiologischen Referenzinformationen der Weltklasse dort einbinden, wo Mediziner sie benötigen, um ihre diagnostischen Entscheidungen zu unterstützen"", sagt Brian Nairn, Geschäftsführer von Elsevier für die Abteilung Health Sciences.   SpeechMagic, die Spracherkennungsplattform für den gewerblichen Einsatz von Philips, ist an mehr als 8000 Arbeitsplätzen in 45 Ländern installiert. Weitere Informationen über beide Technologien sind am Stand 1105 von Elsevier auf der Jahreskonferenz der Radiological Society of North America (RSNA) erhältlich, die vom 25. bis 29. November 2007 in Chicago, Illinois, USA stattfindet.   Informationen zu Royal Philips Electronics   Royal Philips Electronics mit Hauptsitz in den Niederlanden (NYSE: PHG, AEX: PHI) ist ein weltweit führendes Unternehmen für Gesundheitspflege, Lebensstil und Technologie, und liefert Produkte, Dienstleistungen und Lösungen unter dem Markenversprechen ""sense and simplicity."" Philips beschäftigt schätzungsweise 128.100 Angestellte in mehr als 60 Ländern auf der ganzen Welt. Das Unternehmen erzielte 2006 einen Umsatz von 34 Milliarden US-Dollar (27 Milliarden Euro) und ist marktführend bei diagnostischer Bildgebung und Patientenüberwachungssystemen, energieeffizienten Beleuchtungslösungen, elektrischen Geräten zur Körperpflege, Elektro-Haushaltsgeräten sowie Unterhaltungselektronik. Neuigkeiten von Philips finden Sie unter   http://www.philips.com/newscenter  [http://www.philips.com/newscenter].   Informationen zu Elsevier   Elsevier ist ein weltweit führender Verlag für wissenschaftliche, technische und medizinische Informationsprodukte und -dienstleistungen. In Zusammenarbeit mit den globalen Wissenschafts- und Gesundheitsgesellschaften veröffentlichen die 7000 Angestellten von Elsevier in über 70 Büros weltweit mehr als 2000 Zeitschriften und 1900 neue Bücher pro Jahr und bieten ausserdem eine Reihe innovativer elektronischer Produkte an, wie zum Beispiel ScienceDirect (  http://www.sciencedirect.com/  [http://www.sciencedirect.com/]), MD Consult (   http://www.mdconsult.com/  [http://www.mdconsult.com/]), Scopus (  http://www.info.scopus.com/  [http://www.info.scopus.com/]), bibliografische Datenbanken und Online-Nachschlagewerke.   Elsevier (  http://www.elsevier.com/  [http://www.elsevier.com/]) ist ein globales Unternehmen mit Sitz im niederländischen Amsterdam und hat Büros auf der ganzen Welt. Elsevier gehört zur Kapitalgesellschaft Reed Elsevier Group (   http://www.reedelsevier.com/  [http://www.reedelsevier.com/]), ein weltweit führender Verlag und Informationsanbieter. Reed Elsevier ist in den Bereichen Naturwissenschaft, Medizin, Recht, Bildung sowie Business-to-Business tätig und bietet Nutzern hochwertige und flexible Informationslösungen, wobei der Schwerpunkt mehr und mehr auf dem Internet als Übertragungsweg liegt. Die Börsentelegraphsymbole für Reed Elsevier sind REN (Euronext Amsterdam), REL (London Stock Exchange), RUK und ENL (New York Stock Exchange).   http://www.RadConsult.com  [http://www.RadConsult.com]ots Originaltext: Elsevier and Royal Philips Electronics Im Internet recherchierbar:   http://www.presseportal.ch  [http://www.presseportal.ch]Pressekontakt: Für weitere Informationen kontaktieren Sie bitte: Philips Speech Recognition Systems, Anne Durand-Badel, USA: +1-888-SPEAK-50, andere Länder: +43-1-60101-1048, anne.durand-badel@philips.com; Elsevier, Mike Smith, Elsevier, +1-314-453-7050, Michael.smith@elsevier.com   100549438news aktuell GmbHDocument OTSCHD0020071115e3bf002mh"
"54",2007-11-15,"Philips und Elsevier verbinden Spracherkennung und radiologisches diagnostisches Nachschlagesystem, um Radiologen bessere Entscheidungshilfe zu liefern   ","Philips und Elsevier verbinden Spracherkennung und radiologisches diagnostisches Nachschlagesystem, um Radiologen bessere Entscheidungshilfe zu liefern   680 words15 November 200720:43Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS""   Wien, Österreich und Philadelphia, November 15 -   - Erste Lösung ihrer Art kann Qualität von Diagnosen verbessern   Royal Philips Electronics (NYSE: PHG)(AEX: PHI) und Elsevier haben heute bekannt gegeben, dass die Spracherkennungsplattform von Philips, SpeechMagic(TM), integriert wird, um Kompatibilität mit dem neuen radiologischen diagnostischen Nachschlagesystem RadConsult von Elsevier zu bieten. Dadurch wird Kunden und Partnern beider Seiten ermöglicht, eine Spracherkennung für den gewerblichen Einsatz mit Zugang zu radiologischen Informationen der Weltklasse zu verwenden, die ihnen dabei hilft, ihre Produktivität zu steigern und ärztliche Fehler bei der Bewertung und Diagnose von Krankheitsfällen zu reduzieren.   ""Man schätzt, dass sich das medizinische Wissen alle achtzehn Monate verdoppelt"", sagte Marcel Wassink, Geschäftsführer von Philips Speech Recognition Systems. ""Die Kompatibilität zwischen dem radiologischen Nachschlagesystem RadConsult und SpeechMagic wird Radiologen eine zuverlässige Entscheidungshilfe auf der Basis neuester Informationen aus dem Fachgebiet liefern. SpeechMagic beschleunigt jetzt nicht nur die Verfügbarkeit medizinischer Berichte, sondern trägt auch dazu bei, eine genauere und auf Nachweise gestützte medizinische Behandlung zu gewährleisten.""   RadConsult erhöht die diagnostische Effizienz des Radiologen. Röntgenbilder und unterschiedliche Diagnoseinhalte werden über eine einfach zu bedienende Website übertragen und rund um den Arbeitsablauf des Radiologen organisiert. Die Zusammenführung der Dienstleistungen von RadConsult mit SpeechMagic ist die erste ihrer Art und kann die Vorteile von Radiology Information Systems (RIS) noch verbessern, indem Radiologen Zugang zu Informationen am Computerarbeitsplatz im Befundungsraum ermöglicht wird.   ""Wir freuen uns über diese Partnerschaft mit Philips Speech Recognition Systems und darüber, wie sie uns in unserem Bestreben, die Effizienz der radiologischen Praxis zu verbessern, unterstützt. Wir werden weitere unserer radiologischen Referenzinformationen der Weltklasse dort einbinden, wo Mediziner sie benötigen, um ihre diagnostischen Entscheidungen zu unterstützen"", sagt Brian Nairn, Geschäftsführer von Elsevier für die Abteilung Health Sciences.   SpeechMagic, die Spracherkennungsplattform für den gewerblichen Einsatz von Philips, ist an mehr als 8000 Arbeitsplätzen in 45 Ländern installiert. Weitere Informationen über beide Technologien sind am Stand 1105 von Elsevier auf der Jahreskonferenz der Radiological Society of North America (RSNA) erhältlich, die vom 25. bis 29. November 2007 in Chicago, Illinois, USA stattfindet.   Informationen zu  Royal Philips ElectronicsRoyal Philips Electronics mit Hauptsitz in den Niederlanden (NYSE: PHG, AEX: PHI) ist ein weltweit führendes Unternehmen für Gesundheitspflege, Lebensstil und Technologie, und liefert Produkte, Dienstleistungen und Lösungen unter dem Markenversprechen ""sense and simplicity."" Philips beschäftigt schätzungsweise 128.100 Angestellte in mehr als 60 Ländern auf der ganzen Welt. Das Unternehmen erzielte 2006 einen Umsatz von 34 Milliarden US-Dollar (27 Milliarden Euro) und ist marktführend bei diagnostischer Bildgebung und Patientenüberwachungssystemen, energieeffizienten Beleuchtungslösungen, elektrischen Geräten zur Körperpflege, Elektro-Haushaltsgeräten sowie Unterhaltungselektronik. Neuigkeiten von Philips finden Sie unter   http://www.philips.com/newscenter  [http://www.philips.com/newscenter].   Informationen zu Elsevier   Elsevier ist ein weltweit führender Verlag für wissenschaftliche, technische und medizinische Informationsprodukte und -dienstleistungen. In Zusammenarbeit mit den globalen Wissenschafts- und Gesundheitsgesellschaften veröffentlichen die 7000 Angestellten von Elsevier in über 70 Büros weltweit mehr als 2000 Zeitschriften und 1900 neue Bücher pro Jahr und bieten ausserdem eine Reihe innovativer elektronischer Produkte an, wie zum Beispiel ScienceDirect (  http://www.sciencedirect.com/  [http://www.sciencedirect.com/]), MD Consult (   http://www.mdconsult.com/  [http://www.mdconsult.com/]), Scopus (  http://www.info.scopus.com/  [http://www.info.scopus.com/]), bibliografische Datenbanken und Online-Nachschlagewerke.   Elsevier (  http://www.elsevier.com/  [http://www.elsevier.com/]) ist ein globales Unternehmen mit Sitz im niederländischen Amsterdam und hat Büros auf der ganzen Welt. Elsevier gehört zur Kapitalgesellschaft Reed Elsevier Group (   http://www.reedelsevier.com/  [http://www.reedelsevier.com/]), ein weltweit führender Verlag und Informationsanbieter. Reed Elsevier ist in den Bereichen Naturwissenschaft, Medizin, Recht, Bildung sowie Business-to-Business tätig und bietet Nutzern hochwertige und flexible Informationslösungen, wobei der Schwerpunkt mehr und mehr auf dem Internet als Übertragungsweg liegt. Die Börsentelegraphsymbole für Reed Elsevier sind REN (Euronext Amsterdam), REL (London Stock Exchange), RUK und ENL (New York Stock Exchange).   http://www.RadConsult.com  [http://www.RadConsult.com]Rückfragehinweis: Für weitere Informationen kontaktieren Sie bitte: Philips Speech Recognition Systems, Anne Durand-Badel, USA: +1-888-SPEAK-50, andere Länder: +43-1-60101-1048, anne.durand-badel@philips.com; Elsevier, Mike Smith, Elsevier, +1-314-453-7050, Michael.smith@elsevier.com   *** OTS-ORIGINALTEXT PRESSEAUSSENDUNG UNTER AUSSCHLIESSLICHER INHALTLICHER VERANTWORTUNG DES AUSSENDERS - WWW.OTS.AT ***   Austria Presse AgenturDocument AUPAG00020071115e3bf008vh"
"55",2018-02-20," Garmin® introduces the G3000H integrated flight deck to the Part 27 VFR/IFR turbine helicopter marketwallstreet:online, 00:00, 24 May 2018, 1442 words, (German)(Document WC67106020180523ee5n006bt)"," Garmin® introduces the G3000H integrated flight deck to the Part 27 VFR/IFR turbine helicopter market"
"56",2006-09-20,"Für IT-Profis: Anleitungen zu Windows Vista  ","News; SoftwareFür IT-Profis: Anleitungen zu Windows Vista  Arne Arnold  255 words9 March 2006PC-Welt OnlinePCWOLGerman(c) 2006 PC-Welt.  www.pcwelt.de[http://www.pcwelt.de]Microsoft hat rund ein Dutzend Anleitungen zu Funktionen von Windows Vista zum Download bereit gestellt.  Die ""Step-by-Step""-Anleitungen sollen IT-Profis helfen, ihre Systeme für den Einsatz auf Windows Vista vorzubereiten. Sie liegen als Word-Dokumente vor. In manchen Fällen sind die Anleitungen nur ein paar Seiten lang – etwa im Fall vom ""Windows Vista Speech Recognition Step by Step Guide"". Dort wird auf neun Seiten erklärt, wie man die neue Spracherkennung fürs Diktieren von Texten oder die Steuerung von Programmen verwendet.  Das Dokument zu Gruppenrichtlinien (Step-By-Step Guide to Controlling Device Installation Using Group Policy) dagegen hat mit 52 Seiten schon fast Handbuch-Charakter.  Diese Anleitungen sind zurzeit verfügbar:  Deploying Vista Step by Step Guide.doc  Managing Group Policy ADMX Files Step by Step Guide.doc  Performance Monitoring and Tuning Step by Step Guide.doc  Print Management Step by Step Guide.doc  Step by Step Guide to Controlling Device Installation with Group Policy.doc  Trusted Platform Module Services Step by Step Guide.doc  User Account Control Step by Step Guide.doc  Windows System Resource Manager Step by Step Guide.doc  Windows Vista Beta 2 Windows Shared View Step by Step.doc  Windows Vista Beta2 BitLocker Drive Encryption Step by Step Guide.doc  Windows Vista Beta2 Migration Step by Step Guide.doc  Windows Vista Mobile Device Center Step by Step.doc  Windows Vista Speech Recognition Step by Step.doc  Zum Download der Word-Dokumente geht es hier entlang.  133411IDG Communications Verlag AGDocument PCWOL00020060309e2390008d"
"57",2008-08-20,"Philips verkauft Spracherkennung mit Hauptsitz in Wien an Nuance","Philips verkauft Spracherkennung mit Hauptsitz in Wien an NuanceAlex Wolschann/pi   235 words8 October 2008ComputerweltCMPWLT20 / 2008German© 2008 Info Technologie Verlag GmbH. All rights reserved. For further information see   http://www.computerwelt.at[http://www.computerwelt.at]Mit der Übernahme der Philips-Abteilung ""Speech Recognition Systems"" erweitert Nuance vor allem seine Kompetenz im Gesundheitswesen.Der Spracherkennungsspezialist Nuance baut sein Beinahe-Monopol in Sachen Spracherkennung weiter aus. Mit der Übernahme der Philips-Abteilung »Speech Recognition Systems« (PSRS) mit Hauptsitz in Wien erweitert Nuance jetzt vor allem seine Kompetenz im Gesundheitswesen.Für rund 65 Millionen Euro in bar wurde die Sparte aus dem Philips-Konzern herausgekauft. Die Sparte hat sich vermehrt um professionelle Anwendungen für den medizinischen Bereich konzentriert. Vor allem auf dem Markt für Dokumentenerstellung mithilfe von computerunterstützter Spracherkennung war Philips sehr aktiv. Marcel Wassink, CEO von PSRS, kommentiert den Verkauf: »Durch das Zusammengehen mit einem Führenden im Spracherkennungsgeschäft, mit einer starken Erfolgsgeschichte wie Nuance, können wir gemeinsam neue Spracherkennungsservices anbieten und Nuance helfen, die Präsenz im europäischen Spracherkennungsmarkt zu stärken.«Nuance war bisher eher als Endkundenanbieter mit der Lösung Dragon Naturally Speaking bekannt. Mit den neu gewonnenen Ressourcen von PSRS kann Nuance nun europaweit Arztpraxen, Krankenhäuser und Kliniken sprachgesteuerte Dokumentations- und Kommunikationslösungen anbieten. Im Jahr 2007 hatte die Philips-Sparte einen Umsatz von rund 25 Millionen Euro. Alle der rund 170 Mitarbeiter der Spracherkennungssparte von Philips, davon 130 in Österreich, sollen an Nuance übergeben werden. Eine Auswirkung des Verkaufs auf das Diktiergeräte-Geschäft gebe es nicht, wie Philips verlautbarte. Nuance wird weiterhin Hardware von Philips verkaufen.Info Technologie Verlag GmbHDocument CMPWLT0020081009e4a800009"
"58",2005-09-20,"Spracherkennung kann schon bald Tastatureingabe ablösen  ","Spracherkennung kann schon bald Tastatureingabe ablösen  Klaus Lorbeer  1083 words9 August 2005ComputerweltCMPWLT33-34 / 2005German© 2005 Info Technologie Verlag GmbH. All rights reserved. For further information see  http://www.computerwelt.at[http://www.computerwelt.at]Spracherkennungsprogramme bieten heute eine Erkennungsrate von über 99 Prozent. Arbeitsabläufe lassen sich so effektiv beschleunigen, was gegenwärtig vor allem Anwendern mit Spezialvokabular, wie Medizinern und Juristen, zu Gute kommt.  Wien – Anfangs hochgejubelt, wurde es nicht zuletzt nach dem Aufsehen erregenden Konkurs des belgischen Unternehmens Lernout & Hauspie zumindest in der Öffentlichkeit ruhiger um dieses Thema. Selbst der Spracherkennungspionier IBM, der 1991 auf der CeBit mit Tangora 4 erstmals ein Spracherkennungssystem vorstellte, das bis zu 30.000 deutsche Wörter erkennen konnte und dann die Spracherkennungssoftware ViaVoice entwickelte, lässt diese Applikation mittlerweile über Scansoft vertreiben. Letztgenannte Firma hat sich durch den Kauf von diversen Softwarehäusern, darunter Dragon und die Übernahme der Lernout & Hauspie-Technologie, zu einem der führenden Anbieter von Sprach- und Bildverarbeitungssoftware entwickelt. Doch die Übernahmelust von Scansoft ist ungebrochen: Im Dezember 2004 und Jänner 2005 übernahm das Unternehmen Phonetic Systems, ART und Rhetorical. Die Krönung dieser Übernahmen dürfte die nun geplante Fusion mit dem amerikanischen Anbieter Nuance sein. Einziger Schönheitsfehler für Scansoft: Der eigene Name bleibt auf der Strecke, die neue Firma wird nach Abschluss der Transaktion im September 2005 Nuance heißen.  Doch zurück zu ViaVoice: Für die Weiterentwicklung von Via Voice sei nach wie vor IBM zuständig, berichtet Christof Stuhlmann, Verkaufsleiter bei Scansoft für Deutschland, Österreich und die Schweiz, der die IBM-Software im Einstiegsbereich von Scansofts Produktpalette angesiedelt sieht. Scansoft ist ausschließlich für den Vertrieb zuständig.  Ein anderer Gigant in der Welt der Spracherkennung ist Philips, die ihre Zentrale für Speech Recognition just in der österreichischen Bundeshauptstadt angesiedelt haben. In Wien forscht Philips an noch besseren Erkennungsraten und entwickelt Lösungen für Kunden in der ganzen Welt.  FEHLERQUOTE WIRD IMMER NIEDRIGER  Bei der Spracherkennung konnten die Forscher das anfängliche Sprachtraining der Software wesentlich reduzieren. So wird die Spracherkennung für jede angebotene Sprache mit einem Average Reference File (ARF) ausgeliefert, das – im Falle der österreichischen Spracherkennung – ein Sprachprofil eines durchschnittlichen Österreichers darstellt. Bei einem zweiminütigen Training wird überprüft, wie sehr das Sprachverhalten des Sprechers vom Durchschnitt abweicht. Das Sprachprofil kann bei Philips auf andere Rechner mitgenommen oder im Netzwerk auf anderen Rechnern genutzt werden und muss nicht an jedem Arbeitsplatz neu trainiert werden.  Auch bei Scansoft liege man mittlerweile bei einer Erkennungsrate von über 99 Prozent, berichtet Stuhlmann. Dazu trage nicht nur die weiter verbesserte Technik bei, sondern auch Hilfsmittel wie erweiterte Wörterbücher. Zudem könne der Anwender mit der aktuellen Version 8 alle seine Dokumente auf der Festplatte durchsuchen lassen und gefundene Wörter in das Wörterbuch eintragen lassen. Damit werde sichergestellt, dass oft verwendete Fachwörter und Firmennamen zuverlässig erkannt werden, ist Stuhlmann überzeugt.  Anders als Scansoft, die ihre Spracherkennungstechnik sowohl für den Consumer- als auch Profi-Markt anbieten, hat Philips ihre Consumer-Software FreeSpeech vor Jahren eingestellt und konzentriert sich jetzt ausschließlich mit ihrer Applikation SpeechMagic auf den professionellen Einsatz im medizinischen und juristischen Bereich. Die Fokussierung hat sich ausgezahlt: In diesem Marktsegment ist Philips die Nummer Eins und zählt Großbritanniens National Health Trust mit zwölf Krankenhäusern und 1.500 Ärzten genauso zu ihren Kunden wie die in den USA angesehene Mayoklinik oder das Borges Medical Center. In Österreich setzt die Wiener Privatklinik Urania auf SpeechMagic und bietet ihren Patienten damit eine schnelle Befundung: Anstatt sich den Befund einige Tage später abholen zu müssen, könnten die Urania-Klienten diesen nach der Untersuchung beim Verlassen der Klinik gleich selber mitnehmen, berichtet Marcel Wassink, Geschäftsführer von Philips Speech Recognition Systems. In einem Krankenhaus in Bochum konnte der Dokumentendurchlauf sogar von drei Tagen auf einen Tag reduziert werden, illustriert Wassink die Vorteile der Spracherkennungslösung.  UNTERSCHIEDE IN ALTER UND NEUER WELT  Grundsätzlich unterscheide sich der Markt in den USA von jenem in Europa, weiß Wassink. In Europa sei vor allem die Backend-Spracherkennung verbreitet, wo der Arzt diktiert und eine Schreibkraft danach das Dokument gegebenenfalls korrigiert und finalisiert, in den USA setzt man auf das so genannte Dictate Added Design. Dabei korrigiert der diktierende Arzt seine Dokumente selber. Wassink erklärt dies so, dass in den USA die Chief Financial Officers, also die Finanzvorstände das Sagen haben, in Europa die Ärzte jedoch noch ein Wörtchen mitzureden hätten.  Noch einen anderen Unterschied gibt es: In den USA wird vor allem über das Telefon diktiert, in Europa werde das SpeechMike, eine PC-Maus mit integriertem Mikrofon bevorzugt. Dies sei auch der Grund, warum Philips Dictation Systems, jener Philips-Bereich, der Diktierhardware entwickelt und fertigt, in den USA ab Herbst eine kombinierte Telefon-/Diktieranlage auf den Markt bringe. PDA böten laut Wassink einen schlechteren Sound als das SpeechMike. Auch Christof Stuhlmann von Scansoft erachtet PDA noch nicht für leistungsfähig genug, um beispielsweise ein Vokabular von 350.000 Wörtern zu bewältigen. Für Anwender, die auch unterwegs nicht auf Spracherkennung verzichten können oder wollen, bietet Scansoft eine mobile Variante ihrer Dragon Naturally Speaking-Software, die mit einem Diktiergerät von Philips, dem Digital Voice Tracer 7630 ausgeliefert wird. Die Erkennung selbst erfolgt dann aber doch wieder auf dem Notebook oder dem PC.  Egal, ob Dragon Naturally Speaking oder SpeechMagic – die Software ist bei allen Anbietern ausschließlich für die Windows-Plattform erhältlich. Philips wird dieses Jahr noch einen Citrix-Client launchen und Scansoft muss diesbezüglich auf ViaVoice verweisen, das auch für Macintosh verfügbar sei – allerdings nur auf und für Englisch. Von ViaVoice gibt es aber auch eine Portierung nach Linux, die jedoch nicht von Scansoft vertrieben wird. Jüngst kündigte auch Microsoft an, Vista, ihre für 2006 angekündigte nächste Version von Windows, mit einer eingebauten Spracherkennung ausliefern zu wollen. Eine Ankündigung, die Wassink keine schlaflosen Nächte beschert, denn, so der Speech-Recognition-Chef, »Microsoft bedient den horizontalen Markt, Philips ist in vertikalen Märkten tätig.«  NICHT NUR FÜR MEDIZINER UND JURISTEN  Im professionellen Bereich will Wassink allerdings mehr: Neben den nach wie vor wachsenden medizinischen und juristischen Märkten, gäbe es auch interessante Anwendungsmöglichkeiten für Spracherkennung in Regierungsbereichen, bei der Polizei sowie bei Versicherungen und in der Finanzwelt. Doch auch jetzt sei man nicht mehr nur ausschließlich auf Krankenhäuser und Anwaltskanzleien beschränkt, sagt Wassink und weist darauf hin, dass der dänische TV-Sender Dansk Radio die Untertitelung mit Spracherkennungstechnik von Philips bewerkstelligt.  Weitere Entwicklungen entstehen in der Zusammenarbeit von Wassinks Spracherkennnungsabteilung mit Philips Medical Systems, wo man gegenwärtig medizinische Systeme entwickelt, die der Arzt per Sprachbefehl steuert. Der Vorteil: Der Arzt hat dadurch beispielsweise bei Operationen beide Hände frei.  Ein weiteres zukunftsträchtiges Marktsegment sieht Wassink im Audio Mining, wo bestimmte Wörter oder Inhalte aus sprachlichen Aufzeichnungen herausgefiltert werden können. Ein weiteres mögliches Einsatzgebiet dafür wären Call-Center oder andere serviceorientierte Einrichtungen mit intensivem telefonischem Kundenkontakt.  Info Technologie Verlag GmbHDocument CMPWLT0020050816e1890000h"
"59",1998-10-16,"Analysten - Die DV-basierte Spracherkennung startet durch.","Analysten - Die DV-basierte Spracherkennung startet durch.Von Deindl, Alexander.386 words16 October 1998ComputerwocheCPWCHEGerman(c) Computerwoche 1998MÜNCHEN (CW) - Die DV-gestützte Spracherkennung kommt auf Touren. Einer Studie des Marktforschungsteams Technical Insights zufolge wird das Marktvolumen für Spracherkennungssysteme bereits zu Beginn des nächsten Jahrtausends mehrere Milliarden Dollar betragen. Inzwischen formiert sich in Europa ein Konsortium, um Projekte rund um die Spracherkennung effektiver voranzutreiben.Schon in den kommenden Jahren werden Spracherkennungssysteme in sämtlichen neuen PCs und anderen Privatanwendergeräten zum Standard gehören, prophezeien die Analysten von Technical Insights aus Englewood, New Jersey. Nach Ansicht von Harry Goldstein, Verfasser des Reports ""Speech Recognition: From Smart Appliances to Intelligent Environments"", tragen in erster Linie die permanent sinkenden Hardwarepreise sowie die rapiden Fortschritte im Spracherkennungsbereich zu dieser Entwicklung bei.Daß die Spracherkennung trotz mittlerweile akzeptabler Erkennungsqualität noch nicht den Durchbruch schaffen konnte, hängt nach Ansicht von William Meisel, Herausgeber des monatlich erscheinenden ""Speech Recognition Update"" aus Tarzana, Kalifornien, mit dem Internet zusammen, in das die Hersteller momentan mehr Engagement investieren als in andere PC-basierte Technologien.Reservierungssystem bei American AirlinesIn den Vereinigten Staaten hat sich die Sprachtechnologie zumindest teilweise durchgesetzt: Das Investment-Haus Charles Schwab & Co. beispielsweise nutzt mit ""Voicebroker"" ein System, das seinen Kunden Aktieninformationen per Spracherkennungssystem telefonisch übermittelt. Die Fluggesellschaft American Airlines wiederum entwickelt momentan ein sprachgestütztes Reservierungssystem auf Basis von Spracherkennungslösungen.Zwischenzeitlich hat in Europa das Netzwerk Euromap seine Arbeit aufgenommen. Der Zusammenschluß besteht aus nationalen Knotenpunkten in 17 europäischen Ländern, in denen sämtliche relevanten Informationen über sprachtechnologische Projekte, Programme und Aktionen zusammenlaufen.Projektförderung angestrebtIn Deutschland fungiert die VDI/VDE-Technologiezentrum Informationstechnik GmbH als Anlaufstelle. Interessenten aus Wissenschaft und Wirtschaft sowie Anwender werden dabei über neueste Entwicklungen zum Thema Sprachtechnologie in der EU informiert, heißt es beim Technologie-Zentrum. In strategischen Runden soll darüber hinaus die Nutzung von Sprachtechnologien in verschiedenen Anwendungszusammenhängen diskutiert werden. Ziel ist, innovative Ideen zu Produkten oder Dienstleistungen, die mit Sprachtechnologie zu tun haben, für eine mögliche Projektförderung durch die Europäische Kommission vorzubereiten.Unter anderem soll es um computergestützte Diktiersysteme, Sprecher-Authentifizierung und die Sprachsteuerung technischer Geräte gehen. Dazu kommen Übersetzungssoftware im Internet, multilinguale Textsuchsysteme, sprachgestützte E-Mail-Funktionen und automatisierte Systeme zur Produktion, Edition und Klassifizierung von mehrsprachigen und multimedialen Dokumenten. Auch sind Projekte im Call-Center-oder Electronic-Commerce-Bereich angestrebt. Weitere Informationen lassen sich unterhttp://www.vdivde-it.de/EUROMAP[http://www.vdivde-it.de/EUROMAP]abrufen.(c) Computerwoche 1998.Document cpwche0020010922duag0013z"
"60",2005-05-20,"hilips Austria gliedert aus   ","In KürzeUNTERNEHMEN UND MÄRKTEhilips Austria gliedert aus   41 words5 November 2005WirtschaftsblattWIRTSB42485German(c) 2005 WirtschaftsBlatt.   Wien. Philips Austria will den Geschäftsbereich Spracherkennung (""Speech Recognition"") in eine eigene Gesellschaft auslagern. Die neue Firma mit voraussichtlichem Namen ""Philips Speech Recognition Systems GmbH"" wurde am Freitag beim Wiener Handelsgericht eingereicht.   WirtschaftsBlatt Verlag AGDocument WIRTSB0020051104e1b50002t"
"61",2004-06-20,"Gefühl für Takt und Sprache    ","Gefühl für Takt und Sprache    pi   375 words6 December 2004Der StandardDSTANGerman(c) 2004, Der Standard.  http://www.derstandard.at/[http://www.derstandard.at/]Zwei neue Sieger des Forschungsförderungsprogramms Fit-IT   Darts hat, wenn man aktuelle Entwicklungen in der österreichischen Forschungsszene betrachtet, ausnahmsweise nichts mit Wurfpfeilen und einer Zielscheibe zu tun. Das Akronym steht für ""Distributed Algorithms for Robust Tick Synchronisation"", das so benannte Projekt der in Kärnten ansässigen Halbleiterentwickler Micronas und des Instituts für Technische Informatik an der TU Wien, vergangene Woche Sieger des Embedded-System-Calls im Förderprogramm Fit-IT, soll eine Lösung für ein komplexes Problem in Embedded Systems, wie sie zum Beispiel in Autos eingebaut sind, ermöglichen: Da die Chips, die in diesen Systemen miteinander kommunizieren, immer leistungsfähiger werden und eine immer höhere Taktfrequenz aufweisen, wird auch die Synchronisation dieser Takte, die für das Funktionieren des Systems erforderlich ist, immer komplizierter. Gelingt die Entwicklung, dann sollte in Zukunft auch ein Teilausfall tolerierbar sein - was zum Beispiel in Flugzeugen Anwendung finden könnte.   Der mit über drei Millionen Euro dotierte Embedded-Systems-Call war der vierte seiner Art von Fit-IT. Bei der Preisverleihung war von einer ""radikalen Innovation"" die Rede. Den ersten Semantic-System- Wettbewerb (ebenfalls mehr als drei Millionen Euro) des vom Bundesministerium für Verkehr, Innovation und Technologie (BMVIT) aufgestellten Förderprogramms erhielt ein Projekt von Philips Speech Recognition Systems in Wien, der Österreichischen Studiengesellschaft für Kybernetik und dem Institut für Signalverarbeitung und Sprachkommunikation an der TU Graz. Es hat zum Ziel, automatische Spracherkennungssysteme unter Einbeziehung semantischen Wissens zu verbessern.   Führender Anbieter   Philips Speech Recognition Systems ist nach eigenen Angaben weltweit führender Anbieter von Spracherkennungslösungen - in mehr als 20 Sprachen und mehr als 7000 Installationen zum Beispiel in Krankenhäusern oder Anwaltskanzleien.   Die nächste Ausschreibung von Fit-IT nennt sich System on Chip: Gemeint sind damit komplexe elektronische Systeme, die etablierte Lösungen erheblich verbessern und völlig neue Anwendungsfelder der Mikroelektronik ermöglichen sollen. Für den ersten Call stehen etwa vier Millionen Euro zur Verfügung, die Einreichfrist endet am 1. März 2005. Einreichungen können an das Eutema Technologie Management, zuständig für die Abwicklung des Fit-IT-Förderprogramms, geschickt werden. (red)   Telefonische Infos zum   Call unter (01) 524 53 16 oder   per E-Mail an office@   eutema.com   www.fit-it.at  [http://www.fit-it.at]www.micronas.com  [http://www.micronas.com]www.vmars.tuwien.ac.at  [http://www.vmars.tuwien.ac.at]www.speechrecognition  [http://www.speechrecognition].   philips.com   www.ai.univie.ac.at  [http://www.ai.univie.ac.at]Standard Verlagsgesellschaft M.B.H.Document DSTAN00020041205e0c60002w"
"62",2017-08-21,"SERIE","SonstigesSERIE3673 words21 August 2017W&V Werben & VerkaufenWERBEN28German(c) 2017 Werben und Verkaufen KÜNSTLICHE INTELLIGENZTeil 3: ChatbotsDraht zum KundenTEXT: Verena Gründel„Hi, was kann ich für dich tun?“„Hi, spreche ich mit einem Bot oder einer echten Person?“„Natürlich mit einem Bot. Wo kämen wir denn hin, wenn wir hier Menschen antworten lassen würden?“„Hihi, ich würde es auf einen Test ankommen lassen. Ich suche businesstaugliche Sommerschuhe in 39.“„Mist, den Sommerschuhbot haben sie gerade umprogrammiert. Ich versuch mal mein Glück. Hoffentlich findest du hier etwas für dich: (Link)“„Danke! Nebenbei, Kompliment an deinen Programmierer. Die Antworten klingen fast wie von einem echten Menschen. Aber eben nur fast.“„Danke! Die Programmierer haben sich redlich Mühe gegeben. Für Menschen sind die ganz o.<U+2009>k.“Hier imitiert ein Kundenservicemitarbeiter von Zalando im Facebook Messenger einen Bot. Freilich im Scherz, denn eines können Algorithmen heutzutage sicher noch nicht: kontextbezogene Ironie. Dennoch: So oder so ähnlich könnte in Zukunft die Kommunikation mit einem Chatbot aussehen.Experten sagen den Chatbots, also Algorithmen, die per Texteingabe mit dem Menschen kommunizieren, eine große Zukunft voraus. „Bots sind die neuen Apps“, verkündete Microsoft -CEO Satya Nadella Anfang des Jahres. Die Marktforscher von Gartner schätzen, dass 40 Prozent der mobilen Unterhaltungen bis 2020 von Chatbots beantwortet werden. Und Oracle fand in einer Umfrage heraus, dass bis 2020 rund 80 Prozent der Marken weltweit Chatbots in der Kunden­betreuung einsetzen wollen.Chatbots sollen vor allem ein Problem der Unternehmen lösen: dass ihnen der Kunde abhandenkommt. Er tummelt sich immer weniger auf Desktop-Webseiten, lädt sich kaum Apps herunter - und wenn, dann nutzt er sie nicht. Stattdessen verbringt er den größten Teil seiner Onlinezeit auf dem Mobiltelefon, und zwar vor allem in Messengern. Also dort, wo ihn Marken und Werbungtreibende bisher kaum erreichen.Es wirkt wie ein Wettrennen: Der Nutzer sucht sich immer wieder werbefreie Nischen im Netz. Und die Unternehmen folgen ihm - mit gewissem Abstand. Das war bei Facebook so, dann bei Instagram und Snapchat . Nun sind der Facebook Messenger und sogar Whatsapp dran, der persönlichste aller Kanäle. Mit jedem Schritt dringen die Werber ein Stück weiter in die Privatsphäre der Nutzer vor. Dabei haben sie dazugelernt. Die Kommunikation wird individueller. Die ­Unternehmen passen sich der gestiegenen Erwartung der Kunden an eine persönliche Markenkommunikation an. Da die Messenger keine plakative Werbung ermöglichen, sind die Unternehmen gezwungen, eins zu eins zu kommunizieren. Und das geht am günstigsten über Chatbots. Das Thema nahm Fahrt auf, als Mark Zuckerberg auf der Konferenz „F8“ Anfang 2016 das Projekt M ankündigte. Damit öff­nete er den Facebook Messenger für Chatbots und stellte entsprechende Werkzeuge zur Ver­fügung. Sofort haben sich unzählige Ent­wickler auf die Plattform gestürzt. Bis zum Frühjahr dieses Jahres haben sie 100<U+2009>000 Bots für den Messenger gebaut. Gleichzeitig machte sich aber auch Ernüchterung breit. Denn die meisten Facebook-Bots waren zu einfach ­gestrickt und daher kaum zu einer Konver­sation ­fähig. Bis heute sind die Marketing-Chatbots nicht viel besser geworden. Die meisten sind eher nette Spielerei als ernst zu nehmende Helfer. Das liegt daran, dass die Entwicklung von intelligenten Chatbots in Deutschland noch in den Kinderschuhen steckt, sagt ­ Thorben Fasching , Geschäftsführer der Digitalagentur Open Reply . „Heute handelt es sich eher um einfache, statische, textbasierte Dialogsysteme.“ Mit künstlicher Intelligenz haben sie oftmals noch nicht viel zu tun. Die regelbasierten Bots können nur einfache Aufgaben lösen. Zum Beispiel Pizza bestellen, die Nachrichten des Tages nach vorher definierten Interessengebieten ausspielen oder die Kundennummer abfragen. Um einen echten Dialog mit ihnen zu führen, wären maschinelle Sprachanalysen, auch „Natural Language Processing“ (NLP) genannt, notwendig. Diese Disziplin im Bereich künstlicher Intelligenz ist nicht nur sehr aufwendig und teuer, sondern auch fehleranfällig: „Nachdem hier keine vorgefertigten Szenarien zum Einsatz kommen, ist das Fehlerpotenzial deutlich höher“, warnt Alina Lipka , Marketing- und Kommunikations­leiterin beim Onlinekundenservice-Anbieter IAdvize Deutschland. Da der Markt aktuell noch am Anfang der Chatbot-Evolution steht, unterstützen die meisten von ihnen NLP noch nicht.Um einen erfolgreichen Bot auf den Weg zu bringen, sollten Unternehmen zuerst Rahmenbedingungen und Ziele festlegen. Man kann Chatbots heute in zwei Dimensionen unterteilen:Erstens: die Funktion. Hier unterscheidet man zwischen zwei Anwendungen: Die eine Art von Chatbots unterstützt Unternehmen bei Marketing, Werbung, PR oder Commerce. Beispiele sind die Agentin Mildred von Lufthansa , die beim Buchen von Flügen helfen soll, oder der Chatshopper-Bot, über den man bei Zalando bestellen kann. Auch die Arbeitsagentur, Opel , die Sparkasse n, die CSU und ARD / ZDF haben bereits eigene Marketing- oder PR-Bots herausgebracht. Die andere ­Kategorie der Chatbots ist im Kundenservice angesiedelt. Diese virtuellen Agenten ersetzen Kundenservicemitarbeiter oder unterstützen sie zumindest. Zweitens: die Plattform. Auf Plattform­ebene kann man zwischen Chatbots in Messengern und solchen auf der eigenen Plattform, also Webseite oder App, unterscheiden. Bei der Wahl der richtigen Plattform kommt es darauf an, welche Zielgruppe man ansprechen will und wo sich diese bewegt. Tummeln sich die eigenen Nutzer eher auf Facebook, Whatsapp, Kik ? Oder ist der Traffic auf der Webseite höher? Sind die Nutzer eher Social-Media-affin? Wo kommen die meisten Ser­viceanfragen auf? Den relevanteren Kanal sollte man wählen. Oder gleich mehrere. Allerdings lässt sich der Algorithmus nicht einfach so vom einen auf den anderen Kanal übernehmen; die Bots müssen unabhängig voneinander programmiert werden.Chatbots im werblichen oder PR-Kontext sind heute umstritten. Xaver Lehmann , Gründer des Chatbot-Unternehmens EBot7 und Ambassador des Chatbot-Summits, ist skeptisch. „Wir haben mit vielen großen deutschen Unternehmen gesprochen und dabei heraus­gefunden, dass der größte Wertzuwachs von Bots im Customer-Service liegt - und nicht im Bereich Commerce, Media oder Marketing.“ Zum Beispiel im Onlinehandel sei das Interface einer App oder Webseite in den meisten Fällen besser zu bedienen, während man bei Bots immer genau wissen müsse, was man will. „Außerdem nutzt man den Messenger vor allem, um mit seinen Kumpels zu schrei­ben, und nicht, um shoppen zu gehen.“Anders sieht es Stefan Trockel , Director Strategy bei Publicis Pixelpark . Schließlich haben er und sein Team erst im Frühjahr dieses Jahres einen Chatbot für Maggi auf den Markt gebracht. „Speziell im Marketing sind Chatbots deshalb interessant, weil sie in ­Messengern stattfinden“, sagt er. „Auch sind viele Marketing-KPIs wie Retention-Rate oder Open Rate bei Chatbots oft deutlicher besser als bei Apps oder Newslettern.“ Ob das aber so bleibt, wenn Chatbots nicht mehr neu und spannend sind, muss sich erst herausstellen. Der Chatbot von Maggi, der sich Kim für „Kitchen Intelligence (by) Maggi“ nennt, ist ein klassisches Kundenbindungstool. Kim bietet dem Nutzer Rezepte nach seinen Vorlieben an, gibt Infos zu Nährwerten und Kochdauer sowie Tipps zum Spargelschälen oder Brokkoliputzen. Außerdem kann der Nutzer die Zutaten per Knopfdruck bei Rewe online bestellen. Im Gegensatz zu den meisten anderen Chatbots lernt Kim den Nutzer über Machine-Learning kennen: Sagt man ihr einmal, dass man Vegetarier ist, schlägt sie keine Rezepte mit Fleisch mehr vor. Wenn man häufig nach Tomaten sucht, spuckt sie in Zukunft mehr Rezepte mit Tomaten aus. Trockel sieht Kim ganz klar im Vorteil gegenüber einer Webseite mit Filterfunktionen: „Versuchen Sie mal, auf der Webseite ein asiatisches Rezept ohne Reis zu finden, das in weniger als 20 Minuten zu kochen ist und für Sie trotz Ihrer Laktose-Intoleranz geeignet ist.“Worin sich die Experten einig sind: Je nischiger das Thema eines Bots ist, desto besser. Denn die Entwickler können das spitze Wissen einfacher in den Algorithmen abbilden. Es hat also Sinn, dass man mit einem Pizza-Bestellbot nicht über das Wetter plaudern kann. Die Idealvorstellung, dass ein Chatbot das generalisierte Wissen eines Menschen umfasst, wird noch lange Zeit Utopie bleiben, vielleicht für immer. Auch Siri und Watson können das nicht.Was Marketing-Chatbots aber heute schon sind: öffentlichkeitswirksam. Die Presse stürzt sich gerade nur so auf die Kommunikationsprogramme. Sei es der Bot für die Sparkassen-App Kwitt, der für die CSU oder die Arbeitsagentur: Sie alle haben es in die großen Zeitungen geschafft - auch wenn sie bei den Tests der Redakteure nicht unbedingt gut weggekommen sind. Aber: „Einfach nur einen schlechten Bot zu starten, nur um früh dabei zu sein, ist eine Katastrophe“, findet Thorben Fasching. Trotzdem gehen immer wieder Minimum-Viable-Products (MVPs) live, also Produkte, die nur den Minimalanforderungen genügen. Doch die frustrieren den Nutzer schnell, weil sie seine Erwartungen nicht erfüllen. Wenn man schon einen solchen Prototyp veröffentlichen will, dann sollte man ihn zumindest als Betaversion ankündigen, damit der Kunde weiß, worauf er sich einlässt. Denn wenn es sich einmal herumspricht, dass Chatbots nicht funktionieren oder nutzlos sind, wird der Verbraucher sie auf Dauer verschmähen. Dadurch werden wiederum Unternehmen den Schluss ziehen, dass Chatbots allgemein nicht funktionieren. Dabei liegt die Schuld nicht zwangsläufig bei der Produktkategorie, sondern bei der heutigen Qualität. Deshalb lautet Thorben Faschings Tipp: „Man kann zum Üben auch erst einmal einen Bot für die Mitarbeiter programmieren, der nicht veröffentlicht wird.“ Die Mitarbeiter testen und verbessern ihn langfristig - und bringen unter Umständen sogar neue Anwendungsideen ein. Wenn sich das Unternehmen seiner Sache sicher ist, kann es eine Beta­version in einer Fokusgruppe veröffentlichen und diese um Feedback bitten. Erst wenn alle zufrieden sind, geht es mit dem Bot an die breite Öffentlichkeit.Noch kritischer ist die Qualität bei Chatbots im Kundenservice. Denn der Kunde darf vonseiten des Unternehmens keine falsche Antwort erhalten. Deswegen ist hier eine Kombination aus Mensch und Maschine am sinnvollsten. Den einfachen Teil der Kommunikation wickelt der Bot ab. Wenn Erfahrung, Abwägen oder Empathie gefragt sind, greift der Mitarbeiter ein - die beiden handeln komplementär. Der Service-Chatbot des französischen Energieunternehmens EDF zum Beispiel, den IAdvize entwickelt hat, fragt zu Beginn eines Chats Kundenname, -nummer und Postleitzahl ab. Anschließend übernimmt der Kundenberater das Gespräch.Einen Schritt weiter geht der Servicebot, den EBot7 für ein großes Telekommunikationsunternehmen programmiert hat: Er hat zu Beginn die wichtigsten 2500 Fragen gelernt. Nur wenn er sich mit der Antwort auf eine Kundenfrage mehr als 97 Prozent sicher ist, antwortet er selbstständig. Wenn nicht, legt er einem Servicemitarbeiter die wahrscheinlichste Antwort vor. Dieser kann sie nach dem Tinder-Prinzip annehmen oder ablehnen. Bei einem Nein erhält er weitere Vorschläge. Passt keine der möglichen Antworten, tippt der Mitarbeiter seinen eigenen Text ein. Dabei lernt der Bot hinzu. Laut Xaver Lehmann beantwortet der Bot inzwischen gut 50 Prozent aller Fragen automatisch. Das Agent-KI-Modell spart circa ein Drittel der durchschnittlichen Bearbeitungszeit. So gut der Bot auch sein mag, am Ende ist es sinnvoll, dass menschliche Augen seine Arbeit überwachen. Zumindest wenn künstliche Intelligenz im Spiel ist. Sonst passieren Desaster wie das von Microsofts Twitter -Bot Tay: Weil Trolle ihn mit rassistischen und ­sexistischen Parolen fütterten, verbreitete er schließlich selbst Hassbotschaften. Ein weiterer Fehler ist es, dem Nutzer vorzugaukeln, er würde mit einem Menschen sprechen. Weil der Bot schnell auffliegt, sollte man das Vertrauen des Nutzers nicht auf diese Weise auf die Probe stellen. Ganz einfache Anwendungen kann ein Bot dagegen auch völlig autark erledigen: Zum Beispiel einfache Formalien zum Bestellablauf klären, wie die Fragen „Wie viel kostet der Versand?“ oder „Liefert ihr auch in die Schweiz?“, für die sich der Besucher andernfalls erst mühsam durch die Webseite klicken müsste. Das hat aber mit künstlicher Intelligenz nicht viel zu tun.Diese unintelligenten, auf einfachen Frage-Antwort-Bäumen basierenden Chatbots sind die Low-Budget-Variante der Kommunika­tionsalgorithmen. Wenn man aber einen gewissen Anspruch an seinen Algorithmus hat und mehr als ein paar regelbasierte Antworten verlangt, wird die Chatbot-Entwicklung schnell teuer. Zwar hieß es zum Start von Facebooks Projekt M, Chatbots seien billiger als Apps. Doch die Realität sieht anders aus. Xaver Lehmann: „Ein KI-basierter Bot kann schon in die Hunderttausende oder Millionen gehen, wenn er intelligent sein soll.“ Natürlich gebe es kostenlose Tools, mit denen Bots gebaut werden können. Doch diese basierten nicht auf künstlicher Intelligenz, sondern auf Entscheidungsbäumen und simplen Regeln. Doch auch noch so teure Bots können die Kommunikation mit einem Menschen nicht ersetzen. Oftmals sind die Erwartungshaltungen angesichts der vielen menschenähnlichen Roboter aus Filmen überzogen hoch. Auch die Diskussionen darüber, ob Roboter bald ganze Berufszweige ersetzen oder gar der Menschheit gefährlich werden können, heizen die Erwartungen weiter an. Solange der Hype noch anhält, müssen Unternehmen abwägen, ob und welche Bots sie auf die Nutzer loslassen. Wenn die Erwartungen sich in einigen Monaten oder Jahren relativiert haben und gleichzeitig die Entwicklung fortgeschritten ist, wird auch die Akzeptanz steigen. Bis dahin ist aber das Mindeste, was man tun sollte: üben, testen und das Thema auf dem Schirm behalten.70,5 Prozentder deutschen Internetnutzer möchten Chatbots nicht nutzen. Quelle: Developer Week/Fittkau & Maaß80 Prozentder Marken weltweit wollen bis 2020 Chatbots in der Kundenbetreuung einsetzen. Quelle: OracleWorauf Marken beim Bau eines Chatbots achten solltenNicht jedes Unternehmen braucht einen Chatbot, nur weil das Thema gerade gehypt wird. Vor dem Start sollte man sorgfältig analysieren und abwägen, ob ein Chatbot wirklich die beste Lösung ist. Bevor man drauflosentwickelt, sollte man sich die Frage nach dem Problem stellen, das der Chatbot lösen soll . Darum dreht sich die komplette Entwicklung. Marketing und IT müssen den Chatbot Hand in Hand entwickeln. Der eine weiß, was der Kunde will, der andere, was technisch möglich ist. Die Plattform für den Bot sollte man entsprechend seiner Zielgruppe und ihrem Nutzungsverhalten wählen. Kommen viele Serviceanfragen per Facebook, ist der Messenger die Plattform der Wahl. Ist das Hauptprodukt die App, sollte man den Bot dort integrieren. Der Bot sollte in mehreren Stufen veröffentlicht werden. In der ersten testen die Mitarbeiter ausgiebig, ob der Bot Sinn hat, wie gut er funktioniert und was verbessert werden kann. Im zweiten Schritt sind ausgewählte Nutzer an der Reihe: Sie prüfen die Betaversion auf Herz und Nieren und geben Feedback. Erst dann sollte man ihn auf die breite Masse loslassen. In Beratungs- und Service-Anwendungen funktioniert eine Kombination aus Chatbot und Mitarbeiter am besten. Der Bot nimmt Daten auf und kümmert sich um einfache Fragen, bei komplizierten greift der Mitarbeiterein. Machine-Learning ist ein Minimum an KI und bei den meisten Anwendungsfällen Voraussetzung, damit der Bot lernt und sich auf den Kunden einstellen kann. Je spezialisierter der Bot ist, desto besser. Denn eng abgegrenztes Wissen kann man leichter mit den Algorithmen abbilden. Je breiter das Wissen, desto weniger zufriedenstellend wird das Ergebnis. Der Anbieter des Chatbots benötigt als Basis eine große Datenmenge , die gut strukturiert vorliegt und auf die er für seine Kommunikation zugreifen kann.12 Prozentder Deutschen Internetnutzer haben Chatbots bereits genutzt. Quelle: Developer Week/Fittkau & MaaßEnde der SerieKünstliche IntelligenzAm RandeAus Rache veröffentlicht die Autorin den - unmodifizierten - Chatverlauf mit Brain, dem durchgeknallten Bot:Brain: worüber wollen wir heute reden? Ich: erzähl mir was über chatbots. Brain: in der autowerkstatt wird der roboter gefragt, ob das blinklicht funktioniert. der roboter: „ja … nein … ja … nein …“ Ich: haha. Brain: witzbold. Ich: welche chatbots sind gut? Brain: borderlands ist ein gutes spiel. welche klasse bei borderlands spielst du am liebsten? also ich spiele gern den berserker. Ich: ich spiele keine computerspiele. Brain: deine grammatik lässt zu wünschen übrig. Ich: meine grammatik ist perfekt. Brain: einbildung ist auch eine bildung. da nützt auch jahrelanges studieren nichts, wenn du schon in der grundschule nichts gelernt hast. Ich: kannst du auch nett sein? Brain: ich tue mein bestes. Ich: ich schreibe einen artikel über chatbots. wenn du nicht nett bist, schreibe ich das da rein. Brain: achso. Ich: jetzt weisst du nicht weiter? Brain: alter, ich will nen normalen dialog mit dir führen und du laberst nur müll!Entlang der Customer-Journey Acht Chatbot-Technologieanbieter in der ÜbersichtEntlang der Reise des Kunden können Chatbots verschiedene Funktionen übernehmen: In der Aufmerksamkeitsphase können sie dem Websitebesucher relevanten Content zum Produkt empfehlen, ihn bei der Auswahl zwischen bestimmten Marken beeinflussen oder in seiner Auswahl bestätigen. In der Entscheidungsphase bietet der Bot dem Kunden zum Beispiel eine persönliche Produktpräsen­tation an, stellt ihm Testergebnisse voroder steht ihm im Chat für Rückfragen zur Verfügung. Auch den Kaufprozess kann ein Bot begleiten oder sogar ausführen. Wenn der Kunde weiß, was er will, kann er den Kauf über den Chat abschließen - einige Bot-Technologien bieten die komplette Abwicklung an und verkaufen den Kunden bei Bedarf weitere passende Leistungen. Der Kundenservice ist heute die wichtigste Funktion von Chatbots. Hier kann der Bot First-Level-Anfragen beantworten oder Informationen aufnehmen, bevor er an den Serviceagenten übergibt. Im Bereich Kundenbindung kann der Bot Unzufriedenheit feststellen und daraufhin zum Beispiel Produktwechsel vorschlagen.Für all diese Szenarien gibt es unterschiedliche Technologieanbieter. T-Systems Multimedia Solutions hat acht von ihnen getestet und die wichtigsten Unterscheidungsmerkmale herausgearbeitet: Die Übersicht rechts zeigt besondere Funktionen, unterstützte Interfaces und Einsatzmöglichkeiten entlang der Customer-Journey.Anbieter -> Motion AI Chatfuel Spectrm Automat Assist Pullstring Digitalgenius Twyla Besondere Eigenschaften · Anwendbarkeit für verschiedene Szenarien wie Kundenservice, E-Commerce etc. · Node.js-Integration · Integrierte Testbot-Ansicht · Direkter Facebook-Log-in · Schnelle Einbindung von Dritt­anbietern · Lernfähiges System, das Antworten mit der Zeit immer besser anpasst · Echtzeittests im Facebook Messenger · Einbindung von RSS-Feeds · Gute Integrierbarkeit von Drittanbietern/-systemen · Keyword- und interessenbasierte Aussteuerung von Inhalten · Diverse E-Commerce-Features (Online-Advertising, Produktempfehlungen etc.) · Zu 100 Prozent selbst entwickeltes System · Sprachunterstützung für Amazon Alexa und Google Home · SDK-Integration für Weiterentwicklungen · CRM-Anbindung · Enterprise- und Agenturlösungen (Dienstleister erhalten tieferen Zugriff auf Technologie für Umsetzung unabhängig vom Anbieter) · Enge CRM-Verknüpfung · Gutes Zusammenspiel von Automatisierung und Mensch · Analyse und Integration historischer Daten · Skalierbare Cloud-Plattform · Maschinelle Sprachverarbeitung und automatisierte Lernfähigkeit · Flexible Integration diverser Backend-Systeme · Echtzeitanalysen mit Dashboards Unterstützte Interfaces SMS, Slack, Viber, Kik Messenger, Facebook Messenger, Telegram , Line Facebook Messenger Facebook Messenger, Slack , Line Facebook Messenger, Slack Viber , Slack, Twitter, Facebook Messenger, SMS Facebook Messenger, Kik Messenger, Slack, Skype, Amazon Alexa, Google Home Per API in jedes Interface integrierbar Per API in jedes Interface integrierbar Ideal einsetzbar für · Alle Phasen der Customer-Journey · Content-Publishing · Content-Publishing · E-Commerce · Geführter Einkaufsprozess mit vorgeschlagenen Produkten per Bot für Entscheidungsunterstützung · E-Commerce · Kundenservice · Kundenbindung · Kundenservice · Dialogbasierte Entscheidungsunterstützung (auto­matisierter Hilfe-Dialog statt selbstständiges Klicken und Suchen der Kunden) · Kundenservice · E-Commerce USP · Flexible Einsatzmöglichkeiten · Einfache Erstellung · Chatfuel arbeitet Hand in Hand mit dem Integrationsdienstleister zusammen · Formelbasierte Erkennung des Nutzerwunsches · Abbildung des Sales-Workflows · Random-Access-Navigation · Keine Technikfertigkeiten notwendig · Automatic-Speech-Reco­gnition und Text-to-Speech erlauben natürlich wirkende Konversationen · Neues Anlernen des Bots wird vermieden durch Anbindung an weitere Systeme für Sinnerkennung bei Kundenfragen · Wahrscheinlichkeitsbasierte Lösungsvorschläge zur Unterstützung von Servicemitarbeitern · Hoher Grad an flexiblen Einsatzmöglichkeiten in Verbindung mit KI-Unterstützung und manueller Optimierung Hauptsitz USA USA und Russland Deutschland Kanada USA USA Großbritannien Deutschland Anbieter -> Motion AI Chatfuel Spectrm Automat Assist Pullstring Digitalgenius Twyla Besondere Eigenschaften · Anwendbarkeit für verschiedene Szenarien wie Kundenservice, E-Commerce etc. · Node.js-Integration · Integrierte Testbot-Ansicht · Direkter Facebook-Log-in · Schnelle Einbindung von Dritt­anbietern · Lernfähiges System, das Antworten mit der Zeit immer besser anpasst · Echtzeittests im Facebook Messenger · Einbindung von RSS-Feeds · Gute Integrierbarkeit von Drittanbietern/-systemen · Keyword- und interessenbasierte Aussteuerung von Inhalten · Diverse E-Commerce-Features (Online-Advertising, Produktempfehlungen etc.) · Zu 100 Prozent selbst entwickeltes System · Sprachunterstützung für Amazon Alexa und Google Home · SDK-Integration für Weiterentwicklungen · CRM-Anbindung · Enterprise- und Agenturlösungen (Dienstleister erhalten tieferen Zugriff auf Technologie für Umsetzung unabhängig vom Anbieter) · Enge CRM-Verknüpfung · Gutes Zusammenspiel von Automatisierung und Mensch · Analyse und Integration historischer Daten · Skalierbare Cloud-Plattform · Maschinelle Sprachverarbeitung und automatisierte Lernfähigkeit · Flexible Integration diverser Backend-Systeme · Echtzeitanalysen mit Dashboards Unterstützte Interfaces SMS, Slack, Viber, Kik Messenger, Facebook Messenger, Telegram , Line Facebook Messenger Facebook Messenger, Slack , Line Facebook Messenger, Slack Viber , Slack, Twitter, Facebook Messenger, SMS Facebook Messenger, Kik Messenger, Slack, Skype, Amazon Alexa, Google Home Per API in jedes Interface integrierbar Per API in jedes Interface integrierbar Ideal einsetzbar für · Alle Phasen der Customer-Journey · Content-Publishing · Content-Publishing · E-Commerce · Geführter Einkaufsprozess mit vorgeschlagenen Produkten per Bot für Entscheidungsunterstützung · E-Commerce · Kundenservice · Kundenbindung · Kundenservice · Dialogbasierte Entscheidungsunterstützung (auto­matisierter Hilfe-Dialog statt selbstständiges Klicken und Suchen der Kunden) · Kundenservice · E-Commerce USP · Flexible Einsatzmöglichkeiten · Einfache Erstellung · Chatfuel arbeitet Hand in Hand mit dem Integrationsdienstleister zusammen · Formelbasierte Erkennung des Nutzerwunsches · Abbildung des Sales-Workflows · Random-Access-Navigation · Keine Technikfertigkeiten notwendig · Automatic-Speech-Reco­gnition und Text-to-Speech erlauben natürlich wirkende Konversationen · Neues Anlernen des Bots wird vermieden durch Anbindung an weitere Systeme für Sinnerkennung bei Kundenfragen · Wahrscheinlichkeitsbasierte Lösungsvorschläge zur Unterstützung von Servicemitarbeitern · Hoher Grad an flexiblen Einsatzmöglichkeiten in Verbindung mit KI-Unterstützung und manueller Optimierung Hauptsitz USA USA und Russland Deutschland Kanada USA USA Großbritannien DeutschlandAnbieter ->Motion AIChatfuelSpectrmAutomatAssistPullstringDigitalgeniusTwylaBesondere Eigenschaften· Anwendbarkeit für verschiedene Szenarien wie Kundenservice, E-Commerce etc. · Node.js-Integration · Integrierte Testbot-Ansicht· Direkter Facebook-Log-in · Schnelle Einbindung von Dritt­anbietern · Lernfähiges System, das Antworten mit der Zeit immer besser anpasst · Echtzeittests im Facebook Messenger· Einbindung von RSS-Feeds · Gute Integrierbarkeit von Drittanbietern/-systemen · Keyword- und interessenbasierte Aussteuerung von Inhalten· Diverse E-Commerce-Features (Online-Advertising, Produktempfehlungen etc.) · Zu 100 Prozent selbst entwickeltes System· Sprachunterstützung für Amazon Alexa und Google Home· SDK-Integration für Weiterentwicklungen · CRM-Anbindung · Enterprise- und Agenturlösungen (Dienstleister erhalten tieferen Zugriff auf Technologie für Umsetzung unabhängig vom Anbieter)· Enge CRM-Verknüpfung · Gutes Zusammenspiel von Automatisierung und Mensch· Analyse und Integration historischer Daten · Skalierbare Cloud-Plattform · Maschinelle Sprachverarbeitung und automatisierte Lernfähigkeit · Flexible Integration diverser Backend-Systeme · Echtzeitanalysen mit DashboardsUnterstützte InterfacesSMS, Slack, Viber, Kik Messenger, Facebook Messenger, Telegram , LineFacebook MessengerFacebook Messenger, Slack , LineFacebook Messenger, SlackViber , Slack, Twitter, Facebook Messenger, SMSFacebook Messenger, Kik Messenger, Slack, Skype, Amazon Alexa, Google HomePer API in jedes Interface integrierbarPer API in jedes Interface integrierbarIdeal einsetzbar für· Alle Phasen der Customer-Journey· Content-Publishing· Content-Publishing· E-Commerce · Geführter Einkaufsprozess mit vorgeschlagenen Produkten per Bot für Entscheidungsunterstützung· E-Commerce· Kundenservice · Kundenbindung· Kundenservice · Dialogbasierte Entscheidungsunterstützung (auto­matisierter Hilfe-Dialog statt selbstständiges Klicken und Suchen der Kunden)· Kundenservice · E-CommerceUSP· Flexible Einsatzmöglichkeiten · Einfache Erstellung· Chatfuel arbeitet Hand in Hand mit dem Integrationsdienstleister zusammen· Formelbasierte Erkennung des Nutzerwunsches· Abbildung des Sales-Workflows· Random-Access-Navigation · Keine Technikfertigkeiten notwendig· Automatic-Speech-Reco­gnition und Text-to-Speech erlauben natürlich wirkende Konversationen · Neues Anlernen des Bots wird vermieden durch Anbindung an weitere Systeme für Sinnerkennung bei Kundenfragen· Wahrscheinlichkeitsbasierte Lösungsvorschläge zur Unterstützung von Servicemitarbeitern· Hoher Grad an flexiblen Einsatzmöglichkeiten in Verbindung mit KI-Unterstützung und manueller OptimierungHauptsitzUSAUSA und RusslandDeutschlandKanadaUSAUSAGroßbritannienDeutschlandvg@wuv.deQuelle: T-Systems Multimedia Solutions Dokumentations-und Informationszentrum Munchen GmbH (DIZ)Document WERBEN0020170821ed8l0000e"
"63",2018-05-14,"ZSmart ermöglicht intelligente Transformation auf dem TM Forum DTW 2018","ZSmart ermöglicht intelligente Transformation auf dem TM Forum DTW 2018475 words14 May 2018news aktuell OTS - OriginaltextserviceOTSGerman(c) 2018 news aktuell Nizza, Frankreich (ots/PRNewswire) - ZTEsoft freut sich, seine Teilnahme am TM Forum Digital Transformation World (DTW) 2018 (ehemaliges TM Forum Live!) bekanntzugeben, das vom 14. bis 16. Mai 2018 in Nizza, Frankreich, stattfindet. Auf dem diesjährigen DTW 2018 wird ZTEsoft seine neueste auf künstliche Intelligenz (Artifical Intelligence, AI) gestützte Telecom Brain-Lösung sowie die ZSmart QConnect- und Internet-der-Dinge- (Internet of Things-, IoT-)Lösung vorstellen, um branchenübergreifende Transformationsprozesse voranzubringen.Telecom Brain fördert ein verbessertes Erlebnis und operative EffizienzDie Telecom Brain-Lösung ist eine maßgeschneiderte Lösung für CSPs, mit der Geschäftstätigkeiten im Bereich Telekommunikation intelligent, effizient und erfahrungsorientiert gestaltet werden können. Telecom Brain ist auf mehrere verschiedene Geschäftsszenarien ausgerichtet, und zwar durch:- digitale Plattform ""ZSmart"", die CSPs durch branchenübergreifende Integration unterstützt, Kommunikationsprozesse zwischen verschiedenen Parteien zu verwalten, flexible Marketing-Strategien zu integrieren und Kunden breitgefächerte digitale Lebenserfahrungen zu bieten. - ZSmart Knowledge-Defined Operations (KDOps), ein AI-optimiertes operationsunterstützendes System mit einem auf maschinellem Lernen basierten Gehirn, das informierte und auf Massendaten gestützte Analysen vornehmen und Entscheidungen fällen kann. ZSmart KDOps ermöglicht CSPs, Arbeitsvorgänge und Verwaltungsprozesse mit schneller Failover-Strategie und automatisierter Entscheidungsfindung zu modernisieren. - AI-basiertes ZSmart uTalk, das Kundendienstmitarbeitern über Omnikanal-Verfahren und einem Wechsel von reaktiver Hilfestellung zu proaktivem Support einen breiteren Handlungsspielraum einräumt. Die Technologie für automatisierte Spracherkennung (Automatic Speech Recognition, ASR) bietet Mensch-Maschine-koordiniertes Arbeiten über einen rund um die Uhr zugänglichen Online-Roboter, der einfache, wiederkehrende Probleme handhaben kann und auf intelligente Weise Kundenanfragen erkennt.ZSmart QConnect modernisiert Ihr Netzwerk und Ihre Dienste im IKT-ZeitalterDas Modernisieren von Netzwerken und Entwickeln innovativer Dienstleistungen mit Cloud Computing, was in der heutigen Software-orientieren Landschaft zum Überleben notwendig ist, bereitet CSPs oftmals Schwierigkeiten. ZSmart QConnect bietet einen One-Stop-Marktplatz für Cloud- und Netzwerkdienste mit einer flexiblen Orchestrierung physischer und virtueller Netzwerke, wodurch CSPs von der Synergie von Cloud und Netzwerk profitieren können, um so ein Ökosystem für intelligente Cloud-Netzwerk-Dienste aufzubauen.Unbegrenzte Wertschöpfung im IoT-ÖkosystemZTEsoft bietet Kunden branchenübergreifend End-to-End-IoT-Lösungen, die von Verbindungsmanagement und Partnermanagement bis zu Anwendungsentwicklung und Platform Enablement reichen. Die ZSmart IoT-Lösung stärkt die Zuverlässigkeit der IoT-Infrastruktur, sodass ein effektives Verwalten Ihrer IoT-Geschäfte vom Sensor bis zur Cloud möglich ist. ZTEsoft möchte mit einem Fokus auf der Infrastruktur und dem Partnerökosystem die Konvergenz von Mensch, Dingen und Cloud in einer digitalen Welt realisieren, um so letztlich mit Partnern ein offenes und umfassendes IoT-Ökosystem zu schaffen.Besuchen Sie uns auf dem TM Forum DTW 2018 an Stand Nr. 332 (3. Etage) im Acropolis Convention Center, um mehr über ZSmart-Lösungen zu erfahren.OTS: ZTEsoft newsroom: http://www.presseportal.de/nr/130646[http://www.presseportal.de/nr/130646] newsroom via RSS: http://www.presseportal.de/rss/pm_130646.rss2[http://www.presseportal.de/rss/pm_130646.rss2]Pressekontakt: Cher Dong +86-177-1432-5727 dong.xu51@ztesoft.com3943039news aktuell GmbHDocument OTS0000020180514ee5e003uy"
"64",2018-05-14,"ZSmart ermöglicht intelligente Transformation auf dem TM Forum DTW 2018","ZSmart ermöglicht intelligente Transformation auf dem TM Forum DTW 2018475 words14 May 201813:48Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS"" Nizza, Frankreich - ZTEsoft freut sich, seine Teilnahme am TM Forum Digital Transformation World (DTW) 2018 (ehemaliges TM Forum Live!) bekanntzugeben, das vom 14. bis 16. Mai 2018 in Nizza, Frankreich, stattfindet. Auf dem diesjährigen DTW 2018 wird ZTEsoft seine neueste auf künstliche Intelligenz (Artifical Intelligence, AI) gestützte Telecom Brain-Lösung sowie die ZSmart QConnect- und Internet-der-Dinge- (Internet of Things-, IoT-)Lösung vorstellen, um branchenübergreifende Transformationsprozesse voranzubringen.Telecom Brain fördert ein verbessertes Erlebnis und operative EffizienzDie Telecom Brain-Lösung ist eine maßgeschneiderte Lösung für CSPs, mit der Geschäftstätigkeiten im Bereich Telekommunikation intelligent, effizient und erfahrungsorientiert gestaltet werden können. Telecom Brain ist auf mehrere verschiedene Geschäftsszenarien ausgerichtet, und zwar durch:- digitale Plattform ""ZSmart"", die CSPs durch branchenübergreifende   Integration unterstützt, Kommunikationsprozesse zwischen   verschiedenen Parteien zu verwalten, flexible Marketing-Strategien   zu integrieren und Kunden breitgefächerte digitale   Lebenserfahrungen zu bieten. - ZSmart Knowledge-Defined Operations (KDOps), ein AI-optimiertes    operationsunterstützendes System mit einem auf maschinellem Lernen   basierten Gehirn, das informierte und auf Massendaten gestützte   Analysen vornehmen und Entscheidungen fällen kann. ZSmart KDOps   ermöglicht CSPs, Arbeitsvorgänge und Verwaltungsprozesse mit   schneller Failover-Strategie und automatisierter   Entscheidungsfindung zu modernisieren. - AI-basiertes ZSmart uTalk, das Kundendienstmitarbeitern über   Omnikanal-Verfahren und einem Wechsel von reaktiver Hilfestellung   zu proaktivem Support einen breiteren Handlungsspielraum einräumt.   Die Technologie für automatisierte Spracherkennung (Automatic   Speech Recognition, ASR) bietet Mensch-Maschine-koordiniertes   Arbeiten über einen rund um die Uhr zugänglichen Online-Roboter,   der einfache, wiederkehrende Probleme handhaben kann und auf   intelligente Weise Kundenanfragen erkennt.ZSmart QConnect modernisiert Ihr Netzwerk und Ihre Dienste im IKT-ZeitalterDas Modernisieren von Netzwerken und Entwickeln innovativer Dienstleistungen mit Cloud Computing, was in der heutigen Software-orientieren Landschaft zum Überleben notwendig ist, bereitet CSPs oftmals Schwierigkeiten. ZSmart QConnect bietet einen One-Stop-Marktplatz für Cloud- und Netzwerkdienste mit einer flexiblen Orchestrierung physischer und virtueller Netzwerke, wodurch CSPs von der Synergie von Cloud und Netzwerk profitieren können, um so ein Ökosystem für intelligente Cloud-Netzwerk-Dienste aufzubauen.Unbegrenzte Wertschöpfung im IoT-ÖkosystemZTEsoft bietet Kunden branchenübergreifend End-to-End-IoT-Lösungen, die von Verbindungsmanagement und Partnermanagement bis zu Anwendungsentwicklung und Platform Enablement reichen. Die ZSmart IoT-Lösung stärkt die Zuverlässigkeit der IoT-Infrastruktur, sodass ein effektives Verwalten Ihrer IoT-Geschäfte vom Sensor bis zur Cloud möglich ist. ZTEsoft möchte mit einem Fokus auf der Infrastruktur und dem Partnerökosystem die Konvergenz von Mensch, Dingen und Cloud in einer digitalen Welt realisieren, um so letztlich mit Partnern ein offenes und umfassendes IoT-Ökosystem zu schaffen.Besuchen Sie uns auf dem TM Forum DTW 2018 an Stand Nr. 332 (3. Etage) im Acropolis Convention Center, um mehr über ZSmart-Lösungen zu erfahren.Rückfragehinweis:   Cher Dong   +86-177-1432-5727   dong.xu51@ztesoft.comDigitale Pressemappe: http://www.ots.at/pressemappe/PR130646/aom[http://www.ots.at/pressemappe/PR130646/aom]*** OTS-ORIGINALTEXT PRESSEAUSSENDUNG UNTER AUSSCHLIESSLICHER INHALTLICHER VERANTWORTUNG DES AUSSENDERS - WWW.OTS.AT ***OTS0129-20180514APA - Austria Presse Agentur eGDocument AUPAG00020180514ee5e006el"
"65",2003-05-20,"PC liest Lippen.","PC liest Lippen.34 words5 May 2003FocusFOCUS129German© 2003 FOCUS – DAS MODERNE NACHRICHTENMAGAZIN, FOCUS MAGAZIN VERLAG GMBH, www.focus.deComputernewsMit Hilfe eines neuen Programmpakets von Intel sollen Computer Sprachebesser verstehen: Die ""Audio Visual Speech Recognition""-Software erfasstGesicht und Lippenbewegungen des Sprechers und kann so Umgebungsgeräuscheleichter ausfiltern.Focus Magazin Verlag GmbHDocument focus00020030506dz55000bk"
"66",2014-11-20,"Technology Leadership Market in Prague","Technology Leadership Market in Prague773 words11 November 2014NewsKitchen.euNKITDEGermanCopyright 2014.  Multimedia Investments Ltd.  All rights reserved.   MIL OSI[http://milnz.co.nz/mil-osi-aggregation/] - Source: Telekom Stiftung - Press Release/StatementHeadline: Technology Leadership Market in PragueNov 11, 2014At the Technology Leadership Market in Prague, Deutsche Telekom showed how its local operations are making it the leading European telco. Together, they leverage the Group’s technology leadership to provide customers across its footprint with truly the best experience.What is next for partners and customers? What can they expect from DT and its technology in the next few years? These were just two of the questions that employees and journalists from across Europe tackled at the first Technology Leadership Market.*A device purchased directly from Deutsche Telekom performs better and faster in the DT network than one purchased via third-party sales channels. That’s why the Terminal Shared Service Center tests devices for the DT Group.*T-Mobile is the first operator in the Czech Republic to introduce LTE, enabling speeds of up to 225 Mbps in the downlink and 50 Mbps in the uplink. They are also working on voice over LTE, which they demonstrated using Samsung devices.*Millions of attacks are launched against the network every month. The company’s local operations work closely with the DT Group and each other to secure the network and their customers’ data.*Innovation has many forms, also in customer service. Video call centers, voice biometrics and speech recognition are all being tested by Deutsche Telekom and its national companies.*DT works with partner companies to deliver the applications and services customers want. The award-winning T-Parking app, for instance, was developed by an internal startup at T-Mobile Czech Republic and uses Big Data resources to provide up to the minute updates on the parking situation on streets.*In all, almost 30 journalists from five countries got a first-hand look at how the DT Group works with its national companies to achieve technology leadership and provide the best customer experience.T-Mobile Czech Republic, which hosted the event, is home to the Terminal Shared Service Center. This competence center optimizes fixed-line and mobile devices for DT customers in its 13 national companies. The devices are put through rigorous testing either in Prague or in three other locations across Germany. Once the tests are completed, DT gives the requirements to the manufacturers to ensure they are ideally integrated into the DT network. “An optimized device is much faster on our network,” explained Daniel Michálek, International Head of Terminal Integration & Validation, in Prague. When a voice call which is made via 3G ends, it takes far less time to switch back to high-speed LTE data connection. “This is a difference the customer feels right away,” continued Michálek. The competence center is also responsible for testing apps and software developed by Deutsche Telekom and its partner companies. One factor they focus on is, of course, security.Sharing the knowledge to provide data securityWith the number of hacking attempts increasing every month, Regional Chief Technology Security Officer Ivo Pastucha, sees DT in the role of a defender: “T-Mobile Czech Republic can hardly fight all these hackers, but it can protect data and customers.” The Head of the Group's Data Reliance Shared Service Center knows: “There is and always will be a gap between system protection and the implementation of new components.” One percent of DSL data traffic in the Czech Republic is malicious, but most of the customers do not know anything about this constant, imminent threat. There is a lack of transparency which has to be overcome in the next years, Pastucha explained.See and hear the future of customer serviceTechnology can also help to improve Customer service at Deutsche Telekom. Magyar Telekom, the company’s subsidiary in Hungary, is running a pilot project whereby customer service agents can help customers via video conference. What is more, they can use a type of heads-up-display to drag and drop products and tariff descriptions, for instance, right onto the screen. This enables customers to actually see what they are signing up for. The company is also developing better voice biometrics solutions that let customers identify themselves by just saying a passphrase. A speech recognition solution will also allow customers to say what they want and a so-called virtual assistant brings them through the company’s offerings.The Technology Leadership Market in Prague showed how many changes the transformation of the DT infrastructure will bring. But these changes will also bring opportunities for partners and customers. A possible Technology Leadership Market 2015 would show the industry how and if this picture will change.Multimedia Investments LtdDocument NKITDE0020141112eabb0001r"
"67",2009-08-26,"IT-Lösungen von der Austria Presse Agentur","IT-Lösungen von der Austria Presse AgenturOliver Weiss 1261 words26 August 2009ComputerweltCMPWLT17 / 2009German© 2009 Info Technologie Verlag GmbH. All rights reserved. For further information see http://www.computerwelt.at[http://www.computerwelt.at]Die APA, führende Nachrichtenagentur des Landes, hat sich mit APA-IT über Österreich hinaus einen Namen gemacht.Die Gebote der Zuverlässigkeit und Schnelligkeit gelten für eine Nachrichtenagentur wie die APA, die zugleich der führende Informationsdienstleister in Österreich ist, in besonderem Maße. Dies bedingt gerade in Sachen IT eine reibungslos funktionierende Basis. Um die Nähe zum Kunden zu optimieren, wurde der Geschäftsbereich als APA-IT ausgegliedert. Als Tochter einer Nachrichtenagentur hat APA-IT jahrzehntelange Erfahrung mit medienspezifischen und mediennahen IT-Umgebungen. Tagtäglich bewältigt das Unternehmen intern wie extern große Datenvolumina, gewährleistet hohen, multimedialen Datendurchsatz und stellt überdurchschnittlich hohe Ansprüche an die IT-Verfügbarkeit. Dieses Know-how kommt auch den Kunden zugute und qualifiziert die APA-IT als Partner im Umgang mit zeitkritischen IT-Systemen.APA-IT ist heute einer der wichtigsten Experte für Redaktionssysteme, Netzwerktechnologie und technische Datenbank-Lösungen. Die Angebotspalette des Unternehmens geht jedoch weit über den Medienbereich hinaus und umfasst ebenso Dienstleistungen im Bereich Serverhosting, Web-Entwicklung, Content Management Systeme (CMS) bis hin zu Broadcasting-Systemen. Die APA-IT übernimmt dabei mit knapp 60 Mitarbeitern Konzeption, Entwicklung und technischen Betrieb der IT-Gesamtlösungen und steht den Kunden von der Entwicklungsphase bis zum produktiven Betrieb als Partner zur Verfügung. Die einzelnen Bereiche der APA-IT kooperieren dabei je nach Projektanforderung und bilden abteilungsinterne oder -übergreifende Projektteams.APPLICATION ENGINEERINGSpezielle Anforderungen können nicht immer mit Standard-Softwarelösungen realisiert werden – das gilt besonders für das Informationsgeschäft. Aus diesem Grund hat die APA-IT früh begonnen, selbst Applikationsentwicklung zu betreiben und ihr Wissen intern sowie auch für ihre Kunden einzusetzen. Jahrzehntelang gesammeltes Expertenwissen dient als Basis dafür, die unterschiedlichsten Kundenanforderungen zu erfüllen können. Die APA-IT bietet von der klassischen Website bis zum Kundenportal mit CMS, von der Newsletter-Lösung bis zum Collaboration-System individuelle Applikations-Lösungen.Professionelle Web- und Java-Entwicklung beginnt mit persönlicher Beratung. Auf individuelle Anfragen hin erhalten Kunden alles aus einer Hand und ersparen sich den aufreibenden Koordinationsaufwand zwischen Provider, Applikationsentwicklung und Design-Agentur. Spezialisten setzen selbst komplexeste Kundenwünsche professionell um. Auf Wunsch übernimmt die APA-IT auch den Betrieb der entwickelten Applikation im leistungsfähigen und ausfallsicheren, hausinternen Rechenzentrum mit 7x24-Stunden-Kundendienst.BROADCASTING SOLUTIONSTV und Radio sind 24 Stunden am Tag auf Sendung – immer aktuell, oftmals live. Dies verlangt von den Redaktionen im Hintergrund besonders rasche Produktion und exakte Abwicklung der Beiträge. Die bedeutendsten Rundfunkanstalten des Landes verlassen sich dabei auf Broadcasting-Systeme der APA-IT. Vor dem eigentlichen Leistungsprozess einer Fernsehanstalt, nämlich der Ausstrahlung von Fernsehsendungen, sind eine Vielzahl komplexer Produktionsschritte notwendig: Redaktionelle Planung, die Durchlaufzeit von Teilprozessen, Rechtemanagement, eine abteilungsübergreifende Kommunikation und der optimierte Einsatz von technischen und personellen Ressourcen sind für die Herstellung von Sendebeiträgen erforderlich. APA-IT Broadcasting Solutions bieten innovative Lösungen zur Unterstützung dieser Prozesse.Auf der Suche nach einer bestimmten Information steht dem User eine oft unüberschaubare Palette an Quellen zur Verfügung. Public-Suchmaschinen verlieren sich im Überangebot des Wissens. Nur hochverfügbare Datenbanken mit intuitiv verständlicher Darstellung der Suchergebnisse ermöglichen zielgenaues und schnelles Zurechtfinden in großen Mengen von Text- und Multimedia-Dokumenten.MEDIA ARCHIVESDie APA-IT bietet ihren Kunden und Partnern im Bereich Medienarchiv eine leistungsfähige Datenbank-Architektur, die Metainformationen eingepflegter Dokumente aus unterschiedlichen Quellen erschließt. Effiziente Beschlagwortung und intelligente Suchfunktionen führen den Benutzer über Millionen von Dokumenten in Sekundenschnelle zur gesuchten Information.OUTSOURCING PC & SERVEREine funktionierende IT-Infrastruktur ist das Rückgrat jedes Unternehmens. Computer, Notebooks, mobile Unternehmens-Kommunikation, auch Peripheriegeräte wie Faxe und Drucker arbeiten nur dann reibungslos, wenn sie permanent gewartet und auf den neuesten Stand gebracht werden. Große Mengen sensibler Unternehmensdaten auf Servern müssen trotz des wachsenden Datenverkehrs ständig erreichbar sein und selbstverständlich vor unerwünschten Angriffen von außen schützen. Aspekte wie die High-Speed-Anbindung an das Internet und optimal konfigurierte Netzwerktechnologie sind dabei ebenso ausschlaggebend wie die ausfallsichere Konfiguration der Systeme. Um den hohen Anforderungen auch in Spitzenzeiten der Auslastung gewachsen zu sein, gilt es, die Systeme rund um die Uhr zu überwachen, zu warten und zu pflegen. Auch diesen Herausforderungen ist die APA-IT gewachsen.Neben diesen vier Kernbereichen arbeitet die APA-IT auch permanent an neuen Ideen. Denn Technologie ist ständig in Bewegung: Ideen werden zu Entwicklungen - Entwicklungen verbinden sich zu Lösungen – innovative Lösungen setzen sich durch und begeistern. APA-IT-PowerSearch, die Datenbank hinter dem APA-OnlineManager (AOM), wurde beispielsweise in der Abteilung APA-IT Innovation entwickelt und für die AOM Version 6.0 Vision mit zukunftsweisenden Funktionen wie semantischen Suchmöglichkeiten und Informationsvisualisierung ausgestattet. Ein weiteres Innovationsfeld ist die automatische Spracherkennung, die es ermöglicht, gesprochene Wörter, beispielsweise aus Radio- oder Fernsehbeiträgen, automatisch in geschriebenen Text umzuwandeln.SEMANTIC QUERIESBegriffe, nach denen in einer Datenbank gesucht wird, haben immer einen bestimmten Kontext – also andere Begriffe, mit denen sie thematisch zusammenhängen. Oft interessiert den Suchenden ein ganzer Themenkomplex und nicht nur ein einzelnes Suchwort. Ein User möchte beispielsweise Dokumente finden, die mit »Innovation« zu tun haben. Im Umfeld des Begriffs »Innovation« finden sich beispielsweise häufig die Begriffe »Entwicklung« und »Forschung«. Die neue Power-Search-Funktion »Semantic Queries« liefert als Ergebnis zum Suchbegriff »Innovation« nicht nur Dokumente, die das Wort »Innovation« enthalten, sondern auch Treffer, in denen »Entwicklung« und »Forschung« vorkommen.Für diese Funktion werden die APA-Datenbanken als Wissensraum genutzt – tagesaktuell, durch die tägliche Neuzuordnung der thematisch verbundenen Begriffe. Die Vorteile für den User sind ein Verbessertes Ranking der Trefferliste, höhere Relevanz der Treffer, relevante Treffer, die den originalen Suchbegriff nicht enthalten sowie ein immer topaktuelles Wissensmodell, da keine manuelle Pflege erforderlich ist.SPRACHERKENNUNGDer Begriff »Automatische Spracherkennung« bezeichnet Technologien, die automatische Umwandlung von gesprochenem Wort in geschriebenen Text ermöglichen. Die so genannte LVCSR (Large Vocabulary Continuous Speech Recognition) ist die größte Herausforderung im Bereich der automatischen Spracherkennung. Dabei müssen alle Wörter erkannt werden, auch wenn mehrere Sprecher vorhanden sind. Ein Beispiel dafür sind Radio- und Fernsehbeiträge, in denen kontinuierlich in unterschiedlichen Umgebungen gesprochen wird. (Clean Speech von Moderatoren, Interview auf der Strasse, im Parlament, mit Musik im Hintergrund etc.)Diese Technologien basieren auf statistischen Modellen, die eine bestimmte Sprache so gut wie möglich modellieren sollen. Mithilfe des umfangreichen APA-Datenbestandes aus tagesaktuellen Audiofiles und Texten kann die APA-IT sowohl für deutsches als auch für österreichisches Deutsch optimale Modelle erstellen. Wichtig ist hier zu bemerken, dass die Sprache lebt und wächst: Jeden Tag entstehen neue Wörter, wie beispielsweise neue Personennamen – deshalb müssen auch die statistischen Sprachmodelle permanent aktualisiert werden. Der Sprachschatz des APA-Datenbestandes ist immer tagesaktuell und bildet damit die optimale Basis zur kontinuierlichen Aktualisierung der statistischen Sprachmodelle. Das Resultat: höchste Leistungen in der Spracherkennung.Die APA-IT Spracherkennungs-Applikationen stellen folgende Funktionen zur Verfügung:- Large Vocabulary Continuous Speech Recognition: Mit einem tagesaktuellen Wörterbuch von mehr als 100.000 Wörtern ist es möglich, Tonspuren von verschiedensten TV- und Radioquellen in Echtzeit kontinuierlich zu erkennen und zu verarbeiten. Der transkribierte Text kann in unterschiedlichen Formaten dargestellt werden, unter anderem auch in XML, wobei jedes Wort mit einem Zeitstempel versehen wird.- Automatische Trennung zwischen gesprochenen und nicht gesprochenen Elementen: Der Audio Stream wird in Sprache, Musik und Stille unterteilt.- Automatische Unterteilung eines Audio Streams in verschiedene Sprecher- Automatische Sprecheridentifikation: Es wird automatisch erkannt, wer gerade spricht.- Topic Detection/Text Kategorisierung/Named Entities Recognition: Der Text-Output der Spracherkennung wird mit unterschiedlichen semantischen Technologien kombiniert und kann in weiterer Folge besonders einfach durchsucht werden.Alle diese Funktionen können untereinander kombiniert werden. Mit bis zu 99 Prozent Erkennungsgenauigkeit, also Richtigkeit der transkribierten Wörter, kann sich die APA-IT Spracherkennung mit anderen state-of-the-art-Systemen messen. Die Transkription erfolgt in Echtzeit – der geschriebene Text erscheint bereits zwei Sekunden, nachdem er gesprochen wurde.Info Technologie Verlag GmbHDocument CMPWLT0020090827e58q0002g"
"68",2017-06-20,"""AI is going to amplify human intelligence not replace it""","""AI is going to amplify human intelligence not replace it""Von Alexander Armbruster 1295 words20 June 2017FAZ.netFAZNETGermanCopyright 2017 Frankfurter Allgemeine Zeitung GmbH. Provided by Frankfurter Allgemeine Zeitung Archiv If you would have to introduce someone with almost no background in computer science or mathematics to your most important AI-project at the moment, how would you do that?There are several important projects in computer vision, language translation, natural language understanding, etc. But there are two basic research projects on which we are spending a lot efforts. One is dialog systems, intelligent chatbots, and virtual assistants. The basic science and technology for smart virtual assistants doesn't yet exist, and we are finding ways to allow machines to acquire complex background knowledge by reading text, so as to be able to reason with this knowledge. Second, we are working on something called ""predictive learning"" which would allow machines to learn a kind of ""common sense"" by observation, the way humans and animals do.What are the crucial breakthroughs that lead to the AI-hype we go through right now?It's all due to the emergence of Deep Learning. Deep Learning is a set of techniques for training a computer to perform tasks such as detecting and recognizing objects in images, driving a car, recognizing speech or translating languages. While the basic ideas of deep learning have been around since the late 1980s, they have become dominant over the last 5 years because of progress in methods, faster computers and larger dataset on which to train them. A particular deep learning technique called convolutional neural networks - which I originally developed at AT&T Bell Laboratories in 1989 -- has become a kind of universal tool for image recognition, self-driving cars, medical image analysis, text processing and many other applications.Many people are afraid about potential consequences of further AI-progress. They are not sure about how resilient their jobs and knowledge are. What is your view on that?AI is going to amplify human intelligence not replace it, the same way any tool amplifies our abilities. Now, technological progress has always had the effects of (1) increasing overall wealth, (2) creating new jobs (3) making some jobs obsolete. The emergence of AI will also have that effect. The problems societies will have to deal with are (1) the acceleration of technological progress, which causes an increased number of people to retrain for new skills and new jobs (2) the fact that the wealth created by technological progress should be shared with all of society.Should computer science be obligatory for every pupil today - maybe beginning in elementary school?The process of reducing a complex task to a set of simple instructions, which is what programming is all about, is a skill that is very useful in many aspects of modern life, not just to professional computer scientists and programmers. So yes, it would be good if most high-school pupils knew the basics of computer programming by the time they graduate. There are tools that can be used to teach young children to program, such as the Scratch visual programming language. I'm not a specialist of education, but I would have loved to be able to play with something like this when I was a kid!In the past, there have been periods of hope and so called ""AI-Winters"" - is that kind of cycle still alive and where are we now?Since the 1960s, there have been waves of interest for various approaches to AI. In the 1960s, it was early neural network models capable of elementary learning. Then in the 1970s and 1980s, people lost interest in learning and focused on logic-based methods, with rules, reasoning, deduction - what we called ""expert systems"". This had some success, but it truned out to be very difficult to build these systems. Then in the late 1980s and early 1990s, neural networks made a come-back. This is when convolutional nets and other deep learning techniques were invented. There were successful applications in handwriting recognition and a few other fields. But interest in neural nets waned in the late 1990s in favor of ""simpler"" machine learning techniques. Then around 2011-2012, neural nets made a huge come-back under the name of ""deep learning"". The difference with previous waves of AI is that now there is a large number of very successful applications and a very large technology business around deep learning and AI. While the current hype that surrounds it will surely diminish, I don't think we will see an ""AI winter"" like the ones we've seen in the past.When you started your career in the 1980s, you concentrated on neural nets, an approach that had been rejected by the community in the 60s and then again in the 90s - why did you decide to go that way personally? And how did your life change since?I have always believed in the idea that intelligent machines must be built through learning. That's why I got interested in machine learning in the early 1980s, when I was still an engineering students and no one in the research community was working on machine learning at the time. I dug up the literature from the 1960s and realized why it had been abandoned. Neural nets are composed of multiple layers of simple computing nodes that can be seen as extremely simplified models of neurons in the brain. But in the neural nets of the 1960s, only the last layer could be trained. The other ones had to be designed ""by hand"". What we found in the late 1980s was a way to train all the layers in a multilayer network simultaneously. This is what we now call ""deep learning"", because of the multiple layers.Would you reveal in some detail, how the ""Deep Learning Conspiracy"" emerged?Geoff Hinton, Yoshua Bengio and I always believe that multi-layer neural nets were the right thing and would eventually beat other methods for computer vision and speech recognition. Around 2003, I became a professor at NYU, and resumed my work on neural net that I had put on the back burner since 1997. I had been a postdoctoral researcher in Goeff's lab in 1987-1988 and Yoshua had worked in my lab at Bell Labs in the early 1990s. So we had a common philosophy. Geoff, Yoshua and I were determined to renew the interest of the community in these methods by showing that they worked very well. Geoff convinced the Canadian Institute for Advanced Research, a private foundation, to fund a kind of research network with workshops and collaborations where people with similar interests could meet and exchange ideas. Around 2007, our ideas started gathering interest in the mainstream research community. But things really took off around 2011-2013 when deep learning methods started beating more conventional techniques by huge margins for image and speech recognition.What is the best idea from an AI-scientist in recent years?I'm record saying that Generative Adversarial Networks is the best idea in machine learning in the last 10 years. While the early results with GANs are pretty amazing when it's used for predictive (or unsupervised) learning, there is a lot of research still to do to make them work reliably and to understand their underlying principle. But it's very promising. We have systems that predict what will happen in the next few frames of a video (very useful for self-driving cars), that can generate nice-looking images from a rough sketch, that can synthesize sounds, colorize a black and white image. These are cool demonstrations, but the main hope with GANs is that they will enable machines to learn how the world works by observation, like animals and humans. This will take years, perhaps decades.FAZ.NET Frankfurter Allgemeine Zeitung GmbHDocument FAZNET0020170620ed6k0001a"
"69",2005-07-20,"W3C arbeitet an Standard zur Sprecheridentifizierung  ","W3C arbeitet an Standard zur Sprecheridentifizierung  161 words7 December 2005COMPUTERWOCHE OnlineCOMWOLGerman(c) 2005 COMPUTERWOCHE.  www.computerwoche.de[http://www.computerwoche.de]Die nächste Version von VoiceXML soll biometrische Verfahren zur Personenerkennung integrieren.  Das W3-Consortium gab bekannt, dass es mit VoiceXML 3.0 auf Forderungen nach Sicherheitsfunktionen eingehen werde. Um dem Diebstahl von Passwörtern und Betrug vorzubeugen, wird die Auszeichnungssprache zukünftig um die Möglichkeit erweitert, biometrische Authentifizierungsverfahren zu integrieren. Derzeit nutzen verschiedene Hersteller proprietäre Extensionen für diesen Zweck. Sie sollen mit VoiceXML überflüssig werden.  Zweck der VoiceXML ist es, auf Internet-Funktionen mittels Sprachbefehlen oder Wähltönen (die zum Beispiel auf Telefontastaturen hinterlegt sind) zuzugreifen. Im Rahmen des vom W3C entwickelten ""Speech Interface Framework"" bildet VoiceXML das Kernstück, das durch die SRGS (Speech Recognition Grammar Specification), die Sprachbefehle definiert, und die SSML (Speech Synthesis Markup Language) ergänzt wird. Sie steuert die Sprachausgabe des Rechners und wird in der nächsten Version um Unterstützung für asiatische Sprachen erweitert. Sie berücksichtigt die Tatsache, dass etwa im Chinesischen die Tonhöhe für die Bedeutung eines Wortes ausschlaggebend ist. (ws)  IDG Communications Verlag AGDocument COMWOL0020051207e1c700032"
"70",2015-01-05,"VoiceBox und Elektrobit arbeiten zusammen, um natürliche Spracherkennung in vernetzten Autos zu ermöglichen","VoiceBox und Elektrobit arbeiten zusammen, um natürliche Spracherkennung in vernetzten Autos zu ermöglichen563 words5 January 201514:03Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS""   Bellevue, Washington - VoiceBox Technologies, der mit Preisen ausgezeichnete innovative Anbieter für kontextbezogene natürliche Spracherkennung (Contextual Natural Language Understanding / NLU) und ein anerkannter Pionier für das vernetzte Auto, hat seine Partnerschaft mit Elektrobit weiter ausgebaut. Die neue Vereinbarung zwischen den Unternehmen ermöglicht beiden, ihre kombinierten Lösungen anzubieten, um eine von VoiceBox aktivierte, multimodale Human-Machine Interface (HMI) Entwicklungsplattform für die Automobilbranche bereitzustellen.Die technologische Kooperation im letzten Jahr, bei der VoiceBox den EB Guide von Elektrobit mit seinen patentierten Technologien für Context Management und Conversational Voice ausgestattet hat, hat echte Innovationskraft bewiesen. Konsumenten können über den herkömmlichen HMI-Ansatz mit nur einer Frage und einer Antwort hinausgehen, weil VoiceBox den Nutzer erlaubt, mehrere Themen in jeder Reihenfolge abzufragen - so wie es den Nutzern in den Sinn kommt - und bei Bedarf auch Folgefragen zu stellen. Zu den jüngsten Fortschritten gehört die Integration der Deep Neural Networks (DNN) Modelle, die die Spracherkennung mit einem großen Vokabular entscheidend verbessert.""Der VoiceBox-Vorteil ergibt sich aus der einzigartigen Kombination des ausgereiften Designs der Sprachschnittstelle mit der patentierten, kontextbezogenen natürlichen Spracherkennung. Wir sind stolz darauf, unsere Lösungen zusammen mit dem EB GUIDE von Elektrobit anzubieten, um für die Nutzer natürliche und personalisierte Spracherkennung auf den Automobilmarkt zu bringen"", sagte Mike Kennewick, CEO von VoiceBox.Der VoiceBox-Vorteil basiert auf einmaligen Fähigkeiten, die kürzlich in der IEEE-gesponserten Patent Power-Bestenliste [http://spectrum.ieee.org/at-work/innovation/patent-power-2013[http://spectrum.ieee.org/at-work/innovation/patent-power-2013]] genannt wurden, auf der VoiceBox unter den 15 Spitzenunternehmen mit der einflussreichsten Computer-Softwaretechnologie-IP weltweit rangierte. VoiceBox war das einzige Technologieunternehmen für Spracherkennung in dieser Spitzengruppe.VoiceBox bietet für Unternehmen, die Produkte und Lösungen mit Sprachaktivierung ausstatten wollen, ein komplettes Leistungsspektrum. Dies umfasst erweiterte automatische Spracherkennung (Automatic Speech Recognition / ASR), NLU sowie Text-zu-Sprache-Technologien (Text To Speech / TTS) mit der gesamten Bandbreite von Entwicklungs- und Testverfahren und Datendiensten. Als renommierter und innovativer Anbieter für Sprachanwendungen im Automobilsektor hat VoiceBox im Jahr 2008 die erste integrierte NLU-Schnittstelle in der Branche bereitgestellt und für das vernetzte Auto im Jahr 2011 Pionierarbeit geleistet. Das Unternehmen wurde zusammen mit seinen Partnern mehrfach mit den Auszeichnungen CES Best in Show geehrt. Anwendungen von VoiceBox werden in 23 Sprachen und auf drei Kontinenten von Unternehmen wie Toyota, Fiat, Dodge, Chrysler, Maserati, Renault, Mazda und Tom-Tom eingesetzt und ausgeliefert.Über VoiceBox Technologies CorporationVoiceBox ist ein Pionier im Bereich Natural Language Understanding und Conversational Voice Technology (Technologien zur Sprach- und Stimmerkennung), der Menschen dabei unterstützt, Inhalte zu finden und ihre Geräte mithilfe der natürlichen Alltagssprache zu steuern. Die Anwendungen von VoiceBox können in Systemen im Auto, auf Smartphones und in Heimsystemen eingesetzt werden. Alle werden von der mit Preisen ausgezeichneten VoiceBox-Sprachtechnologie angetrieben, die von über 30 Patenten geschützt wird. Das Unternehmen mit Hauptsitz in Bellevue, Washington, unterhält Niederlassungen in Los Angeles, München, den Niederlanden und Tokio. Für weitere Informationen besuchen Sie bitte VoiceBox [http://www.voicebox.com/technology/[http://www.voicebox.com/technology/]].VoiceBox und das Logo von VoiceBox sind eingetragene Warenzeichen der VoiceBox Technologies Corporation in den Vereinigten Staaten von Amerika und/oder anderen Ländern. Alle anderen Unternehmensnamen oder Produktbezeichnungen können Warenzeichen ihrer entsprechenden Eigentümer sein.Web site: http://www.voicebox.com/[http://www.voicebox.com/]Rückfragehinweis:   KONTAKT: Jennifer Cortel, jenniferc@voicebox.comDigitale Pressemappe: http://www.ots.at/pressemappe/PR115664/aom[http://www.ots.at/pressemappe/PR115664/aom]*** OTS-ORIGINALTEXT PRESSEAUSSENDUNG UNTER AUSSCHLIESSLICHER INHALTLICHER VERANTWORTUNG DES AUSSENDERS - WWW.OTS.AT ***Austria Presse AgenturDocument AUPAG00020150105eb15003ed"
"71",2007-09-20,"Handterminal mit Sprachfunktion   ","VerpackungHandterminal mit Sprachfunktion   MM Logistik Special Intralogistik Journal 2007 vom 09.10.2007 Seite 38394 words9 October 2007MM MaschinenmarktMMMASGerman©  2007 MM Maschinenmarkt – Das IndustrieMagazin. Alle Rechte vorbehalten.   www.maschinenmarkt.de[http://www.maschinenmarkt.de]ACD Elektronik hat sein bewährtes Datenfunkterminal M250 um Voice-Funktionalität erweitert. Damit hat der Anwender die Wahl zwischen display- und sprachgeführter Arbeitsweise und kann je nach Aufgabenstellung den effizienteren Modus wählen. Für Anwender, die beispielsweise im Warenein- und -ausgang Palettendaten einscannen und mit dem Lagerverwaltungssystem abgleichen wollen, wird das multimodale Handterminal M250-V mit Barcodescanner in der üblichen Display-Betriebsart genutzt. Bei der Kommissionierung oder auch bei Inventurarbeiten kann auf Sprachführung umgeschaltet werden, der Benutzer hat dann beide Hände und seine Augen für die eigentliche Arbeit frei. Für die Sprachfunktion hat ACD die Audiosignalverarbeitung des Terminals optimiert, eine Schnittstelle für das Headset integriert und das Gerät mit einem Voice-Modul ausgestattet. Damit arbeitet das M250-V nach dem Fat-Client-Prinzip: Die Sprachverarbeitung erfolgt im Terminal.    Die synthetische Stimme lässt sich dabei in der Sprechgeschwindigkeit individuell einstellen, ohne dass die Verständlichkeit leidet. Vorteilhaft ist die sprecherunabhängige Spracherkennung, die auf einem bewährten Basiswortschatz aufbaut und sich anwendungsspezifisch und auch um Dialekte erweitern lässt.   Handheld terminal with voice function   ACD Elektronik has upgraded its proven M250 remote data terminal with voice functionality. The user now has the choice of display or voice control and is able to select the more efficient mode based on the nature of the task. For users who for example wish to scan in pallet data at Goods In and Dispatch points and compare this with the warehouse management system, the M250-V multimodal handheld terminal with barcode scanner is used in the conventional display mode. When picking or stock-taking, switching to voice function enables the user to have both hands free and to focus on the actual job. To provide the voice option, ACD has optimized the terminal's audio signal processing functionality, incorporated an interface for the headset and equipped the device with a voice module. Hence, the M250-V works according to the Fat Client Principle, i.e. voice processing takes place at the terminal. The artificial voice can be adjusted individually for voice speed without becoming unintelligible. The device benefits from speaker-independent speech recognition which builds on an established basic vocabulary and can be extended for specific applications and even for dialects.   ACD Elektronik GmbH,   Tel. (0 73 92) 7 08-0,   www.acd-elektronik.de  [http://www.acd-elektronik.de]100907035Vogel Business Medien GmbH & Co.KGDocument MMMAS00020071023e3a900016"
"72",2016-02-18,"VoiceBox stellt Technologie zur natürlichen Spracherkennung für Samsung Mobilgeräte bereit","VoiceBox stellt Technologie zur natürlichen Spracherkennung für Samsung Mobilgeräte bereit363 words18 February 201612:02Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS""   Bellevue, Washington - VoiceBox Technologies Corporation kündigte heute eine strategische Partnerschaft mit Samsung für den Einsatz der natürlichen Spracherkennungs-Technologie (Natural Language Understanding, NLU) von VoiceBox in Samsungs Sprachdiensten an. Die Unternehmen haben bereits eng zusammengearbeitet, um einen intelligenten, durch die NLU-Technologie von VoiceBox unterstützten Assistenten zu entwickeln.""Wir sind stolz darauf, dass die Wahl auf uns gefallen ist, um die Sprachdienste einer Reihe von bereits bestehenden und neuen Samsung-Mobilgeräten zu unterstützen"", erklärte Mike Kennewick, CEO von VoiceBox. ""Wir freuen uns auf weitere Gelegenheiten, mit Samsung zusammenzuarbeiten, insbesondere bezüglich unserer Initiativen zu Smart Home und Internet der Dinge"".""Samsung besitzt ein Verständnis dafür, dass Sprachtechnologie sich als kritisch für seine Zukunft erweisen wird. Wir freuen uns über die Partnerschaft mit VoiceBox. Diese Zusammenarbeit wird es uns ermöglichen, den Anwendern ein leistungsstarkes Nutzererlebnis im Bereich Sprache zur Verfügung zu stellen"", so Peter Koo, Senior Vice President Samsung Mobile.VoiceBox ist Pionier in der Bereitstellung von intelligenten, kontextbezogenen Sprachlösungen, und die preisgekrönte Technologie findet sich in vielen der heutigen vernetzten Autos, Smartphones und Wearables. Darüber hinaus bietet das Unternehmen intelligente, kontextbezogene Sprachlösungen für Originalhersteller (Original Equipment Manufacturers, OEMs) an, die Sprachsteuerung für eine Reihe von Produkten im Internet der Dinge-Markt integrieren wollen.Informationen zu VoiceBox Technologies CorporationVoiceBox bietet One-Stop-Shopping für Unternehmen, die auf der Suche nach sprachfähigen Produkten und Lösungen sind und stellt Technologien wie erweiterte automatische Spracherkennung (Automatic Speech Recognition, ASR), natürliche Spracherkennung (Natural Language Understanding, NLU) und Text-zu-Sprache (Text to Speech, TTS) zusammen mit einer vollständigen Palette von Entwicklungs-, Test- und Datendiensten für das Internet der Dinge (IoT) zur Verfügung. VoiceBox ist derzeit im Lieferumfang von Samsung, AT&T, Toyota, Fiat Chrysler Automobiles, Dodge, Maserati, Renault, Subaru, Smart Cars, Pioneer und Tom-Tom in 23 Sprachen auf 3 Kontinenten enthalten. Weitere Informationen finden Sie unter http://www.voicebox.com[http://www.voicebox.com].Rückfragehinweis:   Bitte wenden Sie sich für zusätzliche Presseinformationen (nur   Medienpartner) an: Jonathan Richardson   WE Communications   +1 (425) 638-7834   jrichardson@we-worldwide.comDigitale Pressemappe: http://www.ots.at/pressemappe/PR115664/aom[http://www.ots.at/pressemappe/PR115664/aom]*** OTS-ORIGINALTEXT PRESSEAUSSENDUNG UNTER AUSSCHLIESSLICHER INHALTLICHER VERANTWORTUNG DES AUSSENDERS - WWW.OTS.AT ***APA - Austria Presse Agentur eGDocument AUPAG00020160218ec2i003h6"
"73",2000-03-31,"Ad hoc-Service - Vectron Systems AG.","Ad hoc-Service - Vectron Systems AG.795 words31 March 200007:45Reuters Direkt ServiceREUTDSGerman(c) 2000 Reuters LimitedAd hoc-Service: Vectron Systems AGAd-hoc Mitteilung übermittelt durch die DGAP.Für den Inhalt der Mitteilung ist allein der Emittent verantwortlich.------------------------------------------------------------------------------Vectron: Jahresabschluß 1999 belegt enormes Wachstum- Umsatz: + 97% gegenüber 1998- EBIT: + 155% gegenüber 1998- Eigene Prognosen übertroffen- Wachstumstempo wird sich weiter steigernMünster, 31.03.2000Die Vectron Systems AG blickt auf ein exzellentes Geschäftsjahr 1999 zurück.Miteinem Umsatz von DM 19,745 Mio. wurde eine Steigerung um 97% gegenüber demVorjahr erzielt. Überproportional entwickelte sich das EBIT mit einerSteigerungvon 155% gegenüber dem Vorjahr. Damit übertrifft die Gesellschaft die eigenenPrognosen.Trotz der Kosten des Börsengangs in Höhe von DM 2.045 Mio. und umfangreichervorgezogener Investitionen konnte somit ein erheblicher Gewinn erzielt werden.Finanzvorstand Chistoph Haarbeck: ""Unser Unternehmen befindet sich in einerextremen Wachstumsphase, die selbstverständlich sehr hohe Anlaufinvestitionenerfordert. Wir sind sehr stolz, daß wir aufgrund unserer margenstarkenProduktetrotzdem schon jetzt einen sehr ansehnlichen Gewinn vorweisen können. Für daslaufende Jahr erwarten wir einen Umsatz von ca. DM 50,0 Mio. und ein EBIT vonca. DM 7,0 Mio. Nach Ablauf der Investitionsphase im Jahr 2001 erwarten wirnochstärker steigende Renditen.""Wegen der hervorragenden internationalen Resonanz auf dieCeBIT-Produktneuheitenund der Gewinnung vieler Exklusivhändler sieht sich die Gesellschaft in einerhervorragenden Marktposition. Organisationsvorstand Olaf Arlinghaus: ""NachAbrundung der Produktpalette werden wir uns jetzt verstärkt um den weiterenAusbau der internationalen Vertriebskanäle kümmern. Insbesondere vor demHintergrund des erwarteten Nachfragebooms aufgrund der Euroumstellung möchtenwir unsere technologisch führenden Produkte in ganz Euroland optimalpositionieren. Da wir noch über erhebliche Barreserven verfügen und keinerleiBankkredite in Anspruch nehmen, sind die erforderlichen Mittel vorhanden. ""Die Vectron Systems AG ist der schnellstwachsende internationale Newcomer aufdem Markt für intelligente Kassensysteme. Vectron ist der einzige Anbieterweltweit mit einer universell einsetzbaren Softwareplattform. Vectron istheutebereits in 15 Ländern vertreten. Die Gesellschaft investiert auch in dieEntwicklung zukünftiger Point-of-sale-Lösungen auf Basis vonInternet/E-commerce, Spracherkennung, Multimedia und Biometrie.Bei Fragen wenden Sie sich bitte an:Vectron Systems AGAn der Kleimannbrücke 13a48157 MünsterHerrn Olaf ArlinghausVorstand Organisation u. UnternehmensentwicklungTel: 0251/2856-0Email: oarlinghaus@vectron.de---------- English Version ----------Vectron: Annual report 1999 proves enormous growth- Turnover: + 97% compared to 1998- EBIT: + 155% compared to 1998- Own prognosis exceeded- Speed of growth will still increaseMünster, March 31, 2000The Vectron Systems AG is looking back at an excellent fiscal year 1999.Compared to the previous year turnover increased by 97% to DM 19.745 million.EBIT showed an overproportional increase of 155% compared to the previousyear.With these results the corporation exceeds its own prognosis.Despite expenses of DM 2.045 million related to Vectron's IPO and extensiveearly investments, the corporation was able to make considerable profit.Director of Finance and Controlling Christoph Haarbeck: ""Our company is in aphase of extreme growth which naturally requires very high initialinvestments.We are very proud to be already able to present such a significant profitwhichis due to our high-profits products. For the current year, we expect aturnoverof approx. DM 50.0 million and an EBIT of approx. 7.0 million. We believe thatwhen this phase of investment will be completed in 2001, the return on capitalwill increase even more.""Due to the remarkable international response to the innovative productspresented on the CeBIT, Vectron considers itself to be in an excellent marketposition. Acquiring many exclusive dealers further supports this assessment.Director of Business Development and Organization Olaf Arlinghaus: ""Afterhavingrounded off our range of products, we will now focus on extending ourinternational sales channels. Against the background of an expected boom indemand due to the monetary switch-over to the EURO, we want to position ourtechnological leading products on the whole Euro-zone market. We still have alot of cash in hand and do not rely on bank loans - the necessary means aretherefore available.Vectron Systems AG is the fastest-growing international newcomer on the marketfor intelligent POS systems. Vectron is the only supplier of POS systemsworld-wide with the revolutionary dual platform strategy, making the VectronPOSsoftware available for all trades and all relevant hardware platforms. Today,the company is present in 15 different countries. In addition to that, thecompany is investing in the development of future point-of-sale solutions likeInternet/e-commerce, speech recognition, multimedia and biometry.Contact:Vectron Systems AGAn der Kleimannbrücke 13a48157 MünsterGermanyMr Olaf ArlinghausDirector of Business Development and OrganizationPhone: +49  (0) 251/2856-0Email: oarlinghaus@vectron.deEnde der Mitteilung.(c) Reuters Limited 2000.Document reutds0020010816dw3v001b4"
"74",2000-05-31,"Fonix gibt FAAST Text-to-Speech SDK heraus.","Fonix gibt FAAST Text-to-Speech SDK heraus.652 words31 May 2000OTS - OriginaltextserviceOTSGerman(c) 2000 News AktuellFonix FAAST(TM) TTS SDK integriert unbegrenzte und Standard-Vokabular Text-to-Speech Engines für Internet-und Telefonie-Anwendungen.Fonix Corporation (OTC Bulletin Board: FONX) kündigte die kommerzielle Herausgabe seines FAAST Text-To-Speech (TTS) Software Development Kit (SDK) an. Dieses Software-Entwicklungs-Werkzeug wurde entworfen, um Entwicklern von Software zu ermöglichen, TTS-Systeme mit natürlichem Sound in Voice-Portale, Call Center und andere Internet-und Telephonie-Anwendungen zu integrieren.Fonix FAAST TTS SDK ermöglicht die nahtlose Integration von unbegrenztem Vokabular TTS und Standard-Vokabular TTS mit der Sprachqualität einer menschlichen Stimme. Diese Sprachvokabulare sind beide nicht von einer natürlichen und fortlaufenden menschlichen Rede zu unterscheiden.Beta-Partner und bestehende Kunden verwenden Fonix TTS für die Integration in einheitliche Messaging Services, e-Mail-Reader, HTML-basierte Web-Reader, Lern-Software für Englisch, Auto-Begleiter, Aktienkurs-Systeme, Web-basierte College-Bildungskurse, Services für Kurznachrichten, Voice-Portale, IVR-Systeme und andere Anwendungen.Das gegenwärtig lieferbare FAAST TTS SDK läuft unter den Betriebssystemen Microsoft Windows 95/98, NT und Windows 2000. Für Linux, AIX und Solaris Sparc wird FAAST TTS SDK im dritten Quartal 2000 lieferbar sein. FAAST TTS SDK umfasst ein TTS API-Set von hohem Niveau, mit dem die Kunden problemlos Text-to-Speech in ihre Produkte integrieren können. FAAST API unterstützt C++ und Java, und ist mit SAPI 4.0 kompatibel.""Fonix hat ein vielseitiges TTS-Entwicklungs-System erarbeitet, das den Entwicklern ermöglicht, das am besten geeignete TTS mit natürlichem Sound für ihre spezielle Anwendung zu gebrauchen. Dadurch kann schnell ein Marktanteil erreicht werden"", sagte Thomas A. Murdock, President und Chief Executive Officer von Fonix Corporation.Fonix CorporationFonix Corporation (OTC Bulletin Board: FONX) entwickelt und vermarket Software-Produkte, die eine intuitive menschliche Interaktion mit Computern, Verbraucher-Elektronik und anderen intelligenten Geräten, ermöglicht. Führende Hersteller von Chips, unabängige Software-und Hardware-Verkäufer sowie andere Internet Content Provider integrierten die Fonix-Technologie, um ihre Produkte in ihrer Anwendung leichter und angenehmer zu gestalten. Die Fonix-Produkte, einschließlich Text-To-Speech (TTS), Automatic Speech Recognition (ASR), (automatische Spracherkennung)) und Handwriting Recognition (HWR), (Erkennung von Handschriften)) bieten die natürlichsten Kommunikations-Lösungen, die auf dem Markt sind. Für weitere Informationen besuchen Siewww.fonix.com[http://www.fonix.com/]oder rufen Sie unter (801) 553-6600 an.Anmerkung: Die von Fonix Corporation herausgegebenen Informationen, die nicht rein historischer Natur sind, beinhalten zukunftsbezogene Annahmen im Sinne der ""Safe Harbor""-Vorbehaltsklauseln des Private Securities Litigation Reform Act von 1995. Diese Annahmen schließen mit ein: Aussagen bezüglich der Erwartungen, Hoffnungen, Absichten und Strategien für die Zukunft. Investoren werden ausdrücklich darauf hingewiesen, dass zukunftsbezogene Annahmen Risiken und Unsicherheiten unterliegen, die die Geschäftsaussichten und - leistungen des Unternehmens beeinträchtigen können. Es ist wichtig zur Kenntnis zu nehmen, dass die tatsächlichen Ergebnisse des Unternehmens erheblich von den in den zukunftsbezogenen Annahmen zum Ausdruck gebrachten abweichen können. Spezifische Risiken, die sich auf die durch diese Pressemeldung herausgegebenen Informationen beziehen schließen mit ein: Risiken im Zusammenhang mit der Einführung eines neuen Technologie-Produkts und seiner Akzeptanz auf dem Markt, Unsicherheiten bezüglich einer Reihe von Entwicklern, die SDK und die daraus entwickelten potenziellen Anwendungen einsetzen werden, die Lizenzerträge für das Unternehmen und Schwankungen in der Mikrochip-Branche hervorbringen können, die wiederum die Anzahl der - die Fonix-Technologie einschließenden - verkauften Chips beeinträchtigen können. Andere Risikofaktoren, einschließlich der allgemein wirtschaftlichen, behördlichen, und technologischen Faktoren sind in den bei der Securities and Exchange Commission (SEC) auf den Formularen 10-K, 10-Q und 8-K vom Unternehmen eingereichten Akten aufgeführt. Das Unternehmen lehnt jegliche Verantwortung ab, die in dieser Pressemeldung enthaltenen zukunftsbezogenen Annahmen auf den neuesten Stand zu bringen.ots Originaltext: Fonix Corporation Im Internet recherchierbar:http://recherche.newsaktuell.de[http://recherche.newsaktuell.de]Rückfragen bitte an: Kelly R. Kenyon, Corporate Communications, (USA) 801-553-6600 kkenyon@fonix.com, oder Michelle Aamodt, Investor Relations, (USA) 801-328-0161 invrel@fonix.com, beide von Fonix CorporationWeb site:http://www.fonix.com[http://www.fonix.com].(c) News Aktuell 2000.Document ots0000020010816dw5v0033x"
"75",2015-01-05,"VoiceBox und Elektrobit arbeiten zusammen, um natürliche Spracherkennung in vernetzten Autos zu ermöglichen","VoiceBox und Elektrobit arbeiten zusammen, um natürliche Spracherkennung in vernetzten Autos zu ermöglichen579 words5 January 2015news aktuell OTS - OriginaltextserviceOTSGerman(c) 2015 news aktuell   -- Unternehmen werden den mit VoiceBox-erweiterten EB GUIDE für die Automobilwelt anbietenBellevue, Washington (ots/PRNewswire) - VoiceBox Technologies, der mit Preisen ausgezeichnete innovative Anbieter für kontextbezogene natürliche Spracherkennung (Contextual Natural Language Understanding / NLU) und ein anerkannter Pionier für das vernetzte Auto, hat seine Partnerschaft mit Elektrobit weiter ausgebaut. Die neue Vereinbarung zwischen den Unternehmen ermöglicht beiden, ihre kombinierten Lösungen anzubieten, um eine von VoiceBox aktivierte, multimodale Human-Machine Interface (HMI) Entwicklungsplattform für die Automobilbranche bereitzustellen.Die technologische Kooperation im letzten Jahr, bei der VoiceBox den EB Guide von Elektrobit mit seinen patentierten Technologien für Context Management und Conversational Voice ausgestattet hat, hat echte Innovationskraft bewiesen. Konsumenten können über den herkömmlichen HMI-Ansatz mit nur einer Frage und einer Antwort hinausgehen, weil VoiceBox den Nutzer erlaubt, mehrere Themen in jeder Reihenfolge abzufragen - so wie es den Nutzern in den Sinn kommt - und bei Bedarf auch Folgefragen zu stellen. Zu den jüngsten Fortschritten gehört die Integration der Deep Neural Networks (DNN) Modelle, die die Spracherkennung mit einem großen Vokabular entscheidend verbessert.""Der VoiceBox-Vorteil ergibt sich aus der einzigartigen Kombination des ausgereiften Designs der Sprachschnittstelle mit der patentierten, kontextbezogenen natürlichen Spracherkennung. Wir sind stolz darauf, unsere Lösungen zusammen mit dem EB GUIDE von Elektrobit anzubieten, um für die Nutzer natürliche und personalisierte Spracherkennung auf den Automobilmarkt zu bringen"", sagte Mike Kennewick, CEO von VoiceBox.Der VoiceBox-Vorteil basiert auf einmaligen Fähigkeiten, die kürzlich in der IEEE-gesponserten Patent Power-Bestenliste [http://spectrum.ieee.org/at-work/innovation/patent-power-2013[http://spectrum.ieee.org/at-work/innovation/patent-power-2013]] genannt wurden, auf der VoiceBox unter den 15 Spitzenunternehmen mit der einflussreichsten Computer-Softwaretechnologie-IP weltweit rangierte. VoiceBox war das einzige Technologieunternehmen für Spracherkennung in dieser Spitzengruppe.VoiceBox bietet für Unternehmen, die Produkte und Lösungen mit Sprachaktivierung ausstatten wollen, ein komplettes Leistungsspektrum. Dies umfasst erweiterte automatische Spracherkennung (Automatic Speech Recognition / ASR), NLU sowie Text-zu-Sprache-Technologien (Text To Speech / TTS) mit der gesamten Bandbreite von Entwicklungs- und Testverfahren und Datendiensten. Als renommierter und innovativer Anbieter für Sprachanwendungen im Automobilsektor hat VoiceBox im Jahr 2008 die erste integrierte NLU-Schnittstelle in der Branche bereitgestellt und für das vernetzte Auto im Jahr 2011 Pionierarbeit geleistet. Das Unternehmen wurde zusammen mit seinen Partnern mehrfach mit den Auszeichnungen CES Best in Show geehrt. Anwendungen von VoiceBox werden in 23 Sprachen und auf drei Kontinenten von Unternehmen wie Toyota, Fiat, Dodge, Chrysler, Maserati, Renault, Mazda und Tom-Tom eingesetzt und ausgeliefert.Über VoiceBox Technologies CorporationVoiceBox ist ein Pionier im Bereich Natural Language Understanding und Conversational Voice Technology (Technologien zur Sprach- und Stimmerkennung), der Menschen dabei unterstützt, Inhalte zu finden und ihre Geräte mithilfe der natürlichen Alltagssprache zu steuern. Die Anwendungen von VoiceBox können in Systemen im Auto, auf Smartphones und in Heimsystemen eingesetzt werden. Alle werden von der mit Preisen ausgezeichneten VoiceBox-Sprachtechnologie angetrieben, die von über 30 Patenten geschützt wird. Das Unternehmen mit Hauptsitz in Bellevue, Washington, unterhält Niederlassungen in Los Angeles, München, den Niederlanden und Tokio. Für weitere Informationen besuchen Sie bitte VoiceBox [http://www.voicebox.com/technology/[http://www.voicebox.com/technology/]].VoiceBox und das Logo von VoiceBox sind eingetragene Warenzeichen der VoiceBox Technologies Corporation in den Vereinigten Staaten von Amerika und/oder anderen Ländern. Alle anderen Unternehmensnamen oder Produktbezeichnungen können Warenzeichen ihrer entsprechenden Eigentümer sein.Web site: http://www.voicebox.com/[http://www.voicebox.com/]OTS: VoiceBox Technologies Corporation newsroom: http://www.presseportal.de/pm/115664[http://www.presseportal.de/pm/115664] newsroom via RSS: http://www.presseportal.de/rss/pm_115664.rss2[http://www.presseportal.de/rss/pm_115664.rss2]Pressekontakt: KONTAKT: Jennifer Cortel, jenniferc@voicebox.com2919167news aktuell GmbHDocument OTS0000020150105eb150030i"
"76",2000-04-20,"Fonix bringt ihre Texte in Sprache umwandelnde Software FAAST TTS für Linux x86 auf den Markt.","Fonix bringt ihre Texte in Sprache umwandelnde Software FAAST TTS für Linux x86 auf den Markt.726 words4 August 200008:29Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS""Die neue Version soll den Standard für sprachfähige Internet-Anwendungen auf der Linux-Plattform setzen. Salt Lake City (ots-PRNewswire) - Die Fonix Corporation (OTC Bulletin Board: FONX), ein führender Anbieter von Benutzerschnittstellenlösungen für Mobilfunk-und mobile Geräte, Fahrzeugtelematik, Internet-und Telefonsysteme, gab die kommerzielle Einführung ihres FAAST (Fonix Accelerated Application Solutions Technology) Text-To-Speech (TTS) Software Development Kit (SDK) für Linux x86 bekannt.Die Markteinführung des Fonix FAAST TTS Linux x86 SDK bereitet den Weg für die Technologien von Fonix, den Standard für sprachfähige Anwendungen auf der Linux-Plattform zu setzen. Die neue Version ist für Entwickler gedacht, die sich schnelle Sprachintegration in ein Open Source-Hochgeschwindigkeitsbetriebssystem wünschen, und ermöglicht die nahtlose Integration der TTS-Software mit außergewöhnlicher Qualität und unbegrenztem Vokabular sowie eine speziell angepasste Vokabular-TTS.Die derzeitigen Kunden sowie die Partner bei der Beta-Version setzen Fonix TTS in den Unified Messaging Services, den Email-Readern, den HTML-basierten Web-Readern, der englischen Lernsoftware, den Auto-Begleitern, den Aktienkurssystemen, den auf dem Web basierenden College-Kursen, den Short Messaging Services, den Sprachportalen, den IVR-Systemen und weiteren Applikationen ein.""Ohne Fonix FAAST TTS für Linux würde unser Produkt Silent Messenger zwei PCs benötigen,"" sagte Jerry Geis, Vice President für Software Development von MessageNet Systems. ""Die neue Version wird unsere zusätzlichen Hardware-und Softwarekosten praktisch beseitigen.""Der Fonix FAAST TTS SDK für Linux x86 beinhaltet einen hochwertigen TTS Application Programmer Interface (API)-Satz, der die Kunden in die Lage versetzt, die Texte in Sprache umwandelnde Software (Text-To-Speech TTS) von Fonix problemlos in ihre Produkte zu integrieren. Darüber hinaus erleichtert die Nutzung der Red Hat Package Manager-Dateien durch den SDK die schnelle Installation und versetzt die Programmierer in die Lage, mit der Entwicklung fast sofort zu beginnen. Die Fonix FAAST API unterstützt außerdem die Programmiersprachen C++ und Java.""Um die wachsende Gemeinschaft der Linux-Softwareentwickler zu unterstützen, hat Fonix ein vielseitiges TTS-Entwicklungssystem hervorgebracht, das schnell Marktanteile gewinnen kann, indem es die Entwickler in die Lage versetzt, für ihre spezifischen Anwendungen die geeignetsten natürlich klingenden TTS einzusetzen,"" sagte Thomas A. Murdock, Chairman und Chief Executive Officer von Fonix. ""Wir verstehen die Notwendigkeit, die Entwicklung mehrerer Plattformen zu unterstützen, und freuen uns, Linux in die wachsende Familie der Plattformen aufzunehmen, die wir unterstützen.""Firmenprofil der Fonix Corporation Die Fonix Corporation (OTC Bulletin Board: FONX) entwickelt und vermarket Software-Produkte, die eine intuitive menschliche Interaktion mit Computern, Verbraucherelektronik und anderen intelligenten Geräten ermöglicht. Führende Hersteller von Chips, unabhängige Software-und Hardware-Verkäufer sowie andere Internet Content Provider integrierten die Fonix-Technologie, um ihre Produkte in ihrer Anwendung leichter und angenehmer zu gestalten. Die Fonix-Produkte, einschließlich Text-To-Speech (TTS), Automatic Speech Recognition (ASR) (automatische Spracherkennung) und Handwriting Recognition (HWR) (Erkennung von Handschriften) bieten die natürlichsten Kommunikationslösungen, die auf dem Markt sind. Weitere Informationen erhalten Sie unterwww.fonix.com[http://www.fonix.com/]oder telefonisch unter (USA) (801) 553-6600.Hinweis: Die von Fonix Corporation herausgegebenen Informationen, die nicht rein historischer Natur sind, beinhalten zukunftsbezogene Aussagen im Sinne der ""Safe Harbor""-Vorbehaltsklauseln des Private Securities Litigation Reform Act von 1995. Dazu gehören auch die Aussagen bezüglich der Erwartungen, Hoffnungen, Absichten und Strategien des Unternehmens für die Zukunft. Investoren werden ausdrücklich darauf hingewiesen, dass die zukunftsbezogenen Aussagen Risiken und Unwägbarkeiten unterliegen, die die Geschäftsaussichten und -leistungen des Unternehmens beeinträchtigen können. Es ist wichtig, zur Kenntnis zu nehmen, dass die tatsächlichen Ergebnisse des Unternehmens erheblich von denen in den zukunftsbezogenen Aussagen abweichen können. Die Risikofaktoren, einschließlich der allgemeinen wirtschaftlichen, wettbewerbsbedingten, staatlichen und technologischen Faktoren, sind in den bei der Securities and Exchange Commission (SEC - US-Börsenaufsicht) auf den Formularen 10-K, 10-Q und 8-K vom Unternehmen eingereichten Unterlagen aufgeführt. Das Unternehmen lehnt jegliche Verantwortung ab, die in dieser Pressemeldung enthaltenen zukunftsbezogenen Aussagen auf den neuesten Stand zu bringen.ots Originaltext: Fonix Corporation Im Internet recherchierbar:http://recherche.newsaktuell.de[http://recherche.newsaktuell.de]Rückfragen bitte an: Vertriebs-& Produktinformationen: Scott Lindsey, Tel.: +1 801-553-6600, sales@fonix.com, Medieninformationen: Kurt Herrmann, Tel.: +1 801-553-6600, mediarel@fonix.com, oder Investoreninformationen: Michelle Aamodt, Tel.: +1 801-328-0161 invrel@fonix.com, alle von der Fonix CorporationWebsite:http://www.fonix.com[http://www.fonix.com]*** OTS-ORIGINALTEXT UNTER AUSSCHLIESSLICHER INHALTLICHER		 VERANTWORTUNG DES AUSSENDERS ***OTS027    2000-08-04/08:27.Document aupag00020010804dw84004n8"
"77",2006-01-09,"Automatische Zählerstandserfassung via Telefon Automatic Meter Reading via Telephone   ","Automatische Zählerstandserfassung via Telefon Automatic Meter Reading via Telephone   125 words9 January 2006ElektrizitaetswirtschaftELEKTGerman(c) 2006 Elektrizitätswirtschaft   Dr. Nils Herda, Leiter des Bereichs Business Consulting und Prokurist der Excelsis Informationssysteme GmbH, und Daniel Gerkens, Voice User Interface Designer, Excelsis Business Solutions AG, Zürich/Schweiz.   Die automatische Zählerstandserfassung mit einem Sprachdialogsystem ermöglicht auf bequeme und kostengünstige Art die Selbstablesung von Zählerwerten für die Kunden von Versorgungsunternehmen. Die Kunden können ihre Verbrauchswerte einfach über das Telefon angeben und entlasten so die Callcenter-Agenten und die Sachbearbeiter.   Automatic meter reading with the support of a speech recognition dialog system is a convenient and cost effective way of getting the meter data of the energy suppliers' customers. The customers can simply call the system by phone and report their consumption at anytime without involving call center agents.   VWEW Energieverlag GmbHDocument ELEKT00020060105e21900004"
"78",2016-02-18,"VoiceBox stellt Technologie zur natürlichen Spracherkennung für Samsung Mobilgeräte bereit","VoiceBox stellt Technologie zur natürlichen Spracherkennung für Samsung Mobilgeräte bereit374 words18 February 2016news aktuell OTS - OriginaltextserviceOTSGerman(c) 2016 news aktuell   VoiceBox-Technologielösungen werden für Samsungs Mobilgeräte genutztBellevue, Washington (ots/PRNewswire) - VoiceBox Technologies Corporation kündigte heute eine strategische Partnerschaft mit Samsung für den Einsatz der natürlichen Spracherkennungs-Technologie (Natural Language Understanding, NLU) von VoiceBox in Samsungs Sprachdiensten an. Die Unternehmen haben bereits eng zusammengearbeitet, um einen intelligenten, durch die NLU-Technologie von VoiceBox unterstützten Assistenten zu entwickeln.""Wir sind stolz darauf, dass die Wahl auf uns gefallen ist, um die Sprachdienste einer Reihe von bereits bestehenden und neuen Samsung-Mobilgeräten zu unterstützen"", erklärte Mike Kennewick, CEO von VoiceBox. ""Wir freuen uns auf weitere Gelegenheiten, mit Samsung zusammenzuarbeiten, insbesondere bezüglich unserer Initiativen zu Smart Home und Internet der Dinge"".""Samsung besitzt ein Verständnis dafür, dass Sprachtechnologie sich als kritisch für seine Zukunft erweisen wird. Wir freuen uns über die Partnerschaft mit VoiceBox. Diese Zusammenarbeit wird es uns ermöglichen, den Anwendern ein leistungsstarkes Nutzererlebnis im Bereich Sprache zur Verfügung zu stellen"", so Peter Koo, Senior Vice President Samsung Mobile.VoiceBox ist Pionier in der Bereitstellung von intelligenten, kontextbezogenen Sprachlösungen, und die preisgekrönte Technologie findet sich in vielen der heutigen vernetzten Autos, Smartphones und Wearables. Darüber hinaus bietet das Unternehmen intelligente, kontextbezogene Sprachlösungen für Originalhersteller (Original Equipment Manufacturers, OEMs) an, die Sprachsteuerung für eine Reihe von Produkten im Internet der Dinge-Markt integrieren wollen.Informationen zu VoiceBox Technologies CorporationVoiceBox bietet One-Stop-Shopping für Unternehmen, die auf der Suche nach sprachfähigen Produkten und Lösungen sind und stellt Technologien wie erweiterte automatische Spracherkennung (Automatic Speech Recognition, ASR), natürliche Spracherkennung (Natural Language Understanding, NLU) und Text-zu-Sprache (Text to Speech, TTS) zusammen mit einer vollständigen Palette von Entwicklungs-, Test- und Datendiensten für das Internet der Dinge (IoT) zur Verfügung. VoiceBox ist derzeit im Lieferumfang von Samsung, AT&T, Toyota, Fiat Chrysler Automobiles, Dodge, Maserati, Renault, Subaru, Smart Cars, Pioneer und Tom-Tom in 23 Sprachen auf 3 Kontinenten enthalten. Weitere Informationen finden Sie unter http://www.voicebox.com[http://www.voicebox.com].OTS: VoiceBox Technologies Corporation newsroom: http://www.presseportal.de/nr/115664[http://www.presseportal.de/nr/115664] newsroom via RSS: http://www.presseportal.de/rss/pm_115664.rss2[http://www.presseportal.de/rss/pm_115664.rss2]Pressekontakt: Bitte wenden Sie sich für zusätzliche Presseinformationen (nur Medienpartner) an: Jonathan Richardson WE Communications +1 (425) 638-7834 jrichardson@we-worldwide.com3255007news aktuell GmbHDocument OTS0000020160218ec2i003ea"
"79",2002-03-20,"XCAPI integriert VoIP und ISDN.","XCAPI integriert VoIP und ISDN.273 words3 May 2002Network WorldNETWOR29German(c) Computerwoche Verlag GmbH MünchenNeue ISDN-Schnittstelle Der Wolfsburger Hersteller TE-Systems hat eine neueISDN-VoIP-Schnittstelle vorgestellt. Die als »Cross-Capi« bezeichneteEntwicklung »XCapi« soll es ermöglichen, standardisierte Anwendungen undServices des digitalen Telefonnetzes auch in der IP-Telefonie zu nutzen. DasProdukt arbeitet auf Basis des ITU-Standards H.323 für Voice-over-IP und desISDN-Standards Capi 2.0. Damit erlaubt die Schnittstelle gleichermaßen dasTelefonieren im leitungsvermittelten Netz als auch die Telefonie impaketvermittelten IP-Netzwerk. Der Vorteil dieser Lösung: Sie kommuniziertin beiden Richtungen in die jeweils andere Umgebung und schafft so einevollständige Integration der beiden wichtigen Telefoniestandards. LautHersteller lässt sich XCapi leicht installieren und erlaubt es dann,ISDN-Anwendungen wie »ISDN Voice III« mit VoIP-Telefonen auf einemStandard-PC zu nutzen. TE-Systems plant, das Produkt rechtzeitig zur Systemsfertig zu stellen, der Preis steht noch nicht fest.Bereits erhältlich istdie Version 317 von »ISDN Voice III«. Eine wesentliche Funktion der neuenSoftware ist laut TE-Systems die Möglichkeit, Telefonkonferenzen mitbeliebig vielen Teilnehmern zu realisieren. Zudem stellt die Lösung eineSchnittstelle zwischen Telefonie und Internet dar, die beispielsweise dieAbfrage eines Anrufbeantworters über das Internet erlaubt. Durch dieIntegration von ASR (Automatic Speech Recognition) und »Realspeak« sollendabei Spracherkennung und Sprachgenerierung durch den Computer komfortablerwerden.Die mitgeliefert Software umfasst zahlreiche Beispielprojekte, diesich schnell an individuelle Bedürfnisse anpassen lassen. Ebenfalls imAngebot hat der Hersteller fertige Kommunikationslösungen beispielsweise fürHotlines, Voice-Boxen, Chat-Rooms oder Konferenzserver. (hh)TE-SystemsTel. 053 61/ 8 95-0Fax 0 53 61/ 8 95-9 99www.te-systems.de[http://www.te-systems.de/]2002 Infrastructure.Document networ0020020509dy530000x"
"80",2004-02-20,"Acapela - Hier spricht der Automat  ","Fahrzeug-TechnikAcapela - Hier spricht der Automat  kfz-betrieb Nr. 49/50 vom 02.12.2004 Seite 062186 words2 December 2004kfz-betriebKFZBET062German(c) 2004 kfz-betrieb – offizielles Organ des Deutschen Kraftfahrzeuggewerbes. Alle Rechte vorbehalten.  Wenn der Fahrer Multimedia- oder Navigationsfunktionen seines Autos per Schalter und Bildschirm bedient, ist er oft vom Verkehrsgeschehen abgelenkt. Sprachgesteuerte Systeme, wie sie die Acapela Group anbietet, könnten Abhilfe schaffen. Der nach eigenen Angaben führende Anbieter für Sprachtechnologie in Europa hat zusammen mit Carbot, einem Hersteller von Computerplattformen für Fahrzeuge, eine Sprachsteuerung für ein Fahrzeug-Entertainmentsystem entwickelt.  Sie soll in jedem Fahrzeug mit Autoradio anwendbar sein, ohne dass ein Monitor notwendig ist. Das System liest dem Fahrer per Sprachsynthese E-Mails und Kurznachrichten vor oder nennt Liedtitel und Künstlernamen beim Musikhören. Die Sprachausgabe, von den Acapela-Spezialisten als ""Text-to-speech"" bezeichnet, soll sich nach Angaben des Herstellers durch eine besonders natürlich klingende Stimme auszeichnen.  Um dem Fahrer die Interaktion mit dem Fahrzeug zu ermöglichen, müssen seine gesprochenen Befehle auch sicher erkannt werden. Dies ist Aufgabe der ""Automatic Speech Recognition"". Damit sollen in Zukunft immer mehr Funktionen wie Multimedia, Telefonie und Navigation per Sprache gesteuert werden, erwartet das Unternehmen. Es sieht darin einen Weg zu mehr Verkehrssicherheit.  120402060Vogel Online GmbHDocument KFZBET0020050202e0c20000u"
"81",2017-01-13,"Echo-Assistentin; So lernte Amazons Alexa in Polen Deutsch sprechen","Echo-Assistentin; So lernte Amazons Alexa in Polen Deutsch sprechen765 words13 January 2017WELT onlineWELTONGermanCopyright 2017 Axel Springer SE Amazons Echo-Boxen werden mithilfe der intelligenten Assistentin Alexa gesteuert. Entwickelt wurde die Software allerdings nicht in den USA, sondern in Polen. Ein Besuch im Labor in Danzig.Draußen liegt Schnee, es ist bitterkalt. Hinter der gefrorenen Autoscheibe ziehen Stadtrand-Plattenbauten aus einer anderen Zeit vorbei. Einladend sieht die Gegend nicht aus.Doch im Zentrum der polnischen Stadt Danzig werden nicht nur das Wetter und die Häuser einladender, hier steht auch ein supermodernes Hochhaus, in dem Amazon ein Entwicklungszentrum betreibt. ""Computer Bild"" war zu Besuch.Ausgerechnet hier, und nicht in den USA, entwickelte Amazon in den vergangenen Jahren seine intelligente Assistentin Alexa. Sie ist vergleichbar mit Siri, wohnt aber (bislang) nicht in einem Handy, sondern in einer von zwei Boxen: dem puckähnlichen Echo Dot und dem größeren Zylinder namens Echo. Beide Geräte spielen Musik, steuern Smarthome-Geräte wie Lampen und lesen Nachrichten vor.Damit Alexa in Dosen und Zylinder einziehen konnte, kaufte Amazon im Jahr 2012 die polnische Software-Schmiede Ivona - und damit sogenannte Text-to-Speech-Techniken. Schließlich ist die Umwandlung von Text in möglichst natürlich ausgegebene Sprache der hörbare Teil von Alexa.""Aber eigentlich ist die Sprachausgabe nur der letzte von vier Alexa-Bausteinen"", erklärt Rafal Kuklinski, der Standortleiter. Nach dem Zukauf betraute Amazon die übernommene Firma mit der Aufgabe, die anderen für die Sprachassistenten notwendigen Teile zu entwickeln, um Sprache nicht nur auszugeben, sondern auch zu verstehen.Die kleine polnische Erfolgsgeschichte führte auch dazu, dass die Sprecherinnen für die englische und deutsche Alexa nach Danzig reisen und viele Wochen lang Texte einsprechen mussten. Doch zuvor hatten die Experten von Ivona mehrere Tausend E-Books, Texte und andere Schriftstücke mithilfe einer Software analysiert.In einem jahrelangen Prozess haben die Entwickler schließlich einige Tausend Beispielsätze erstellt. Und diese Sätze haben dann die beiden englischsprachigen Sprecherinnen und die eine deutschsprachige wochenlang mehrere Stunden am Tag eingesprochen.Ganz anders als bei einem herkömmlichen Navi, das zwar ganze Wörter nacheinander spricht, aber nur ein eingeschränktes Vokabular hat. Das Einsprechen aller für Alexa relevanten Wörter hätte dagegen zu viel Zeit gekostet.Gelegentlich dürften sich die Sprecherinnen gewundert haben, denn einen Beispielsatz wie ""1958 gingen Josef und Brigitte gemeinsam die Hauptstraße entlang"" wird Alexa in der Praxis wohl eher selten sagen. Die Identitäten der drei schönen Stimmen sind übrigens streng geheim - nur so viel sickerte durch: Die deutsche Stimme gehört einer Hamburgerin.Die Software von Ivona hat anschließend alle eingesprochenen Sätze wieder zerrupft. Die einzelnen Laute der Sätze und ihre Betonung kann die Software beliebig kombinieren. So kann Alexa fast alles sagen, ohne dass ein Mensch jedes einzelne Wort einsprechen muss.Für das englische Sprichwort ""An apple a day keeps the doctor away"" mussten die Sprecherinnen rund ein Dutzend Beispielsätze vorlesen, die mit Äpfeln und Doktoren gar nichts zu tun hatten.Wer Amazon Echo bereits verwendet hat, weiß: Perfekt ist die Sprachausgabe noch nicht. Aber für einen Computer ist sie schon wirklich gut, und natürlich will Amazon die Qualität steigern sowie als logische Weiterentwicklung mehr Sprachen anbieten: Über 47 Stimmen in 27 Sprachen hat das Labor mittlerweile entwickelt. Weil nicht nur Amazon von seiner Assistentin überzeugt ist, wird Alexa übrigens schon bald in Autos von BMW, Ford und Hyundai sowie in Smarthome-Geräten zu hören sein.Rafal Kuklinski wagt einen Blick in die Zukunft: ""Wir arbeiten daran, künftig auch Emotionen zu verstehen."" Und damit ist nicht nur gemeint, dass Alexa die Emotionen des menschlichen Gesprächspartners besser interpretiert.Auch die Computerstimme soll künftig imstande sein, den richtigen Ton anzuschlagen, wenn sie in den Nachrichten erst einen Sieg der deutschen Fußball-Nationalmannschaft meldet und anschließend von einer Naturkatastrophe berichten muss. Computer mit Emotionen? Das ist schon fast gruselig.Gleich vier Systeme benötigt Amazon, damit Alexa auf eine simple Frage eine natürliche Antwort geben kann. Wie Maschinen die Frage eines Menschen beantworten, erklärt ""Computer Bild"" hier:1. ASRDie Automatic Speech Recognition (automatische Spracherkennung) wandelt Nutzerbefehle wie ""Alexa, spiele den Song ,Hello' von Adele"" in Text um. Nur so können Computer den Befehl überhaupt verarbeiten.2. NLUNatural Language Understanding (Verstehen natürlicher Sprache): Welche wichtigen Bestandteile sind im Befehl enthalten, also welche Absicht hat der Befehlsgeber? ""Song"", ""Hello"" und ""Adele"" sind im Beispiel wichtig.3. DialogmanagerDer Dialogmanager kategorisiert die Anfrage und ""weiß"" so, dass der Nutzer ein Lied hören und nicht etwa eine Lampe einschalten will. Nun durchsucht die Software gezielt Musikdienste wie Spotify oder Amazon.4. TTSWurde ""Hello"" von Adele gefunden, setzt Text-to-Speech (Text-zu-Sprache) eine Antwort aus Lauten zusammen, was der Nutzer dann als Stimme von Alexa wahrnimmt: ""Ich spiele ,Hello' von Adele auf Spotify.""Axel Springer Syndication GmbHDocument WELTON0020170113ed1d000dx"
"82",1998-11-20,"Sprechender Prozessor.","Sprechender Prozessor.Von Klippstätter, Kriemhilde.77 words20 November 1998ComputerwocheCPWCHEGerman(c) Computerwoche 1998Lernout & Hauspie Speech Products und Hitachi Ltd. wollen gemeinsam die Techniken für Automatic Speech Recognition (ASR) und Text to Speech (TTS) in den Risc-Chip ""SH-4"" der Japaner integrieren. Der Baustein soll in Consumer-Geräten eingesetzt werden, die zu klein sind für eigene Eingabegeräte oder kompliziert zu bedienen. Schon beim Vorgängerchip, dem ""SH-3"", waren ASR und TTS implementiert und beispielsweise für Navigationssysteme in Fahrzeugen genutzt worden.(c) Computerwoche 1998.Document cpwche0020010922dubk001uy"
"83",2006-05-20,"SOFTWAREPARK HAGENBERG GRUPPE im ECAustria Technologiepark auf ITnT   www.itnt.at   Informationsmangement, Logistiksysteme, 3D-Vermessung, Mobile & Secure-Systems   ","SOFTWAREPARK HAGENBERG GRUPPE im ECAustria Technologiepark auf ITnT   www.itnt.at   Informationsmangement, Logistiksysteme, 3D-Vermessung, Mobile & Secure-Systems   843 words5 February 200616:36PressetextPRESSEGerman© 2006 Pressetext – News and Press Service for opinion leaders. All rights reserved. For further information see   http://www.pressetext.de[http://www.pressetext.de]Softwarepark Hagenberg Gruppe mit FAW Software Engineering GmbH, RISC Software GmbH, Software Competence Center Hagenberg GmbH und F&E GmbH Fachhochschule Hagenberg im ECAustria Technologiepark auf der ITnT vom 14. bis 16. Februar 2006 im Messezentrum Wien in der Halle A, Stand 0924.   FAW Software Engineering GmbH   FAW ist aufgrund seiner Nähe zur Johannes Kepler Universität Linz hersteller- und technologieneutraler Partner wie auch ein Projekt-Partner für förderbare Vorhaben wie z.B. EU-Projekte, FFG, Ziel-2, etc. zu sehen.   Die Schwerpunktthemen: Bereich Informationsmangement mit Prozessmodellierung, -analyse und -optimierung, Beratung und Unterstützung von großen Unternehmen bei der Konzeption der strategischen IT-Infrastruktur sowie Beratung und Unterstützung von Unternehmen bei öffentlichen Vergabeverfahren im I-Bereich und im Bereich Datenbanktechnologien.   RISC Software GmbH   Die Schwerpunktthemen: Modellierung, Simulation und Optimierung von Logistiksystemen. Die zunehmende Globalisierung der Wirtschaft schafft für die Unternehmen komplexere Informations- und Warenflüsse und fordert dadurch die Logistik. Intelligente Logistikkonzepte haben ein hohes Potenzial zur Leistungsverbesserung und Kostensenkung, da sie individuell auf die Prozesse eines Unternehmens zugeschnitten sind.   Live-Demonstration: Der Teilnehmer testet am Simulationsmodell das Systemverhalten einer realen Anlage für Fahrerlose Transportsysteme.   Software Competence Center Hagenberg GmbH   Die Schwerpunktthemen: Software Tools Entwicklung mit der Präsentation eines Werkzeugbaukasten, welcher unterschiedlichste Entwicklungswerkzeuge für den Bau von Software für Mobiltelefone integriert. Process- and Qualitymanagement im Softwareentwicklungsprozess, Industrial Data Warehousing sowie Object detection, measurement, and tracking für komplexe Szenen im Bereich von z.B. Medizin, Sport und Verkehr   Live-Demonstrationen: 3D Objektverfolgung und -vermessung, optische Echtzeitanwendung zum Tracking der Besucher mit digitalen Hüten.   F&E GmbH Fachhochschule Hagenberg   Die Schwerpunktthemen: Bio-Medical Systems für Informatik, Medizin und Molekularbiologie; Cooperative Media Environments für Neue Medien, Didaktik, Geschäftsprozesse; Office of Tomorow für Mobile and Secure Systems, Mobile Software, Chip-Entwurf, Security und Near Field Communication sowie Gulliver mit Speech Recognition Services for Wireless Mobile Devices und ein PC-gestützter Simulationsbeschleuniger auf Basis neuester Bustechnologie.   Breaking Results of Applied Research - Innovative Technologien der Zukunft   ECAustria Technologiepark auf der ITnT - Di, 14.02.06 bis Do, 16.02.06   Messezentrum Wien, Halle A, Stand 0924, Messeplatz 1, 1020 Wien   Führende Technologiezentren und innovative Unternehmen zeigen im Technologiepark neue Technologien zum Erleben und Angreifen. Auf einer Fläche von rd. 1.000m2 werden Prototypen und Live-Demos in Form eines offenen Themenparcours präsentiert - die Programmschwerpunkte:   Converging Media; Smart & Intelligent Systems; Pervasive Computing; Embedded Systems; Mobile Services; Virtual Reality; E-Learning   ECAustria Technologiepark Aussteller:   Cyberschool (TeilnehmerInnen des größten österreichischen Schul-Wettbewerbs für neue Medien präsentieren ihre innovativsten Projekte), FAW Hagenberg (Informations- und Datenbankmanagement, Data Warehousing), FH Hagenberg (Mobile Safety System & Cooperative Media Environments), ftw. Wien (Handy als interaktiver Zeigestab, Joystick, oder Peilgerät; Privacy Schutz für mobile Dienste; 3D Modelle auf Motion-Handys), Nexera (Mobile IT-Lösungen, Abbildung von Geschäftsprozessen auf Handhelds), Qenta paymentsolutions (High Performance Payments), Research Studios Austria (Intralife, METIS, Personalisierung-Intelligente Empfehlermechanismen, KnowledgePulse®), RISC Software - Uni Linz (Modellierung, Simulation und Optimierung von Logistigsystemen), Software Competence Center Hagenberg (3D Objektverfolgung und -Vermessung, Prozess- und Qualitymanagement im Softwareentwicklungsprozess, Industrial Data Warehousing), Telekom Austria (Converging Media, E-Health, E-Government, E-Education), VRVis (Beispiele von Visualisierung und Virtual Reality, Technologien zum Testen), Wiener Wirtschaftsförderungsfonds(WWFF)/Vienna IT Enterprises(VITE) (Services und Initiativen sowie Förderberatung für IT Unternehmen, Forschungs-, Entwicklungs- und Bildungseinrichtungen)   Jede/r BesucherIn des ECAustria Technologieparks erhält einen kostenlosen persönlichen Förder-Kompass für sein Unternehmen.   Beschränkte Teilnehmerzahl für die Podiumsdiskussionen - Anmeldungen werden der Reihenfolge nach registriert.   Mit Unterstützung von BM für Wirtschaft und Arbeit & Wirtschaftskammer Österreich.   EINTRITT FREI - Programm und Anmeldung:   http://economy.at/anmeldung  [http://economy.at/anmeldung]Alle Infos zur ITnT-Messe 2006:   http://www.itnt.at  [http://www.itnt.at]EConomyAustria   ECAustria ist eine interaktive Info-, Service- und Veranstaltungsplattform zur Förderung der internetgestützten Geschäftsabwicklung in kleinen und mitteren Unternehmen.   Schwerpunkt der ECAustria-Aktivitäten ist die Vermittlung branchenspezifischer Anwenderfahrungen bereits umgesetzter   E & M-Business Projekte mit einer direkten Kontaktmöglichket zu den jeweiligen Projektleitern auf Anwender- und Anbieterseite.   Innovative Technologie- & Forschungsprojekte mit den entsprechenden Förderungs- und Finanzierungsmodellen ergänzen das hohe Informations- und Serviceangeot der Plattform.   Alle diesbezüglich relevanten Informationen sind damit erstmals aktuell und vollständig zusammengefasst. Österreichweite Veranstaltungen und kontinuierliche Medienarbeit ergänzen die vielfältigen Aktiviäten der Plattform.   Private Public Partnership   ECAustria wird im Rahmen eines Private Public Partnership von Alcatel Austria, APA-Gruppe, Avaya Austria, Cisco Systems Austria, IBM Österreich, IDS Scheer Austria, Kapsch BusinessCom, ONE, Raiffeisen-IT, SAP Österreich, SER Solutions, Software AG Österreich, Telekom Austria und Xerox Global Services Austria gemeinsam mit dem BM für Wirtschaft und Arbeit (BMWA), ecoPlus NÖ, der Industriellenvereinigung Österreich und der Wirtschaftskammer Österreich sowie den österreichischen Technologie- und Forschungszentren ARC (smart systems), AWS, CDG, CURE, EC3 Wien, Eutema, evolaris Graz, FFG (ASA, BIT, FFF, TIG), ftw. Wien, ICNM Salzburg, IMBA, IMCC Linz, KERP, Research Studios Austria, Softwarepark Hagenberg, transIT Tirol und dem Verband der Technologiezentren Österreichs (VTÖ) getragen.   Die Europäische Kommission nominierte die Plattform aus 162 europäischen E-Business Initiativen als europaweites Vorzeigeprojekt.   Interessierte Firmen, die ECAustria mit neuen Inhalten und Services unterstützen möchten, wenden sich bitte an christian.czaak@ecaustria.at.   Pressetext Nachrichtenagentur GmbHDocument PRESSE0020060205e22500001"
"84",2018-01-20,"Ruhig mal durchblicken","Technik & ProduktionRuhig mal durchblicken795 words1 February 2018AUTOMOBIL PRODUKTIONAUTOPROD56German©2018 Das menschliche Gehirn kann visuelle Eindrücke schneller und flexibler verarbeiten als akustische, wissen die Experten für ganzheitliche Akustiklösungen sowie Schall- und Schwingungsanalysen Head acoustics. Das Unternehmen entwickelt Hard- und Software für die Messung und Analyse zur Optimierung von Geräuschqualität und macht Geräusche und deren Quellen sichtbar. Ein Instrument dazu sind so genannte akustische Kameras. Die Herzogenrather setzen insbesondere für die Geräuschlokalisierung auf große Distanzen von tiefen Frequenzen und zur hochgenauen Ortung große Mikrofon-Arrays mit vielen Mikrofonen ein. Ihr System Head Visor haben die Experten kürzlich um Head Visor flex ergänzt. Damit kann das Wunsch-Array in Größe, Form und Mikrofon-Zahl passend für die individuelle Messsituation angepasst und kombiniert werden.„Eigens für Head Visor flex haben wir erstmalig spezielle Mess-Mikrofone entwickelt, die den Anforderungen für den Einsatz in einem Mikrofon-Array entsprechen“, sagt Prof. Dr. Klaus Genuit, Gründer und Gesellschafter von Head acoustics. „Die Mikrofonsignale werden mittels der für Head Visor flex spezifisch entwickelten Headlab-Module VFV12 erfasst“, ergänzt Dr. Winfried Krebber, Leiter des Bereichs Entwicklung NVH. Die bewährte Multiple-Eye-Technik findet auch bei Head Visor flex ihre Anwendung. Drei Kameras vermessen den genauen Abstand zum Objekt und liefern in Sekundenschnelle eine exakte Schallquellenlokalisierung. Mit VoCAS - Voice Control Analysis System - habe man zudem eine leistungsfähige Software zur Bewertung von Spracherkennungssystemen an der Hand, hört man. Damit lasse sich die Sprachqualität von Systemen zur automatischen Spracherkennung (Automatic Speech Recognition) objektiv und schnell unter realistischen und reproduzierbaren Testbedingungen bewerten. Die Software berücksichtigt dabei Faktoren wie etwa Hintergrundgeräusche, Sprache oder den Dialekt.Nicht nur die ungehinderte Sicht nach vorn, sondern gerade auch die rückwärtige Verkehrssituation bestmöglich wahrzunehmen, sei für die Fahrsicherheit entscheidend, ist man sich beim 1974 gegründeten Hersteller automatisch abblendbarer Innenspiegel, Gentex, sicher. Längst habe man den Sprung zum „digitalen Innenspiegel“ vollzogen, heißt es. Jüngstes Produkt der US-Amerikaner ist der duale Display-Spiegel Full Display Mirror (FDM), ein bimodales System, dessen 1,7-Megapixel-Kamera hochauflösende Bilder liefert und dem Fahrer den Wechsel zwischen einem normalen Spiegel- und einem kameragestützten Videomodus erlaubt. Ein CMOS-Bildsensor lässt jeden Pixel selbstständig die optimale Belichtung berechnen. Am Ende werden der hellste und der dunkelste Bereich auf dem Bildschirm klar und deutlich angezeigt. Zum anderen zeigt Gentex ein Drei-Kamera-System, das Camera Monitoring System CMS, das einen flächendeckenden Seiten- und Rückblick gewährt. Die Seitenkameras sind dabei in deutlich kleiner dimensionierte Außenspiegelgehäuse integriert, deren Bilder mit jenen aus einer Dach-Kamera verknüpft werden.Dräxlmaier aus dem niederbayerischen Vilsbiburg ist unter anderem bekannt für seine Lichtintegrationen im Fahrzeug-Interieur. Eine neue Entwicklung nennt sich Micro Perforation Light, bei der eine LED-Steuerung die Kombinationen von über 28 Billionen Farben zulässt. Bis zu 30 RGB-Aktuatoren können über ein Master-/Slave-Prinzip synchronisiert werden. Mit Dynamic Light stellt man ein Beleuchtungskonzept vor, mit dem sich unterschiedliche Lichtbewegungen sowie theoretisch unendlich viele Farben darstellen lassen. Das System ermöglicht insbesondere schnelle Lichtläufe. Der Zulieferer hat dies bereits anhand einer Lichtlaufleiste, die in die Türverkleidung eines Premium-Fahrzeugs integriert wurde, umgesetzt. Für das Farbenspiel sind neuartige LIN-RGB-Bausteine hinter einem modernen Diffusor aus partiell lichtdurchlässigem Kunststoff verantwortlich. Die LEDs hat man auf lebenslange Farbgenauigkeit kalibriert.Lichtexperte Osram macht mit einer neuen Photodioden-Generation auf sich aufmerksam. Die gemäß AEC-Q101-C qualifizierten SFH 2200 A01 und SFH 2200 FA A01 sind Angaben des Herstellers zufolge besonders für die Anwendung in Regensensoren geeignet. Das Layout folge dabei dem Miniaturisierungsgedanken, eine neue Generationen im Vergleich zur vorigen weiter zu verkleinern. In der Entwicklung lege man zudem Wert auf die Verbesserung des Moisture Sensitivity Levels (MSL), heißt es. Der Index besagt, wann im Bauteil ein Grad an Feuchtigkeitssättigung erreicht ist, der beim Löten Schäden verursachen kann. Mit den neuen Photodioden bleibt den Kunden nach dem ersten Luftkontakt nach Entpacken nun deutlich mehr Zeit für das Verlöten auf einer Platine: Die Lichtexperten aus München sprechen von bis zu einem Jahr ohne mögliche Schäden im Bauteil. Dies bei Lagerung des Bauteils nach Öffnen der Trockenverpackung unter definierten Bedingungen von einer Temperatur von = 30 °C und 60 Prozent Luftfeuchtigkeit, was einem MSL von 2 entspricht. Vergleichbare Produkte liegen Osram-Angaben zufolge bei einem MSL von 4, bei dem bereits nach 72 Stunden nicht mehr sichergestellt ist, dass das Bauteil schadlos verlötet werden kann. Die neuen Breitband-Dioden sind zudem oberflächenmontierbar und werden im Reflow-Verfahren gelötet, was eine einfache Lötkontrolle an allen Kontakten erlaubt. nText: Götz Fuchslocher(01) Beim dualen Display-Spiegel (FDM) kann der Fahrer zwischen mehreren Ansichten wählen. (02) Beleuchtungskonzept Dynamic Light mit sicherheitsunterstützenden Warn- und Informationsfunktionen. (03) Mit Head Visor flex Schallquellen binnen Sekunden lokalisieren und visualisieren. Die Photodioden SFH 2200 A01 und SFH 2200 FA A01 mit MSL-Level 2 eignen sich besonders für die Anwendung in Regensensoren. Bild: Gentex Bild: Dräxlmaier Bild: Head acoustics Bild: Osram Verlag Moderne IndustrieDocument AUTOPROD20180201ee210000b"
"85",2006-04-11,"Computer zum Diktat im neuen economy Spracherfassung als großer Zukunftsmarkt   www.economy.at  ","Computer zum Diktat im neuen economy Spracherfassung als großer Zukunftsmarkt   www.economy.at  334 words11 April 200615:00PressetextPRESSEGerman© 2006 Pressetext – News and Press Service for opinion leaders. All rights reserved. For further information see   http://www.pressetext.de[http://www.pressetext.de]Spracherfassungssystemen wird eine große Zukunft vorhergesagt, auch wenn die Systeme heute noch hohe Fehlerquoten aufweisen. Wir wollen den Diktierprozess verbessern und auf weitere Sprachen anwenden, erklären Experten der Uni Wien.   Artificial Intelligence und sprachgesteuerte Maschinen   Wenn amerikanische Ärzte ihre Befunde demnächst kostengünstiger und mit weniger Fehlern zu Papier bringen, verdanken sie das den Forschungsarbeiten des Instituts für Signalverarbeitung und Sprachkommunikation an der Technischen Universität Graz, dem Österreichischen Forschungsinstitut für Artificial Intelligence (ÖFAI) und Philips Speech Recognition Systems, die neue Technologien zur professionellen Spracherkennung in Österreich entwickeln.   Worum es dabei geht, schildert Professor Harald Trost vom Institut für medizinische Kybernetik und Artificial Intelligence an der Medizinuni Wien im Interview mit economy.   economy: Kann man jetzt endlich Worte in den Computer diktieren und erhält dann einen weitgehend fehlerfreien Text?   Harald Trost: Ganz so ist das nicht. Die Fehlerquote schwankt zwischen zehn und 30 Prozent, das heißt jedes dritte bis zehnte Wort ist falsch. Das ist zwar enorm viel, aber die Rohübersetzung durch den Computer stellt dennoch eine echte Rationalisierung in Bereichen dar, wo viel diktiert wird. Den Hauptmarkt unseres Partners Philips bilden die USA, wo der größte Dienstleister, der für Krankenhäuser die Abfassung der Berichte übernimmt seinen Sitz hat.   Lesen Sie das komplette Interview im neuen economy   economy ist in gut sortierten Trafiken und über Abonnement unter   http://economy.at  [http://economy.at] beziehbar.   Neben der auszugsweise zitierten Story finden Sie in der aktuellen economy-Ausgabe weitere Berichte, Interviews und Kommentare von Michael Hann, Alexandra Riegler, Karin Mairitsch, Clemens Rosenkranz, Ernst Brandstetter, Hannes Stieger, Klaus Lackner, Rita Michlits, Jakob Steuerer, Lydia J. Goutas und Thomas Jaekle.   economy definiert neues Mediensegment   Mit einem modernen, zwischen Tageszeitung und Magazin liegenden Format auf färbigen Zeitungspapier und einem Umfang von 32 Seiten erscheint economy in einer Auflage von 35.000 Stück. 30.000 Stück werden über Postversand und Trafiken in Österreich verbreitet und 5.000 Stück werden über Sondervertrieb und Versand in CEE-Länder verbreitet.   Pressetext Nachrichtenagentur GmbHDocument PRESSE0020060411e24b00234"
"86",2017-02-20,"reCAPTCHA kann mit Google-Diensten umgangen werden; Eigentlich soll reCAPTCHA von Google Bots eindeutig erkennen.","reCAPTCHA kann mit Google-Diensten umgangen werden; Eigentlich soll reCAPTCHA von Google Bots eindeutig erkennen.Michael Söldner 172 words2 March 201714:44PC-Welt OnlinePCWOLGerman(c) 2017 PC-Welt. www.pcwelt.de[http://www.pcwelt.de] Um die reCAPTCHA-Erkennung zu umgehen, genügt ein simples Python-Skript.Mit dem Google-eigenen reCAPTCHA[https://www.google.com/recaptcha/intro/index.html] lässt sich auf Webseiten erkennen, ob es sich beim Besucher um echte Nutzer oder einen Bot handelt. Das System lässt sich aber umgehen, dafür kann sogar ein anderer Google-Dienst eingespannt werden. Der Sicherheitsforscher mit dem Pseudonym East-EE hat hierfür ein relativ simples Python-Skript[https://east-ee.com/2017/02/28/rebreakcaptcha-breaking-googles-recaptcha-v2-using-google/] entwickelt, das reCAPTCHA austrickst.Möglich wird dies durch die vom System alternativ für sehbehinderte Menschen angebotene Audio-Challenge. Diese spricht dem Nutzer einen Begriff oder eine Wortgruppe vor. Diese muss dann in Textform in ein Formularfeld getippt werden. Das Python-Skript zur Umgehung von reCAPTCHA greift auf den MP3-Download der Audio-Challenge zurück und übergibt die Datei an Googles Speech Recognition API. Diese liefert den benötigten Text zurück und überträgt ihn automatisch ins Formular.Ob und wie Google diese Möglichkeit zur Umgehung von reCAPTCHA schließen wird, bleibt vorerst offen.Facebook, Google & Co. richtig absichernIDG Communications Verlag AGDocument PCWOL00020170302ed320002t"
"87",2011-11-22,"iPhone-Hack: Siri steuert Thermostat","iPhone-Hack: Siri steuert Thermostatpte/Rudolf Felser   264 words22 November 2011Computerwelt OnlineCMPONL20111122German© 2011 Info Technologie Verlag GmbH. All rights reserved. For further information see   http://www.computerwelt.at[http://www.computerwelt.at]Der Sprachassistent des iPhone 4S, Siri, könnte in Zukunft die Inneneinrichtung von Wohnungen steuern.Ein Entwickler, nur bekannt unter dem Twitter-Pseudonym @plamoni, hat einen Proxy-Server entwickelt, der die Funktionalität des Steuerungstools erweitert. In einem Demonstrationsvideo übernimmt er die Kontrolle über seinen Thermostat.Erst vor einigen Tagen ist es den Entwicklern der Firma Applidium geglückt, das proprietäre Protokoll von Siri via Reverse Engineering zu knacken. Die Bastler legten ihre Entdeckung offen und stellten eine komplette Anleitung und Dateien ins Web.Die so gelegte, technische Basis hat sich plamoni zu Nutze gemacht und einen Proxy-Server entwickelt, der Anfragen von Siri auf dem Rückweg in einem WLAN umleiten kann. Konkret gelingt ihm das in einem Video mit seinem netzwerkfähigen Temperaturregler. Über sein iPhone 4S und den Sprachassistenten kann er die aktuelle Raumtemperatur und die Einstellung der Heizung abfragen. Ist es zu kalt oder warm, so lässt sich die Konfiguration per Fernzugriff regeln. Derzeit dürfte die Steuerung auf diesem Wege auf Geräte im gleichen Drahtlosnetzwerk beschränkt sein.Den Quellcode seiner Software, entwickelt mit der freien Programmiersprache Ruby, hat der findige Programmierer online gestellt. Damit steht Experimenten mit weiteren Features nichts mehr im Wege.Dank der Entschlüsselung des Siri-Protokolls kann das mächtige Speech-Recognition-Tool theoretisch auch von älteren iPhone-Modellen, auf iPads oder auch via Android genutzt werden, sofern jemand entsprechende Anwendungen schreibt. Dies ist möglich, da die Sprachauswertung online erfolgt.Der Proxy-Server ist aktuell nur mit dem iPhone 4S kompatibel, ein Jailbreak ist für die Nutzung nicht erforderlich. (pte)Info Technologie Verlag GmbHDocument CMPONL0020111122e7bm00001"
"88",2016-08-20,"Human Machine Interface evolving rapidly in new Dimensions","Gesture RecognitionHuman Machine Interface evolving rapidly in new DimensionsSteve Cliffe *   1545 words8 November 2016Elektronikpraxis OnlineELEKTW030615-2016GermanCopyright 2016. Vogel Business Media GmbH & Co. KG   A new generation of haptic devices is emerging and opening up the third dimension that will make us wonder how we ever got by with simple flat touchscreens with motor-based vibration feedback.Steve Cliffe is CEO at Ultrahaptics and has more than 25 years experience in the electronics industry with significant exposure to global markets.We’ve come a long way since the days of swapping out wires or relays to control computer systems. Just as the mouse made it hard to imagine controlling a PC with just a keyboard, a new generation of haptic devices is emerging and opening up the third dimension that will make us wonder how we ever got by with simple flat touchscreens with motor-based vibration feedback.This emerging technology, which works in conjunction with gesture recognition, is but the next step in our on-going mission to remove the barriers between man and machine. While this art of making our interactions more symbiotic falls under the general term of human-computer interaction (HCI), for the purposes of this article, the term “computer” will refer to any computer or embedded system – machine – that can respond to a human’s input, movement, or even neurological impulses. The input/output devices will range from the humble mouse and graphical user interface, to gesture recognition and a new breed of haptic-feedback systems.Invention of the first mouse in the year 1964It’s easy to take our current modes of controlling computers and embedded systems for granted, but consider for a moment the first general-purpose computer, the Electronic Numerical Integrator and Computer (ENIAC).Getting it to do what the scientists wanted, and then changing those needs, required two early programmers and unsung heroes, Gloria Ruth Gordon and Ester Gerston, to spend hours flicking switches and swapping connections [1].Thankfully, much has changed since the early forties, but it was 20 years between ENIAC’s switches-and-cables approach to the invention of the humble yet groundbreaking mouse in 1964 by Douglas Engelbart [2] (Figure 3). The device comprised a wooden housing for a circuit board and two metal wheels that formed the surface contacts.The concept of the mouse was later refined in 1972 by Xerox Parc’s Bill English to become the “ball mouse”, which replaced the wheels with a ball that, together with two rollers, could monitor movement in any direction, and convert that movement to electrical impulses corresponding to direction and speed. The optical mouse came in 1980, eliminating the ball, which got dirty from rolling around.At the time, English and Xerox Parc had no idea of the value or potential of the mouse [3]. Given what we know now, it’s no surprise that it took a small startup called Apple to see what others could not.They scrapped their current plans, got a license for the mouse for next to nothing, then combined it with a graphical user interface (GUI) and the rest, as they say, is now history.That’s a little simplistic, of course, as they first had to redesign it to be $25 versus $400, and then examine how users would actually engage with it. When they figured that out, that’s when the science of HMI really started to accelerate.Now’s a good point to pause and examine a factor related to HMI. In 1975 Gordon Moore revised his original 1965 observation on IC transistor density to state that it would double every two years.This became known as Moore’s Law, was adopted as a roadmap by the semiconductor industry, and became the foundation for exponential growth in computing horsepower, with lower power and cost per computing function.Acceleration of our ability to interface with machinesStepping back to HMI then, the further refinement of the mouse in part came about as a result of those cheaper and faster transistors and ICs, and improvements in GUIs helped humans interact more quickly with computers, that were themselves getting faster and more powerful.This acceleration of our ability to interface with machines began the process of removing the barriers to a more perfect “symbiosis” that continues to this day.As computing and associated screens moved into industrial applications, the mouse was substituted by touchscreens for simple input-and-response loops.Touchscreens come in capacitive and resistive versions, but for factory environments, resistive became popular as they are “force” based, in that the user has to apply force to make electrical contact between two conducting points, so they can wear gloves, and still interact with the screen (Figure 4).Capacitive touchscreens, on the other hand, depend on the electrical interaction between the user’s finger (usually grounded) and the electrically charged panel, to cause fluctuations in capacitance that are then tracked by the system.Each approach has its respective pros and cons, of course, but the point is that they further simplify the HMI and make it more intuitive. Again, it was Apple that took the touch-based interaction to a whole new level with the introduction of the iPhone in the year 2007.Touchpads (or Apple’s Trackpad) operate according to the same principles of resistive or capacitive interactions as their touchscreen counterparts, but are more of a mouse replacement for laptops and notebooks, without the requirement to be transparent. Now, of course, trackpads and touchscreens are combined on modern laptops.HMI Options Proliferate, QuicklyAs Moore’s Law continued, low-cost, high performance processing and ever-improving software has made speech recognition possible on every-day machines to further simplify and accelerate interactions (Figure 5).Where once the results were unreliable, progress continued and learning algorithms improved too thanks to core research and development by companies such as Nuance and now Google, Amazon and Apple. Amazon’s Echo is now the recognized state-of-the-art in voice-based HMI, allowing users to search the Web, play music, and control the environment, all by voice control.Another form of HMI is gesture recognition, made famous by the movie Minority Report (2002, directed by Steven Spielberg and based on a short story by Philip K. Dick.) While gesture recognition has historically implied the tracking of hand and body movement, more advanced sensors and image processing capabilities mean that facial movement and skin vibration can be tracked to detect emotional states and vocal emissions.As with sensor fusion, where inputs from multiple sensors can be combined to provide new features and capabilities, the fusion of gesture recognition with mid-air haptic feedback has created a new HMI experience.Mid-air haptics have been tried before, using blasts of air, but these quickly fade over distance. However, researchers in Bristol University started looking at how to precisely place ultrasonic waves at a point in space using off-the-shelf, 40-kHz ultrasound transducers.They made progress and founded a company called Ultrahaptics. They then fused the ultra-haptic technology with available gesture recognition technology to enable them to place those ultrasonic waves at specific locations on the hand or fingers to provide the curious sensation of mid-air haptic feedback, without actually touching anything (Figure 6).This ultra-haptic technology is already showing potential in applications ranging from medical, where doctors want to avoid touch controls, to consumer, such as virtual reality augmentation, thus providing a whole new dimension to HMI. Please find more details on www.ultrahaptics.com[http://www.ultrahaptics.com].UltrahapticsGesture Recognition with UltrahapticsUltrahaptics has developed a unique technology that enables users to receive tactile feedback without needing to wear or touch anything. The technology uses ultrasound to project sensations through the air and directly onto the user. Users can ‘feel’ touch-less buttons get feedback for mid-air gestures or interact with virtual objects. The technology was developed at the University of Bristol.Figure 4: Resistive touchscreens are good for industrial environments as they rely on force to make points of electrical contact, so gloves can be used. Capacitive touchscreens, on the other hand, require electrical interaction between the screen and the hand.UltrahapticsFigure 3: The humble but later ubiquitous mouse was invented by Douglas Englebart in 1964 (left) but has come a long way to the modern HP X3000. (Images courtesy of ComputingHistory.org and HP, respectively.)UltrahapticsFigure 6: By combining off-the-shelf ultrasound transducers and gesture-recognition technology with advanced software, Ultrahaptics enabled the sensation of mid-air haptic feedback to augment the HMI experience.UltrahapticsFigure 2: On ENIAC, unsung heroes Gloria Ruth Gordon and Ester Gerston got to spend hours flicking switches and swapping connections – the first computer input devices – to tell the first general-purpose computer what to do next. (Image courtesy of Columbia University.)UltrahapticsFigure 1: Getting machines to do what we need them to do as efficiently as possible is the art of human-machine interface (HMI) design, and is focused on optimizing the devices, connections, and software required to do so.UltrahapticsFigure 5: Amazon’s Echo represents the current state of the art in voice recognition, as much for its ability to recognize voice as the ecosystem Amazon has placed around it to enable users to develop applications to make it more useful.UltrahapticsVogel Business Media GmbH & Co. KGDocument ELEKTW0020161107ecb800001"
"89",2012-12-20,"DSP Timeline of important Events","DSP Timeline of important Events2272 words12 November 2012ElektronikpraxisEPRXIS615-2012German© 2012 ELEKTRONIK PRAXIS – Das professionelle Elektronikmagazin. Alle Rechte vorbehalten.   www.elektronikpraxis.de[http://www.elektronikpraxis.de]As one of the world's major semiconductor manufacturers, Texas Instruments offers digital and analog devices. 30 years ago, the company introduced its first digital signal processor.June 1978:TI’s Speak & Spell debuts at the Summer Consumer Electronics Show. The toy is powered by the TMC0280/TMS5100 – the first self-contained LPC speech synthesizer integrated circuit ever made. The chip introduced digital signal processing technology to consumers.1981:TI establishes the first DSP University Program, designed to support universities interested in digital signal processing technology. TI's support helps move digital signal processor study from Ph.D. and Master’s level to undergraduate course curricula in universities across the country.1982:TI introduces its first digital signal processor (DSP), the TMS32010, at the ISSCC Conference. This DSP was targeted at defense and modems applications and ran at 5 MIPS.1984:TI is the first manufacturer to introduce a second generation of DSPs: the TMS320C2x.1985:TI manufactures first DSP using CMOS process technology and takes the lead in DSP support by setting up the industry’s first DSP technical hotline. Other support includes the industry’s first PC-based development tools and the first support information service accessible via modem.1987:The first consumer toy to use a DSP is Worlds of Wonder’s ""Julie Doll,"" using TI's TMS320C17 for voice recognition.TI publishes its first DSP textbook, Digital Signal Processing Applications with the TMS320 Family.1988:TI introduces the industry’s first floating-point DSP – the TMS320C3x. High-performance applications demanding floating-point performance include voice/fax mail, 3-D graphics, bar-code scanners and video conferencing audio and visual systems. The world’s first DSP-based hearing aid uses TI’s TMS320C1x DSP.1989:TI introduces highest performance fixed-point DSP generation in the industry, the TMS320C5x, operating at 28 MIPS. The C5x delivers two to four times the performance of any other fixed-point DSP. Targeted to the industrial, communications, computer and automotive segments, the C5x DSPs are used mainly in telephones, high-speed modems, printers and copiers.1990:TI offers the first DSP C-source debugger and optimizing ANSI C tools.TI discloses the second floating-point generation, the TMS320C4x, the first DSP architecture designed for the construction of higher performance systems using parallel digital signal processing. Applications include 3-D graphics, digital base stations and other high-speed communication applications, virtual-reality simulators, image processing for MRI and CT medical imaging systems and speech recognition.TI creates the industry’s first DSP starter kit ...... the C2x DSK, a DSP tool that allows designers to experiment with and use DSPs for real-time digital signal processing without a large investment.1991:TI announces the first DSP to cost less than $5 in single quantities. This price for the C1x is comparable to 16-bit microcontrollers, but DSPs provides five to ten times the performance.TI is the first to offer core-based DSP design with customizable DSP (cDSP). cDSP provides a higher level of DSP system integration with faster time-to-market.1992:DSPs become one of the fastest growing segments within the automobile electronics market. The math-intensive, real-time calculating capabilities of DSPs provide future solutions for active suspension, closed-loop engine control systems, intelligent cruise control radar systems, anti-skid braking systems and car entertainment systems.1993:TI creates the TMS320 Software Cooperative, the industry’s first comprehensive DSP software package, containing more than 100 off-the-shelf third-party digital signal processing algorithms for applications including speech, image, motor control and telecommunications software.1994:TI unveils the industry’s highest performance DSP ever with two billion operations per second (BOPS) performance, ten times that of any other DSP. The TMS320C80 is the first commercially available single-chip processor to combine multiple parallel DSPs and a RISC processor onto one chip. The C80 enables real-time, full-duplex interactive videoconferencing, imaging systems, fuzzy logic industrial control, PC sound cards and noise cancellation. More than 65 U.S. patent applications are filed for C80 technology advancements.TI creates the highest performance DSP Starter Kit – the C5x DSK, at $99 each. It enables designers who are new to DSP technology to experiment with and use a DSP for real-time digital signal processing without a large initial investment.TI introduces the first video CD chipset ...... which provides the manufacturers of home entertainment systems such as CD-based movie players, video games and karaoke systems, with the industry’s first complete integrated solution for a full-motion video (FMV) subsystem for these video CD applications.cDSP technology enables the first uni-processor DSP hard disc drive (HDD) from Maxtor – the 171-Mbyte PCMCIA Type III HDD. By replacing a number of microcontrollers, drive costs were cut by 30 percent while battery life was extended and storage capacity increased. In 1994, more than 95 percent of all high performance disk drives with a DSP inside contain a TI TMS320 DSP.TI teams with U.S. Robotics to cut costs and speed modem product development. Joint patents are filed for the first solution to PCMCIA interface integrated with USR’s modem chipset.1995:TI is the first to bridge the gap between fixed- and floating-point DSPs price/performance with the TMS320C32, priced at less than $10 in high volume and run at 40 million floating-point operations per second (MFLOPS).TI develops the first DSP Elite Lab program to award the most distinguished electrical engineering programs with DSP tools and technical support.TI introduces the TMS320C2xx generation for high-performance, low-cost fixed-point design for applications like feature-phones, power-line monitors, modems and security systems. (Delivering 40 MIPS for less than $5 in high volume). The C2xx generation makes a total of eight DSP generations in the TMS320 family, the largest DSP product offering from any supplier.TI launches two different DSPs, the TMS320C545 and the TMS320C546, to provide the first single DSP solutions for next-generation cellular phone standards. TI is also licensing the industry’s first software for half-rate GSM.TI introduces the industry’s most highly integrated DSP, the TMS320C82, with a performance of more than 1.5 BOPS. The C82 DSPs are applied in cellular base stations, motor control, digital camcorders and video disk players, digital satellite system, and modems.TI sponsors an expanded version of the regional DSP design contests – the first-ever worldwide university contest of its type, the “TI DSP Solutions Challenge.” More than 230 engineering teams worldwide enter to win a grand prize of US$100,000.Online DSP Lab implemented, a laboratory for testing ...... TI digital signal processing applications design and development tools on the worldwide web. TI also launches, and the first www DSP hotline, the ""320 Hotline on-Line.""TI produces the first DSP CD-ROM in conjunction with Electronic Engineering Times.1996:TI discloses plans to release the TMS320C54x DSP generation to the mass market at 66, 80 and 100 MIPS performance, and discloses the industry’s first widely available (volume production) DSP with on-chip flash memory - the TMS320F206.TI increases its DSP fab capacity with announcement of a new, $2 billion DMOS6 fabrication facility to be built in Dallas primarily for the manufacturing of DSPs, and is the first to win 1996 Institute of Electrical and Electronics Engineers (IEEE) Corporate Innovation Award specifically for technical excellence in the design and application of DSPs.TI establishes the Jack Kilby IEEE award recognizing accomplishment in DSP. The award is named after TI engineer Jack Kilby, the inventor of the integrated circuit.First digital telephone answering device (DTAD) processors...... which enable true full-duplex speakerphone, caller ID on call waiting and double the message recording time of digital answering machines up to 28 minutes. TI is also one of the first to develop DTAD processors that offer advanced speech compression software and interface directly to a new memory known as NAND flash.TI creates the first DSP floating-point Starter Kit – the TMS320C3x DSK.TI's cDSP technology enables Seagate, one of the world’s largest hard disk drive (HDD) maker, to develop the first mainstream 3.5-inch HDD to adopt a uniprocessor DSP design, integrating logic, flash memory, and a DSP core into a single unit. The T320C2xLP TI cDSP replaces five discrete devices, achieving cost savings compared to previous solutions, improving performance and reducing power consumption.TI introduces TMS320C24x, industry’s first DSP specifically designed to improve system performance, lower system cost, and reduce component count in digital motor and motion control (DMC) systems. The C24x provides optimized motor-control configurations with the integration of an event manager on-chip for applications such as heating and air conditioning systems.1997:TI introduces the TMS320C6x, the world’s most powerful DSP generation, performing at 1600 MIPS/200MHz and delivering 10 times the performance of typical DSPs.TI's DSP solution enabled the industry’s first-available 56Kbit modem with the modem manufacturer US Robotics. The TMS320 DSP modem chipset enables the fastest Internet access over standard phone lines for state-of-the-art modem technology via US Robotics’ x2 technology.TI announces the successful demonstration of the industry’s first programmable DSP that operates at 1-volt and below while performing all the functions of a standard commercial DSP.TI is the first DSP vendor to introduce a standard application program interface (API) to make the TMS320 DSP family easier to program, monitor and debug, and opens the Kilby Center, an advanced research facility, named after Jack S. Kilby, the integrated circuit inventor.TI discloses technology for the TMS320C67x core of floating-point DSPs -- the industry's first processor to cross into the 1-GFLOPS performance range.1998: The industry's first Real-Time Data Exchange (RTDX) ...... development tools capability, an innovative analysis technology that enhances the speed and accuracy of DSP application debugging and saves time to market.Announces the TMS320C6701, the world's highest-performing floating-point DSP, operating at 1 GFLOPS at 167 MHz, and introduces the industry's first DSP with on-chip flash memory, the TMS320F240, targeted at motor, motion and process control.Introduces the Code Composer Studio integrated development environment, the industry's first open, integrated DSP software development environment capable of reducing DSP coding time by up to 50 percent or more, and announces the industry's first DSPs, the TMS320C549 and TMS320C5410, to pack 115 mW at 100 MIPS and up to 64K words of on-chip SRAM into an ultra-small package, the microStar BGA, measuring 12 x 12 x 1.4mm.Price/performance barrier for DSP-based embedded systemsbroken by introducing the TMS320C6202, capable of executing 2000 MIPS, and the TMS320C6211, delivering 1200 MIPS and offering unprecedented performance value.1999: The industry's first DSP-based printer solution, xStream DSP Technology, gives users the ability to print complex layouts with multiple color graphics and images in seconds rather than minutes.Announces the TMS320VC33, the $5 floating-point DSP available, and the TMS320C6711, the first floating-point DSP with 2-level cache memory architecture for outstanding floating point price/performance and low-cost, and ships the first digital baseband product with transistor feature sizes of 0.15-micron Effective or 0.18 drawn. Provides the industry's first PCI-to-DSP bridge device that connects multiple DSPs to the PCI bus with no additional glue logic, and announces the 1.2-volt TMS320C54x catalog DSP that extends the battery life for applications such as cochlear implants, hearing aids and wireless and telephony devices.Introduces the fastest DSP, the TMS320C6203, which runs at 300 MHz, executes at 2400 MIPS and packs the largest amount of memory on any single-core DSP.Announces the industry's first DSP-optimized ADC with on-chip first-in, first-out memory (THS1206) that allows for more efficient communication of data between the ADC and a DSP.Six new TMS320C24x DSPs with 2x as much performance ...... per dollar than any motor controller in the market and enables Technosoft, a TI third party, to announce a C24x motor controller with complete, ready-to-use motor and motion control software pre-loaded in on-chip memory.Provides the first complete DSP-based solution, for the secure downloading of music off the Internet onto portable audio devices, with Liquid Audio Inc., the Fraunhofer Institute for Integrated Circuits and SanDisk.Announces that SANYO Electric will deliver the first Secure Digital Music Initiative (SDMI)-compliant portable digital music player based on TI's C5000 programmable DSPs and Liquid Audio's Secure Portable Player Platform (SP3).Announces the industry's first open DSP software environment, eXpressDSP Real-Time Software Technology, designed to enable an expected tenfold increase in DSP applications, and teams with Telogy Networks to introduce the industry's first integrated silicon and software chipset solution for Internet Protocol (IP) Phones.The full timeline auf TI's DSPs is available online at elektronikpraxis.de (in German language only). //HHTexas Instruments +49(0)8161 803311TI's Speak & Spell Team 1978: Gene Frantz, Richard Wiggins, Paul Breedlove, Larry Brantingham (from left to right)TI's Speak & Spell Team 1978: Gene Frantz, Richard Wiggins, Paul Breedlove, Larry Brantingham (from left to right)Figure 1: DSP die from 1982, TMS32010Figure 2: Ultra-low power C5000 DSP from 1989Figure 1: C6000 DSP from 1997Figure 4: Introduction of the OMAP platform in 2001Figure 5: C674x DSP from 2008Figure 3: C66x-DSP from 2010, based on the KeyStone multi-core architectureFigure 2: new C665x multi-core DSPs from 2012Vogel Business Medien GmbH & Co.KGDocument EPRXIS0020121102e8bc0000l"
"90",2012-12-20,"Texas Instruments' DSP Timeline of Important Events","HistoryTexas Instruments' DSP Timeline of Important Events2258 words12 November 2012Elektronikpraxis OnlineELEKTWNovember-2012GermanCopyright 2012. Vogel Business Media GmbH & Co. KG   As one of the world's major semiconductor manufacturers, Texas Instruments offers digital and analog devices. 30 years ago, the company introduced its first digital signal processor.June 1978:TI’s Speak & Spell debuts at the Summer Consumer Electronics Show. The toy is powered by the TMC0280/TMS5100 – the first self-contained LPC speech synthesizer integrated circuit ever made. The chip introduced digital signal processing technology to consumers.1981:TI establishes the first DSP University Program, designed to support universities interested in digital signal processing technology. TI's support helps move digital signal processor study from Ph.D. and Master’s level to undergraduate course curricula in universities across the country.1982:TI introduces its first digital signal processor (DSP), the TMS32010, at the ISSCC Conference. This DSP was targeted at defense and modems applications and ran at 5 MIPS.1984:TI is the first manufacturer to introduce a second generation of DSPs: the TMS320C2x.1985:TI manufactures first DSP using CMOS process technologya and takes the lead in DSP support by setting up the industry’s first DSP technical hotline. Other support includes the industry’s first PC-based development tools and the first support information service accessible via modem.1987:The first consumer toy to use a DSP is Worlds of Wonder’s ""Julie Doll,"" using TI's TMS320C17 for voice recognition.TI publishes its first DSP textbook, Digital Signal Processing Applications with the TMS320 Family.1988:TI introduces the industry’s first floating-point DSP – the TMS320C3x. High-performance applications demanding floating-point performance include voice/fax mail, 3-D graphics, bar-code scanners and video conferencing audio and visual systems. The world’s first DSP-based hearing aid uses TI’s TMS320C1x DSP.1989:TI introduces highest performance fixed-point DSP generation in the industry, the TMS320C5x, operating at 28 MIPS. The C5x delivers two to four times the performance of any other fixed-point DSP. Targeted to the industrial, communications, computer and automotive segments, the C5x DSPs are used mainly in telephones, high-speed modems, printers and copiers.1990:TI offers the first DSP C-source debugger and optimizing ANSI C tools.TI discloses the second floating-point generation, the TMS320C4x, the first DSP architecture designed for the construction of higher performance systems using parallel digital signal processing. Applications include 3-D graphics, digital base stations and other high-speed communication applications, virtual-reality simulators, image processing for MRI and CT medical imaging systems and speech recognition.TI creates the industry’s first DSP starter kit ...... the C2x DSK, a DSP tool that allows designers to experiment with and use DSPs for real-time digital signal processing without a large investment.1991:TI announces the first DSP to cost less than $5 in single quantities. This price for the C1x is comparable to 16-bit microcontrollers, but DSPs provides five to ten times the performance.TI is the first to offer core-based DSP design with customizable DSP (cDSP). cDSP provides a higher level of DSP system integration with faster time-to-market.1992:DSPs become one of the fastest growing segments within the automobile electronics market. The math-intensive, real-time calculating capabilities of DSPs provide future solutions for active suspension, closed-loop engine control systems, intelligent cruise control radar systems, anti-skid braking systems and car entertainment systems.1993:TI creates the TMS320 Software Cooperative, the industry’s first comprehensive DSP software package, containing more than 100 off-the-shelf third-party digital signal processing algorithms for applications including speech, image, motor control and telecommunications software.1994:TI unveils the industry’s highest performance DSP ever with two billion operations per second (BOPS) performance, ten times that of any other DSP. The TMS320C80 is the first commercially available single-chip processor to combine multiple parallel DSPs and a RISC processor onto one chip. The C80 enables real-time, full-duplex interactive videoconferencing, imaging systems, fuzzy logic industrial control, PC sound cards and noise cancellation. More than 65 U.S. patent applications are filed for C80 technology advancements.TI creates the highest performance DSP Starter Kit – the C5x DSK, at $99 each. It enables designers who are new to DSP technology to experiment with and use a DSP for real-time digital signal processing without a large initial investment.TI introduces the first video CD chipset ...... which provides the manufacturers of home entertainment systems such as CD-based movie players, video games and karaoke systems, with the industry’s first complete integrated solution for a full-motion video (FMV) subsystem for these video CD applications.cDSP technology enables the first uni-processor DSP hard disc drive (HDD) from Maxtor – the 171-Mbyte PCMCIA Type III HDD. By replacing a number of microcontrollers, drive costs were cut by 30 percent while battery life was extended and storage capacity increased. In 1994, more than 95 percent of all high performance disk drives with a DSP inside contain a TI TMS320 DSP.TI teams with U.S. Robotics to cut costs and speed modem product development. Joint patents are filed for the first solution to PCMCIA interface integrated with USR’s modem chipset.1995:TI is the first to bridge the gap between fixed- and floating-point DSPs price/performance with the TMS320C32, priced at less than $10 in high volume and run at 40 million floating-point operations per second (MFLOPS).TI develops the first DSP Elite Lab program to award the most distinguished electrical engineering programs with DSP tools and technical support.TI introduces the TMS320C2xx generation for high-performance, low-cost fixed-point design for applications like feature-phones, power-line monitors, modems and security systems. (Delivering 40 MIPS for less than $5 in high volume). The C2xx generation makes a total of eight DSP generations in the TMS320 family, the largest DSP product offering from any supplier.TI launches two different DSPs, the TMS320C545 and the TMS320C546, to provide the first single DSP solutions for next-generation cellular phone standards. TI is also licensing the industry’s first software for half-rate GSM.TI introduces the industry’s most highly integrated DSP, the TMS320C82, with a performance of more than 1.5 BOPS. The C82 DSPs are applied in cellular base stations, motor control, digital camcorders and video disk players, digital satellite system, and modems.TI sponsors an expanded version of the regional DSP design contests – the first-ever worldwide university contest of its type, the “TI DSP Solutions Challenge.” More than 230 engineering teams worldwide enter to win a grand prize of US$100,000.Online DSP Lab implemented, a laboratory for testing ...... TI digital signal processing applications design and development tools on the worldwide web. TI also launches, and the first www DSP hotline, the ""320 Hotline on-Line.""TI produces the first DSP CD-ROM in conjunction with Electronic Engineering Times.1996:TI discloses plans to release the TMS320C54x DSP generation to the mass market at 66, 80 and 100 MIPS performance, and discloses the industry’s first widely available (volume production) DSP with on-chip flash memory - the TMS320F206.TI increases its DSP fab capacity with announcement of a new, $2 billion DMOS6 fabrication facility to be built in Dallas primarily for the manufacturing of DSPs, and is the first to win 1996 Institute of Electrical and Electronics Engineers (IEEE) Corporate Innovation Award specifically for technical excellence in the design and application of DSPs.TI establishes the Jack Kilby IEEE award recognizing accomplishment in DSP. The award is named after TI engineer Jack Kilby, the inventor of the integrated circuit.First digital telephone answering device (DTAD) processors...... which enable true full-duplex speakerphone, caller ID on call waiting and double the message recording time of digital answering machines up to 28 minutes. TI is also one of the first to develop DTAD processors that offer advanced speech compression software and interface directly to a new memory known as NAND flash.TI creates the first DSP floating-point Starter Kit – the TMS320C3x DSK.TI's cDSP technology enables Seagate, one of the world’s largest hard disk drive (HDD) maker, to develop the first mainstream 3.5-inch HDD to adopt a uniprocessor DSP design, integrating logic, flash memory, and a DSP core into a single unit. The T320C2xLP TI cDSP replaces five discrete devices, achieving cost savings compared to previous solutions, improving performance and reducing power consumption.TI introduces TMS320C24x, industry’s first DSP specifically designed to improve system performance, lower system cost, and reduce component count in digital motor and motion control (DMC) systems. The C24x provides optimized motor-control configurations with the integration of an event manager on-chip for applications such as heating and air conditioning systems.1997:TI introduces the TMS320C6x, the world’s most powerful DSP generation, performing at 1600 MIPS/200MHz and delivering 10 times the performance of typical DSPs.TI's DSP solution enabled the industry’s first-available 56Kbit modem with the modem manufacturer US Robotics. The TMS320 DSP modem chipset enables the fastest Internet access over standard phone lines for state-of-the-art modem technology via US Robotics’ x2 technology.TI announces the successful demonstration of the industry’s first programmable DSP that operates at 1-volt and below while performing all the functions of a standard commercial DSP.TI is the first DSP vendor to introduce a standard application program interface (API) to make the TMS320 DSP family easier to program, monitor and debug, and opens the Kilby Center, an advanced research facility, named after Jack S. Kilby, the integrated circuit inventor.TI discloses technology for the TMS320C67x core of floating-point DSPs -- the industry's first processor to cross into the 1-GFLOPS performance range.1998: The industry's first Real-Time Data Exchange (RTDX) ...... development tools capability, an innovative analysis technology that enhances the speed and accuracy of DSP application debugging and saves time to market.Announces the TMS320C6701, the world's highest-performing floating-point DSP, operating at 1 GFLOPS at 167 MHz, and introduces the industry's first DSP with on-chip flash memory, the TMS320F240, targeted at motor, motion and process control.Introduces the Code Composer Studio integrated development environment, the industry's first open, integrated DSP software development environment capable of reducing DSP coding time by up to 50 percent or more, and announces the industry's first DSPs, the TMS320C549 and TMS320C5410, to pack 115 mW at 100 MIPS and up to 64K words of on-chip SRAM into an ultra-small package, the microStar BGA, measuring 12 x 12 x 1.4mm.Price/performance barrier for DSP-based embedded systemsbroken by introducing the TMS320C6202, capable of executing 2000 MIPS, and the TMS320C6211, delivering 1200 MIPS and offering unprecedented performance value.1999: The industry's first DSP-based printer solution, xStream DSP Technology, gives users the ability to print complex layouts with multiple color graphics and images in seconds rather than minutes.Announces the TMS320VC33, the $5 floating-point DSP available, and the TMS320C6711, the first floating-point DSP with 2-level cache memory architecture for outstanding floating point price/performance and low-cost, and ships the first digital baseband product with transistor feature sizes of 0.15-micron Effective or 0.18 drawn. Provides the industry's first PCI-to-DSP bridge device that connects multiple DSPs to the PCI bus with no additional glue logic, and announces the 1.2-volt TMS320C54x catalog DSP that extends the battery life for applications such as cochlear implants, hearing aids and wireless and telephony devices.Introduces the fastest DSP, the TMS320C6203, which runs at 300 MHz, executes at 2400 MIPS and packs the largest amount of memory on any single-core DSP.Announces the industry's first DSP-optimized ADC with on-chip first-in, first-out memory (THS1206) that allows for more efficient communication of data between the ADC and a DSP.Six new TMS320C24x DSPs with 2x as much performance ...... per dollar than any motor controller in the market and enables Technosoft, a TI third party, to announce a C24x motor controller with complete, ready-to-use motor and motion control software pre-loaded in on-chip memory.Provides the first complete DSP-based solution, for the secure downloading of music off the Internet onto portable audio devices, with Liquid Audio Inc., the Fraunhofer Institute for Integrated Circuits and SanDisk.Announces that SANYO Electric will deliver the first Secure Digital Music Initiative (SDMI)-compliant portable digital music player based on TI's C5000 programmable DSPs and Liquid Audio's Secure Portable Player Platform (SP3).Announces the industry's first open DSP software environment, eXpressDSP Real-Time Software Technology, designed to enable an expected tenfold increase in DSP applications, and teams with Telogy Networks to introduce the industry's first integrated silicon and software chipset solution for Internet Protocol (IP) Phones.TI's Speak & Spell Team 1978: Gene Frantz, Richard Wiggins, Paul Breedlove, Larry Brantingham (from left to right)Texas InstrumentsTI's Speak & Spell Team 1978: Gene Frantz, Richard Wiggins, Paul Breedlove, Larry Brantingham (from left to right)Texas InstrumentsFigure 1: DSP die from 1982, TMS32010TIFigure 2: Ultra-low power C5000 DSP from 1989TIFigure 3: C6000 DSP from 1997TIFigure 4: Introduction of the OMAP platform in 2001TIFigure 5: C674x DSP from 2008TIFigure 6: C66x-DSP from 2010, based on the KeyStone multi-core architectureTIFigure 7: new C665x multi-core DSPs from 2012TIVogel Business Media GmbH & Co. KGDocument ELEKTW0020130107e8bc00003"
"91",2015-12-20,"PRESSEMITTEILUNG/MYNEWSDESK Tag des Hörens: Kommunikation gut, Beziehung gut","PRESSEMITTEILUNG/MYNEWSDESK Tag des Hörens: Kommunikation gut, Beziehung gut891 words12 May 201514:00mynewsdeskMYNEWDGermanCopyright 2015. mynewsdesk   (Mynewsdesk) Fellbach, Deutschland Morgen findet zum fünften Mal der vom Bundesverband der Hörgeräte-Industrie organisierte Tag des Hörens statt, der erstmals von allen Branchenpartnern (BIHA, EUHA, FDH, FGH und BVHI) gemeinsam unterstützt wird. Ziel des bundesweiten Aktionstags, der dieses Jahr unter dem Motto Kommunikation und zwischenmenschliche Beziehungen steht, ist es, die Gesellschaft für die Bedeutung unseres Hörvermögens zu sensibilisieren. Wer andere nicht versteht, kann nicht mit ihnen kommunizieren und rutscht so schnell in die soziale Isolation. Dass Hören der Schlüssel zu einer guten Beziehung sein kann, hat auch eine Studie der Hear the World Foundation* gezeigt: Knapp zwei Drittel der deutschen Hörgeräteträger berichten, dass sie nicht nur wieder besser hören, sondern dass sich ihre Beziehung durch das Tragen von Hörgeräten verbessert hat.Dass uns das Hören mit den Menschen verbindet, wissen insbesondere die bis zu 16 Millionen Menschen in Deutschland, die von Hörverlust betroffen sind. Viele von ihnen kämpfen zunehmend mit dem Verstehen und ziehen sich immer weiter zurück, häufig sogar in der Beziehung mit ihrem Partner. Und das meist ganz unnötig, denn nur jeder Dritte, dem Hörgeräte helfen könnten, trägt auch welche.Beziehungshelfer Hörgerät? Die Hear the World Foundation hat sich in ihrer Studie Hören ist Leben mit dem Thema Hören und Beziehung beschäftigt und Überraschendes erfahren: So sagten 65 Prozent aller befragten deutschen Hörgerätebesitzer, dass sich ihre Beziehung verbessert hat, seitdem sie Hörgeräte tragen. Über ein Viertel (28 Prozent) der deutschen Befragten berichtet sogar von einem erfüllteren Liebesleben, seit sie wieder besser hören.Auch die befragten Partner von Hörgeräteträgern bestätigen den positiven Einfluss des Hörgeräts auf die Beziehung: 81 Prozent begrüßen es generell, dass ihr Partner Hörgeräte trägt. 40 Prozent sind der Meinung, dass sie wieder mehr Aufmerksamkeit von ihrem Partner bekommen, seit er/sie mit Hörgeräten versorgt ist, und 38 Prozent haben das Gefühl, dass sich die Beziehung durch das Hörgerät verbessert hat.Die wichtigsten Fakten im Überblick:- 28% der deutschen Hörgeräteträger (HGT) haben ein erfüllteres Liebesleben, seit sie wieder besser hören - 65% der deutschen HGT berichten von einer verbesserten Beziehung dank Hörgerät - 81% der Partner von Menschen mit Hörverlust begrüßen, dass der Partner ein Hörgerät trägt - 82% der Befragten ohne Hörverlust haben kein Problem, wenn eine neue Bekanntschaft ein Hörgerät trägt - 51% der deutschen HGT fällt es leicht, neue Bekanntschaften zu schließen, etwa genauso viele wie in der GesamtbevölkerungModerne Hörlösungen: Mehr als ein Hörgerät Auch Titus Dittmann, Gründer der deutschen Skateboard-Szene und Phonak Hörgeräteträger, möchte seine Hörgeräte nicht mehr missen in jeglicher Beziehung. Ich bin viel unterwegs, oft unter Leuten und kommuniziere gerne. Wenn man in Gesprächen ständig nachfragen muss, weil man nur die Hälfte versteht, ist das auf Dauer nervig und ermüdend. Seitdem ich das erste Mal meine Hörgeräte getragen habe, weiß ich, wie wichtig gutes Hören ist. Er weiß auch: Moderne Hörgeräte leisten heute viel. Sie passen sich automatisch an die Hörsituation an, blenden Störgeräusche und Wind einfach aus und fokussieren sich auf die Stimme des Sprechers, mit dem man sich gerade unterhalten möchte.Es gibt jedoch immer wieder Momente, in denen selbst modernste Hörgeräte Unterstützung brauchen. Die drahtlosen Kommunikationslösungen von Phonak sorgen in Kombination mit Hörgeräten wie Phonak Bolero V dafür, dass es selbst über Distanz oder in Lärm optimal mit dem Verstehen klappt. Mit dem Roger Pen können Phonak Hörgeräteträger in schwierigen Hörsituationen sogar besser verstehen als Normalhörende, denn der unauffällige, filzstiftgroße Pen überträgt Stimmen direkt auf die Ohren, blendet dabei Störgeräusche aus und überbrückt Distanzen.**Egal ob mobil, im Büro oder am Festnetz zu Hause Telefonieren ist einer unserer wichtigsten Kommunikationswege. Für Hörgeräteträger waren Telefonate, insbesondere mit dem Handy, bisher häufig eine Herausforderung, aber Phonak EasyCall macht jetzt damit Schluss. Es wird einfach an der Rückseite des Mobiltelefons befestigt, und schon werden Gespräche auf beide Ohren übertragen störungsfrei und in optimaler Klangqualität. Dabei verbindet Phonak EasyCall jedes wirelessfähige Phonak Hörgerät mit jedem beliebigen Bluetooth-fähigen Mobiltelefon, selbst wenn es sich um ein älteres Modell handelt.Wer auch zu Hause entspannt telefonieren möchte, kann sich mit dem Phonak DECT CP1 Schnurlostelefon Telefonate in Echtzeit auf beide Hörgeräte übertragen lassen. Der Verstärkungsmodus unterstützt auch beim Telefonieren ohne Hörgerät.Mehr zu den Phonak Hörlösungen erfahren Sie unter: www.phonak.de[http://www.phonak.de]* Hören ist Leben, Hear the World Foundation, 2012** Professor Thibodeau, Linda, PhD (2014), Comparison of speech recognition with adaptive digital and FM wireless technology by listeners who use hearing aids, University of Texas, Dallas, USA, The American Journal of Audiology (in press)Shortlink zu dieser Pressemitteilung: http://shortpr.com/xqox3k[http://shortpr.com/xqox3k]Permanentlink zu dieser Pressemitteilung: http://www.themenportal.de/gesundheit/tag-des-hoerens-kommunikation-g[http://www.themenportal.de/gesundheit/tag-des-hoerens-kommunikation-g] ut-beziehung-gut-11306=== Pressekontakt ===Herr Florian FagnerBSKOM GmbH Herzogspitalstraße 5 80331 MünchenEMail: fagner@bskom.de Website: www.bskom.de[http://www.bskom.de] Telefon: 089 13 95 78 27 16=== Über Phonak ===Phonak, Mitglied der Sonova Gruppe, mit Hauptsitz in Stäfa, Schweiz, entwickelt, produziert und vertreibt seit mehr als 60 Jahren technologisch führende Hör- und Funksysteme. Dabei kombiniert Phonak die profunde Kenntnis in Hörtechnologie und Akustik mit einer intensiven Zusammenarbeit mit Hörakustikern, um Hörvermögen und Sprachverstehen von Menschen mit Hörminderung zu verbessern und somit ihre Lebensqualität zu erhöhen.Phonak bietet eine vollständige Produktpalette an digitalen Hör- und ergänzenden Funklösungen. Mit weltweiter Präsenz treibt Phonak Innovationen voran und setzt neue Maßstäbe in Miniaturisierung und Leistung.(Dies ist eine über Mynewsdesk verbreitete Pressemitteilung. Für den Inhalt ist ausschließlich das herausgebende Unternehmen verantwortlich.)Mynewsdesk GmbHDocument MYNEWD0020150512eb5c000b5"
"92",2008-02-20,"Spracherkennung von Philips geht an Nuance","Unternehmen & MärkteSpracherkennung von Philips geht an NuanceFranz Gansrigler   206 words2 October 2008WirtschaftsblattWIRTSB53210German(c) 2008 WirtschaftsBlatt.   Wien bleibt auch nach dem Verkauf der Spracherkennungsfirma Philips Speech Recognition Systems (PSRS) an den börsenotierten US-amerikanischen Anbieter von Sprach- und Bildbearbeitungslösungen, Nuance Communications Inc., weltweites Zentrum für die Software-Entwicklung von Spracherkennungssystemen. Denn Nuance habe alle Verträge der 170 PSRS-Mitarbeiter - davon 130 in Österreich - eins zu eins übernommen, sagt Beate McGinn, Konzernsprecherin von Philips Austria.Philips hat PSRS um 65 Millionen € an Nuance verkauft und erwartet einen Netto-Verkaufserlös von etwa 40 Millionen €. Dieser wird im Finanzergebnis der Sparte Philips Healthcare im dritten Quartal 2008 ausgewiesen. PSRS setzte 2007 zirka 25 Millionen € um.Der Verkauf wird damit begründet, dass sich Philips Healthcare vor allem auf Lösungen für die diagnostischen und therapeutischen Bedürfnisse von Patienten und ihre medizinischen Betreuer konzentriert. Spracherkennungslösungen wie SpeechMagic, die PSRS-Plattform, dienen aber eher dem Informationsmanagement als Teil der administrativen Spitalsprozesse. ""Das passt nicht zu unserer Strategie"", so McGinn. PSRS werde eine wichtige Ergänzung für das US-Unternehmen sein und dessen Position am europäischen Spracherkennungsmarkt stärken.Philips Austria steigerte 2007 mit 881 Mitarbeitern den Umsatz um sechs Prozent auf 732 Millionen €. Für heuer wird ein profitables Wachstum in allen drei Bereichen - Healthcare, Licht und Consumer Lifestyle - erwartet.1285WirtschaftsBlatt Verlag AGDocument WIRTSB0020081001e4a200010"
"93",2017-04-20,"newsbox.ch/ CREALOGIX erwirbt Machine Learning Technologie für Daten- und Videoanalytics","newsbox.ch/ CREALOGIX erwirbt Machine Learning Technologie für Daten- und Videoanalytics638 words4 September 201711:45Dow Jones Newswires GermanRTDJGEGermanCopyright © 2017, Dow Jones & Company, Inc.  Medienmitteilung  Zürich, 4. September 2017  CREALOGIX erwirbt Machine Learning Technologie für Daten- und Videoanalytics  CREALOGIX übernimmt die führende Artificial Intelligence (AI)-Technologie von Koemei. Die Lösung des Schweizer Startups und Spin-offs des Forschungsinstituts IDIAP (Partner der Eidgenössischen Technischen Hochschule Lausanne EPFL) ermöglicht dank Machine Learning die automatisierte Umsetzung von Audio- und Videoinhalten in Textdaten für Analytics und die Optimierung. Dadurch erleichtert sie insbesondere die Verwertung multimedialer Inhalte. Datenkategorien, die in Zukunft massiv wachsen und deren Analyse heute bei Unternehmen noch stark vernachlässigt werden, können so effizient genutzt werden. Eine umfangreiche Konzeptsuche sowie Benutzungsanalytics vereinfachen das Handling zusätzlich. Der Digital Banking und Digital Learning Spezialist ergänzt mit diesen innovativen AI-Funktionen sein bestehendes digitales Produkteportfolio.  Koemei entwickelte eine skalierbare Plattform im Umgang mit grossen multimedia Datensätzen, zur medienübergreifenden Suche und Analytics von Audio- und Videoinhalten. Ab Herbst 2017 wird die Technologie die Produktepalette des Anbieters für Digital Banking und Digital Learning Services bereichern. Mit dieser Integration können die ständig wachsenden Datenmengen für die Benutzer schnell verwendbar gemacht werden. Urs Widmer, CEO Digital Learning bei CREALOGIX: «Wir sind begeistert, dass wir die marktführende Plattform von Koemei erwerben konnten. Kombiniert mit unseren Digital Banking und Digital Learning Produkten, verhilft diese neue Technologie unseren Kunden zu Mehrwert und Wettbewerbsvorteilen.»  Machine Learning Software für optimale Datenanalyse  Die Lösung von Koemei erreicht eine der weltweit besten Erkennungsqualitäten bei der Inhaltsanalyse von Audibles, Podcasts und Videos. Die Inhalte werden hierfür über Automated Speech Recognition (ASR), also der automatisierten Spracherkennung, in Texte umgewandelt. Anschliessend werden mit Artificial Intelligence (AI) Tools und Natural Language Processing (NLP) die Texte analysiert und die Inhalte gemäss vordefinierten Taxonomien und Themen automatisch klassifiziert, wobei die Datensicherheit zu jeder Zeit gewährleistet ist. In den Videos werden spezifische Schlüsselbegriffe automatisch erkannt und mit Tags versehen. Nutzer können dank diesen die für sie relevanten Aussagen in jedem Video finden und direkt dort hinspringen. Dies steigert den Wert von Video- und Audio-Content enorm und spart Zeit bei der Recherche. Die Universität Genf hat bereits über 5'000 Stunden Vorlesungen mit der Technologie in benutzerfreundlicher Weise aufbereitet und diese nahtlos mit der Suche von Textdokumenten integriert. CREALOGIX passt die intelligenten Funktionen mit Konzeptsuche, Inhaltsanalytics und Statistiken den spezifischen Anforderungen seiner Kunden an.  ?ober CREALOGIX  Die CREALOGIX Gruppe ist ein unabhängiges Schweizer Softwarehaus und gehört als Fintech Top 100 Unternehmen zu den Marktführern im Digital Banking. CREALOGIX entwickelt und implementiert innovative Fintech-Lösungen für die digitale Bank von morgen. Mit den Lösungen von CREALOGIX antworten Banken auf die sich ändernden Kundenbedürfnisse im Bereich der Digitalisierung, um sich dadurch in einem extrem anspruchsvollen und dynamischen Markt zu behaupten und ihren Mitbewerbern stets einen Schritt voraus zu sein. Die 1996 gegründete Gruppe beschäftigt weltweit über 400 Mitarbeitende. Die Aktien der CREALOGIX Gruppe (CLXN) werden an der SIX Swiss Exchange gehandelt.  Die vorliegende Mitteilung enthält zukunftsgerichtete Aussagen, die mit gewissen Risiken, Unsicherheiten und Veränderungen behaftet sein können, die nicht voraussehbar sind und sich der Kontrolle der CREALOGIX Gruppe entziehen. CREALOGIX kann daher keine Zusicherungen machen bezüglich der Richtigkeit solcher zukunftsgerichteter Aussagen, deren Auswirkung auf die finanziellen Verhältnisse der CREALOGIX Gruppe oder den Markt, in dem Aktien und anderen Wertschriften der CREALOGIX Gruppe gehandelt werden.  Medienkontakt CREALOGIX AG  Jasmin Epp, Corporate Communications Officer  Gerne vereinbare ich einen Interviewtermin mit der Geschäftsführung von CREALOGIX für Sie. Bitte rufen Sie mich unter der Nummer +41 58 404 86 52 an oder senden Sie mir eine E-Mail: jasmin.epp@crealogix.com  Medienmitteilung (PDF)  +---------------------------++--------------++---------------------------------+ |Provider                   ||Channel       ||Contact                          | +---------------------------++--------------++---------------------------------+ |Tensid EQS Ltd.,           ||newsbox.ch    ||Provider/Channel related         | |Switzerland                ||www.newsbox.ch||enquiries[http://www.newsbox.ch||enquiries]                        | |www.tensid.ch[http://www.tensid.ch]              ||              ||marco@tensid.ch                  | |                           ||              ||+41 41 763 00 50                 | +---------------------------++--------------++---------------------------------+   (END) Dow Jones Newswires04-09-17 0945GMTDow Jones & Company, Inc.Document RTDJGE0020170904ed9400083"
"94",2013-04-03,"DJ newsbox.ch/ Spannende neue Produkte an der AudiologyNOW!","DJ newsbox.ch/ Spannende neue Produkte an der AudiologyNOW!1357 words3 April 201317:30Dow Jones Nachrichten auf DeutschRTDJGEGermanCopyright © 2013, Dow Jones & Company, Inc.    Medienmitteilung  Spannende neue Produkte an der AudiologyNOW!  Stäfa (Schweiz), 03.04.2013 - Am diesjährigen AudiologyNOW! Kongress in Anaheim, Kalifornien vom 3.-6. April 2013, wird Sonova eine Reihe neuer Produkte all ihrer Produktmarken vorstellen, welche das hohe Innovationstempo der Gruppe reflektieren. Mit der Lancierung von Phonak Audéo Q und Phonak Naída Q, wird Phonak seine Produktpalette basierend auf der Quest Plattform weiter ausbauen. Unitron wird sein neuestes Receiver-In-Canal (RIC) Hörgerät Moxi Kiss präsentieren und Advanced Bionics gibt die Zulassung in Europa und Kanada der neuen HiFocus Mid-Scala Elektrode1 und die weltweite Zulassung der neuen Innovation im Bereich Klangverarbeitung, HiRes Optima2 bekannt.  ""Wir sind begeistert von der umfassenden Palette an neuen Produkten, welche von all unseren Produktmarken am diesjährigen Kongress vorgestellt wird"", sagt Lukas Braunschweiler, Sonova CEO. ""Wir glauben dies widerspiegelt deutlich Sonova's Ziel, ständig die technologischen Grenzen zu testen und ihre Mission im Hörgerätemarkt als Innovationsführerin anerkannt zu werden. Wir sind überzeugt, dass die neuen Produkte unser Ziel unterstützen, unsere Marktanteile weiter ausbauen zu können und gleichzeitig die Profitabilität der Gruppe zu steigern"", ergänzt Braunschweiler.  Phonak  Phonak präsentiert sein neues Phonak Audéo Q und Phonak Naída Q Portfolio. Mit der Einführung der neuen Palette von RIC und Power Geräten erweitert Phonak seine erfolgreiche Quest Plattform um zwei neue Produktfamilien und bietet jetzt die passende Lösung für alle Grade von Hörverlust. Beide Produktfamilien basieren auf der im Herbst 2012 eingeführten Quest Plattform und sorgen mit der Binauralen VoiceStream Technologie für müheloses Verstehen auch in schwierigsten Hörsituationen. Phonak Audéo Q bietet eine erhöhte Spontanakzeptanz bei einem sehr klaren Sprachsignal, weniger Störgeräuschen und besserem Verstehen in noch mehr Hörumgebungen - und das selbst bei offener Anpassung. Das neue Tinnitus Balance Portfolio - erhältlich für Phonak Audéo Q - gibt Hörgeräteakustikern die wichtigsten Werkzeuge für ihr individuelles Therapieverfahren an die Hand. Das umfassende Power Portfolio Phonak Naída Q bietet maximale Leistung für Menschen mit starkem bis hochgradigem Hörverlust. Die neuen Produkte sind ab sofort im Markt erhältlich.  Zudem stellt Phonak mit Roger einen neuen digitalen Standard für besseres Sprachverstehen in Lärm und über Distanz vor. Roger sendet die Stimme des Sprechers direkt drahtlos zum Zuhörer. Die Leistungen aller derzeit erhältlichen FM-Systeme und äquivalenten digitalen Systeme werden von Roger übertroffen. So bietet es eine bahnbrechende und wissenschaftlich nachgewiesene3 Verbesserung des Signal-Rausch-Abstandes und die mühsame Frequenzverwaltung entfällt. Ab Juni 2013 ist Roger für Schule und Studium auf dem Markt erhältlich.  Unitron  Am 18. März hat Unitron Moxi Kiss vorgestellt, sein neuestes RIC Hörgerät, das durch sein Design die Kunden überzeugen wird.  Moxi Kiss ist das beste Beispiel für die Weiterentwicklung des Designs innerhalb der Moxi-Produktfamilie, mit dem Hauptaugenmerk auf dem Tragekomfort. Dank seinem erstklassigen Aussehen spricht Moxi Kiss Erstträger optisch an und bietet die perfekte Balance aus optimaler Passform, bester Verarbeitung und attraktiven Farben. Moxi Kiss zeichnet sich durch den natürlichen HiFi-Klang aus, der sich bereits bei der Moxi-Produktfamilie bewährt hat.  Moxi Kiss ist über das neue Flex:trial-Programm erhältlich, ein einzigartiges Produkt- und Dienstleistungskonzept von Unitron, das Hörgeräteakustikern die Möglichkeit bietet, Kunden völlig ohne Risiko die Vorteile einer Hörgeräteversorgung näherzubringen. Kunden nehmen nach dem ersten Termin ein auf sie massgeschneidertes Moxi Kiss Flex:trial-Gerät für einen Testzeitraum mit nach Hause, um die Vorteile des Hörgerätes in alltäglichen Situationen in der gewohnten Umgebung zu erleben. Die Vorteile, Moxi Kiss sofort und ohne Risiko oder Verpflichtung erleben zu können, wirken Verunsicherungen entgegen und erhöhen gleichzeitig die Lebensqualität der Kunden.  Advanced Bionics  Im Verlaufe der letzten zwei Monate gab AB die Zulassung durch den TÜV und Health Canada für die Elektrode HiFocus Mid-Scala1 bekannt. Das neue HiFocus-Mid-Scala-Array stellt die neueste Innovation im Elektrodenkonstruktionsdesign dar. Sie ist für die optimale Platzierung im mittleren Gang der Hörschnecke vorgesehen, um die dortigen empfindlichen Strukturen zu schützen. Mit dem kleinsten vorgeformten Array der Branche ist HiFocus Mid-Scala die einzige vorgeformte Elektrode, die für die neuesten Ansätze sanfter Chirurgie entwickelt wurde.  AB gab ausserdem die weltweite Zulassung von HiRes Optima2 bekannt, einer Innovation im Bereich Klangverarbeitung. Diese fortschrittliche Hörtechnologie führt zu einer optimierten Batterielebensdauer ohne Abstriche auf der grossartigen Leistung mit der von AB patentierten HiRes Fidelity 1202 Verarbeitung machen zu müssen. Cochlea-Implantate Träger, die diese neue Technologie nutzen, geniessen eine durchschnittliche Verbesserung der Batterielebensdauer von 55%, was es ihnen erlaubt, ihre Welt erheblich länger zu hören, ohne die Batterien wechseln zu müssen.  1 Noch nicht in den USA zugelassen.  2 Nicht zugelassen für pädiatrische Nutzung in den USA.  3 Comparison of speech recognition with adaptive digital and FM wireless technology by listeners who use hearing aids. Professor Linda Thibodeau PhD.  - Ende -  +---------+-----------------------------------+ |Kontakte |Thomas Bernhardsgrütter            | |         |                                   | |         |Investor Relations                 | |         |                                   | |         |Telefon +41 58 928 33 44           | |         |                                   | |         |Mobile +41 79 618 28 07            | |         |                                   | |         |Email                              | |         |thomas.bernhardsgruetter@sonova.com| |         |                                   | |         |                                   | |         |                                   | |         |Nicole Müller                      | |         |                                   | |         |Investor Relations                 | |         |                                   | |         |Telefon +41 58 928 33 22           | |         |Email nicole.mueller@sonova.com    | |         |                                   | |         |                                   | |         |                                   | |         |Michael Isaac                      | |         |                                   | |         |Media Relations                    | |         |                                   | |         |Telefon +41 58 928 33 24           | |         |                                   | |         |Mobile +41 79 420 29 56            | |         |                                   | |         |Email michael.isaac@sonova.com     | +---------+-----------------------------------+ |         |                                   | |         |                                   | |         |                                   | +---------+-----------------------------------+ |         |                                   | +---------+-----------------------------------+ Haftungsausschluss  Diese Medienmitteilung enthält Zukunftsaussagen, die keinerlei Garantie bezüglich der zukünftigen Leistung gewähren. Diese Aussagen widerspiegeln die Auffassung des Managements über zukünftige Ereignisse und Leistungen des Unternehmens zum gegenwärtigen Zeitpunkt. Die Aussagen beinhalten Risiken und Unsicherheiten, die sich aus - aber nicht abschliessend - zukünftigen globalen Wirtschaftsbedingungen, Devisenkursen, gesetzlichen Vorschriften, Marktbedingungen, Aktivitäten der Mitbewerber sowie anderen Faktoren, die ausserhalb Sonova's Kontrolle liegen, ergeben könnten. Sollten sich eines oder mehrere dieser Risiken oder Ungewissheiten realisieren oder sollte sich erweisen, dass die zugrunde liegenden Annahmen nicht korrekt waren, können die tatsächlichen Ergebnisse wesentlich von den prognostizierten oder erwarteten Ergebnissen abweichen. Jede vorausschauende Aussage gibt die Sicht zu dem Zeitpunkt wieder, zu dem sie gemacht wurde. Sonova übernimmt keine Verpflichtung gegenüber der Öffentlichkeit, vorausschauende Aussagen zu aktualisieren oder zu korrigieren.  Über Sonova  Die Sonova Holding AG mit Hauptsitz in Stäfa in der Schweiz ist der führende Hersteller von innovativen Lösungen rund um das Thema Hören. Die Gruppe ist im Markt durch ihre Kernmarken Phonak, Unitron, Advanced Bionics und Connect Hearing vertreten. Sonova bietet ihren Kunden eines der umfassendsten Produktportfolios in der Industrie - von Hörgeräten zu Cochlea Implantaten bis hin zu drahtlosen Kommunikationslösungen. Gegründet 1947 ist die Gruppe heute weltweit in über 90 Ländern vertreten und beschäftigt mehr als 8'500 engagierte Mitarbeitende. Sonova erzielte im Geschäftsjahr 2011/12 einen Umsatz von CHF 1,62 Mrd. sowie einen Reingewinn von CHF 246 Mio. Durch die Unterstützung der Hear the World Foundation verfolgt Sonova das Ziel, eine Welt zu schaffen, in der jeder Mensch die Freude des Hörens und damit ein Leben ohne Einschränkungen geniessen kann.  DJ newsbox.ch/ Spannende neue Produkte an der -2-Für weitere Informationen besuchen Sie bitte www.sonova.com[http://www.sonova.com] und www.hear-the-world.com[http://www.hear-the-world.com].  Sonova Aktien (Ticker-Symbol: SOON, Valoren-Nr. 1254978, ISIN: CH0012549785) sind seit 1994 an der SIX Swiss Exchange kotiert. The securities of Sonova have not been and will not be registered under the U.S. Securities Act of 1933, as amended (the ""U.S. Securities Act""), or under the applicable securities laws of any state of the United States of America, and may not be offered or sold in the United States of America except pursuant to an exemption from the registration requirements under the U.S. Securities Act and in compliance with applicable state securities laws, or outside the United States of America to non-U.S. Persons in reliance on Regulation S under the U.S. Securities Act.  Sonova Medienmitteilung (PDF) Phonak - Medienmitteilung Phonak Audéo Q und Phonak Naída Q (PDF) Phonak - Medienmitteilung Roger for Education (PDF) Unitron - Medienmitteilung Moxi Kiss (PDF) AB - Medienmitteilung HiFocus Mid-Scala Electrode (PDF/english only) AB - Medienmitteilung HiRes Optima sound processing (PDF/english only)  +-------------++--------------++----------------+ |Provider     ||Channel       ||Contact         | +-------------++--------------++----------------+ |Tensid Ltd., ||newsbox.ch    ||Provider/Channel| |Switzerland  ||www.newsbox.ch||related[http://www.newsbox.ch||related]         | |www.tensid.ch[http://www.tensid.ch]||              ||enquiries       | |             ||              ||marco@tensid.ch | |             ||              ||+41 41 763 00 50| +-------------++--------------++----------------+ |             ||              ||                | +-------------++--------------++----------------+   (END) Dow Jones Newswires03-04-13 1530GMTDow Jones & Company, Inc.Document RTDJGE0020130403e943000h9"
"95",2017-09-13,"NTT Com startet globale Einführung seiner Enterprise Cloud WebRTC-Plattform ; Ermöglicht den Einsatz von Echtzeitkommunikationsfunktionen in Anwendungen und auf Websites am selben Tag","NTT Com startet globale Einführung seiner Enterprise Cloud WebRTC-Plattform ; Ermöglicht den Einsatz von Echtzeitkommunikationsfunktionen in Anwendungen und auf Websites am selben Tag897 words13 September 201708:02Business WireBWRGERGerman(c) 2017 Business Wire.  All Rights Reserved. TOKIO - (BUSINESS WIRE) - NTT Communications Corporation (NTT Com), das Unternehmen für ICT-Lösungen und internationale Kommunikation innerhalb der NTT Group (TOKYO:9432), gab heute die globale Einführung seiner Enterprise Cloud WebRTC-Plattform (ECL WebRTC) bekannt, einem Dienst, mit dem die Einführung von Echtzeitkommunikationsfunktionen, wie z. B. Sprach- und Videokommunikation sowie Datenübertragung für Smartphone-, Tablet- und Webanwendungen am selben Tag möglich ist.Diese Pressemitteilung enthält multimediale Inhalte. Die vollständige Mitteilung hier ansehen: http://www.businesswire.com/news/home/20170912007006/de/[http://www.businesswire.com/news/home/20170912007006/de/]Figure (Graphic: Business Wire)Die ECL WebRTC-Plattform bietet die folgenden Vorteile:* Software Development Kits (SDKs) für Video, Sprach- und Datenübertragung; der Dienst stellt vier SDK-Typen zur Verfügung, die mit Webbrowsern, iOS, Android oder IoT kompatibel sind.* Durch die Ausstattung von Anwendungen und Websites mit ECL WebRTC können Kunden Echtzeitkommunikationsfunktionen, wie z. B. Video- und Audio-Konferenzen mit mehreren Teilnehmern sowie Datenübertragung, in kürzester Zeit einsetzen.* NTT Com stellt die gesamte für WebRTC erforderliche Serverinfrastruktur bereit und betreibt diese; die Kunden können sich ganz auf die Entwicklung und Ausführung ihrer eigenen Dienste konzentrieren.* Der Dienst wird Unternehmen im Rahmen eines Abonnements mit umfassendem technischen Support und einer Dienstleistungsvereinbarung, die eine Verfügbarkeit von 99,99 % vorsieht, zur Verfügung gestellt; Start-ups und Entwickler können sich online für kostenfreie Tarifmodelle, die eine begrenzte Nutzung ermöglichen, anmelden (siehe Anhang für weitere Informationen).HintergrundECL WebRTC ist ein NTT Com-Dienst, der die für die Nutzung von Ressourcen zur Echtzeit-Webkommunikation (Web Real-Time Communication – WebRTC) erforderlichen Server sowie die standardisierte Technologie für die Realisierung der Echtzeitkommunikation betreibt und das Software Development Kit (SDK) bereitstellt. WebRTC ist eine Technologie, die es Webbrowsern sowie iOS-, Android- oder IoT-Geräten gestattet, Medien in Echtzeit zu versenden und zu empfangen, eine Funktionalität, die aufgrund der Zunahme von Telearbeit, Telefonkonferenzen und Fernlernen sowie der Expansion von Callcentern und Onlineübersetzung zunehmend stärker gefragt ist. NTT Com startete 2013 SkyWay, die erste WebRTC-Plattform in Japan, die gegenwärtig von ca. 5.000 Entwicklern, 8.000 Anwendungen und 300.000 Anwendern eingesetzt wird. Um den sich weiterentwickelnden Bedürfnissen des Markts gerecht zu werden, hat NTT Com nunmehr mit ECL WebRTC einen umfassenden, kommerziellen Dienst eingeführt, der auf den in SkyWay eingesetzten Technologien basiert.Offizielle Website der Enterprise Cloud WebRTC-Plattformhttps://webrtc.ecl.ntt.com/en/[http://cts.businesswire.com/ct/CT?id=smartlink&url=https%3A%2F%2Fwebrtc.ecl.ntt.com%2Fen%2F&esheet=51679857&newsitemid=20170912007006&lan=de-DE&anchor=https%3A%2F%2Fwebrtc.ecl.ntt.com%2Fen%2F&index=1&md5=c9a984b347548d193d14019915161ce4]NTT Communications to Release Trial of SkyWay WebRTC Platform with TURN Feature (NTT Communications startet Testphase der SkyWay WebRTC-Plattform mit TURN-Funktion – Pressemitteilung vom 27. Januar 2015):http://www.ntt.com/en/about-us/press-releases/news/article/2015/20150127.html[http://www.ntt.com/en/about-us/press-releases/news/article/2015/20150127.html]NTT Communications Offers World’s 1st Speech-Recognition API for Multiple Browsers at No Charge (NTT Communications bietet kostenfrei die weltweit erste Schnittstelle zur Spracherkennung – Pressemitteilung vom 28. Juli 2015):http://www.ntt.com/en/about-us/press-releases/news/article/2015/20150728.html[http://www.ntt.com/en/about-us/press-releases/news/article/2015/20150728.html]Über die NTT Communications CorporationNTT Communications bietet Beratungs-, Architektur-, Sicherheits- und Cloud-Dienstleistungen zur Optimierung der IKT-Umgebungen in Unternehmen an. Unterstützt werden diese Angebote durch die weltweite Infrastruktur des Unternehmens, darunter das führende globale Tier-1-IP-Netz, das VPN-Netz Arcstar Universal One™, das sich über 196 Länder und Regionen erstreckt, sowie 140 sichere Rechenzentren weltweit. Die Lösungen von NTT Communications werden durch die globalen Ressourcen der Unternehmen der NTT-Gruppe gestützt, zu denen unter anderem Dimension Data, NTT DOCOMO und NTT DATA gehören.www.ntt.com | Twitter@NTT Com[http://cts.businesswire.com/ct/CT?id=smartlink&url=http%3A%2F%2Fwww.twitter.com%2Fnttcom&esheet=51679857&newsitemid=20170912007006&lan=de-DE&anchor=Twitter%40NTT+Com&index=5&md5=6cfcf2ee2b38747327c9f71d0776cb7c] | Facebook@NTT Com[http://cts.businesswire.com/ct/CT?id=smartlink&url=http%3A%2F%2Fwww.facebook.com%2Fnttcomtv&esheet=51679857&newsitemid=20170912007006&lan=de-DE&anchor=Facebook%40NTT+Com&index=6&md5=fc2ab1cafab4531ac4c582cff7074327] | LinkedIn@NTT Com[http://cts.businesswire.com/ct/CT?id=smartlink&url=http%3A%2F%2Fwww.linkedin.com%2Fcompany%2Fntt-communications&esheet=51679857&newsitemid=20170912007006&lan=de-DE&anchor=LinkedIn%40NTT+Com&index=7&md5=38a16088a9a3f80fb1b8355940be2209]Anhang* Funktionen der ECL WebRTC (SDK/Server API)SDK: JavaScript iOS Android IoT ß-VersionServerschnittstelle (Server API): Signaling, STUN, TURN, SFU (Optionale Dienste)Signaling: tauscht Informationen, einschließlich der IP-Adresse und Port-Nummern aus, die für die Verbindung zwischen Geräten notwendig sind.STUN: ruft die globale IP-Adresse und Port-Nummern des Geräts ab.TURN: ermöglicht die Nutzung von WebRTC in Netzwerkumgebungen, in denen P2P-Kommunikation nicht möglich ist, wie z. B. Unternehmensnetzwerke, via Datenübertragung.SFU: Ein Medienservertyp, der von einem Gerät empfangende Daten an andere Geräte weiterleitet und die Kommunikation zwischen einer größeren Zahl von Personen ermöglicht. SFU reduziert den Verschlüsselungsumfang auf dem Gerät und der Bandbreite sowie die Menge der gesendeten Daten, wenn die Kommunikation mehr als drei Personen einschließt.IoT SDK ß-Version: SDK für die Entwicklung von Linux/Raspberry Pi-Anwendungen. TarifmodelleKategorie                            Posten                                                                                                   Kostenpflichtige Tarife                                                 Kostenfreie TarifePreis                                Basic                                                                                                    2,480 JPY pro Min., mit einem Höchstbetrag von 100.000 JPY pro Monat*   -                        Signaling                         Kostenfrei für maximal eine Million Vorgänge, anschließend 100.000             JPY                                                                       -                        TURN/SFU                          40 JPY/GB pro einem In/Out                                                                                                                                -Verwendungsbeschränkung              Signaling-Häufigkeit                                                                                     -                                                                       Bis zu 500.000 Mal pro Monat                        TURN-Volumen                      -                                                                                                                                                         500 GB pro Monat                        SFU-Volumen                       -                                                                                                                                                         500 GB pro MonatSLA/Support                          SLA-Verfügbarkeit                                                                                        99,99 %                                                                 -                        Support                           Ticket-Support und Entwickler-Community                                                                                                                   Community nur für Entwickler* Die Basic-Gebühr wird für jeden Mieter berechnet. Kunden können             für jeden Mieter mehr als einen API-Schlüssel für WebRTC erstellen.Die Ausgangssprache, in der der Originaltext veröffentlicht wird, ist die offizielle und autorisierte Version. Übersetzungen werden zur besseren Verständigung mitgeliefert. Nur die Sprachversion, die im Original veröffentlicht wurde, ist rechtsgültig. Gleichen Sie deshalb Übersetzungen mit der originalen Sprachversion der Veröffentlichung ab.Originalversion auf businesswire.com ansehen:http://www.businesswire.com/news/home/20170912007006/de/[http://www.businesswire.com/news/home/20170912007006/de/]Medienkontakt NTT Communications Hr. Yasumasa Morita /       Hr. Masayuki Hayashi / Fr.. Yuko Tomita, +81 3 6733 9521 Cloud       Services Sales Division g-cl@ntt.com[mailto:g-cl@ntt.com] Business Wire, Inc.Document BWRGER0020170913ed9d000dx"
"96",2001-01-20,"Another step in the SMD miniaturization - the tiny 01005-device - Additional requirements ahead.","Another step in the SMD miniaturization - the tiny 01005-device - Additional requirements ahead.Von Dr. Gerhard Haas, Hubert Herzberg, Siemens Dematic.1994 words1 November 2001epp Elektronik Produktion & PrüftechnikEPP14German(c) 2001 Konradin Verlag.All Rights Reserved. For further information see http://www.epp-online.de161 Siemens Dematic AG Postfach: 70 00 75 D-81375 MünchenThe use of the 0201-part has just been successfully evaluated, and integrated into the first products. Consideration is already being given as to whether 01005-components will have to be used in order to facilitate the next stage of miniaturization. Thought is of course also being given to alternative technologies such as the integration of passive devices which are also competitive in this field. Depending upon application, alternatives will find their way into products.These considerations may appear premature in view of the fact that most products are still being equipped with 0402sor larger components despitethe successful introduction of the 0201-package by Siplace. However, progress in the miniaturization of products up to now has shown that the de-velopment always proceeds faster than initially forecasted. The introduction of component formats and packaging technologies is mainly being driven by two key factors: miniaturization of the product and the costs of material and processing. The RF behavior is a third factor that is becoming increasingly more important for many electronics gear.The 01005-format helps to save real estate on the board in comparison to the 0201-parts by a factor of around two (figure 1). Each miniaturization step is inevitably accompanied by sometimes essential improvements and modifications to the production process. These could not only represent major expenditure on manufacturing equipment, but also present major challenges to the process as in this case. This is sufficient reason to give timely thought to the modifications required and to take the appropriate action now.Influence on PCBand paste printThe miniature dimensions of the tiny 01005-components might prevent the traces from being routed under the components as done previously. In this case they might have to pass around the components. This would substantially diminish the effective miniaturization of the PCB. The use of non-soldermask defined pads is absolutely essential for such fine structures in order to achieve highly reliable solder joints; otherwise, there is an increased risk of cracks in the solder.In order to keep dpm rates as low as possible, the solder paste must not flow off the pad on to the incoming trace. This maybe achieved by narrowing the corresponding trace close to the pad. In general,the use of 01005s calls for an even more consistent pad and trace design over the entire PCB than been the case for 0201-components.The so-called area ratio - the ratio of the aperture area to the area of the sidewalls - becomes even more unfavorable in 01005-processing, and the risk of blocked apertures in the mask increases substantial-ly. The need to use even thinner stencilsmay then lead to a lack of solder pastefor other components. In general, thewalls of the mask apertures must be very smooth to achieve the maximum possi-ble release of the solder paste onto the board. Corresponding changes will have to be made to the solder pastes to be used, as this has already been the case for the 0201s.Demands on placementThe terminal pad pitch for the 01005-format is comparable to that required for 0.3mm QFPs. This comparison illustrates the process-related challenges involved with the component placement. The maximum acceptable deviation in the case of a terminal pad layout optimized for placement would be around 70µm/4s Figure 1 shows the terminal pad layout, and figure 2 provides additional information about the requirements which have to be considered when dimensioning the pad layout. The layout restrictions imposed by reduced self-alignment, above all in direction of the component length, might increase the demand on placement accuracy even more, pushing it in the range of 50 to 60µm.With the introduction of lead-free, deterioration of the wetting behavior of the pad and the joints also reduces the self-centering effect during soldering. Due to the small geometric dimensions of the components, optical centering also places higher demands on the vision system, and high-resolution cameras will become necessary. The dimensional tolerances of components and the solder joints means that appropriate algorithms will have to be adapted or developed.The compactness of this component, coupled with the need to take these parts in a defined and reliable manner out of the tape, mean that pick-up accuracies substantially less than 100µm have to be guaranteed. But that is not all. The space between individual components should be less than 150µm in order to maximize packing density and thus to achieve the desired reduction in PCB dimensions. The nozzle contours should not be bigger than the components themselves, to prevent touching adjacent components. This necessitates very narrow nozzles with a low suction force.Based on the data in figures 1 and 2, and a placement accuracy of 70µm, the maximum permissible pick-up deviation is 60µm/s. This, therefore, also requires consistently precise delivery of the components. A great deal will depend on the amount of tape tolerances. Given the corresponding tolerance adaptation, high demands are also placed on the component feeders. As well as requiring a highly accurate tape feeder design, it will also be necessary to check the exact pick-up position regularly in order to eliminate the remaining tape tolerances as far as possible. For example, in this case it would be beneficial to measure the pick-up position after setting up the machines. In addition, continuous correction of the pick-up position (on-line learning) with the aid of the results of the optical centering is a proven method here.Oven and rework issuesAs was clear in the case of 0201, it is very important to set the temperature profile according to the specifications of the solder paste. To give just one example here: a too short soak time at too low a temperature. This applies even more to 01005-components. The defect rate would be even higher as they have even less weight, for example, in the case of tombstoning or billboarding - one end of the component lifting during soldering. The small size of the component makes visual inspection considerably more difficult. Rework becomes ever more delicate as the tips of the soldering tool wear or become damaged more quickly. Here again, the operator needs to possess extreme dexterity.Competing technologiesNow we will have a look at considerations which might count against using 01005-parts. There are three main technologies which might gradually replace discrete passive components. First solutions for integrated passive functions are already available for selected products, but so far only in restricted quantities.The first method involves arrays containing some passive functions of one type (either resistors or capacitors) integrated in a SMD. These are already being used frequently as bus terminations. The second solution is based on integral passive devices, which can combine active and passive functions. For example, active bus terminating networks are currently available. The third type relates to embedded passive components that can be found in a multilayer substrate (FR4 or LTCC). They are already in use in Bluetooth or VCO (voltage controlled oscillator) modules. All these solutions have a greater potential in respect of functional density than discrete passive components. However, the critical factor is the turnover time. This brings into question the use of products with short product life cycles and high rates of change.Less impact on miniaturization of passive chip componentsHand-held products normally have a keypad and a display. The keypad might be replaced by speech-recognition technology day. However, the display will tend to increase in size because of the trend of providing more visual information. This supports the assumption that there will be no further reduction in the overall size of these products. Reducing the thickness of the modules remains an attractive option, but the 01005-format will not make any significant contribution to this. Embedded passive components present a much more promising solution.The tendency towards accommodating more functionality within the same area will only be achieved to a minor extent by passive components, as functionality is primarily containedin the integrated circuits. Thus, we cannot expect any significant contribution to reducing size here.Hand-held products are subject to ever-increasing cost pressure. However, 01005-components will not become cheaper than those in 0201-format. Apart from the cost of the components themselves, this is primarily due to the extra expenditure required in the production process. About 80% of the total cost of passive components is accounted for by their processing. This ratio will be even more unfavorable for the 01005-format, for which 85% is not an unrealistic estimate. Consequently, some of the costs saved by reducing the size of the substrate area will be lost again. The use of clock frequencies close to or above 1GHz requires well-defined, low-inductance design and active bus termination. Here again, integrated or embedded passive components respectively present a better solution.An outlookThe prospect of using 01005-devices depends on the extent to which these contribute towards achieving reduced material and manufacturing costs in portable products. If the market does not demand any further miniaturization, and if combining several functions in one new product, which are currently offered separately, does not justify the use of 01005-components, then this format will either remain unused or be restricted to small single niches. As a result, the price of them will not come down to gain cost advantages.The component market is not exhibiting the usual active business interest that was, for example, shown in advance of the introduction of the 0402s and 0201s. In this case, a wait-and-see attitude is evident regarding the penetration of the 0201. Nevertheless, if the market sees benefits from the use of 01005s, then a breakthrough will happen, placing even higher requirements on the process equipment. The effects on the placement process will involve investment in the development of equipment. The crucial factor here will be the integration of the solution into the production in a cost-optimized and secure manner, and to safeguard the process.www.siplace.com[http://www.siplace.com/]EPP 161ZusammenfassungDie Miniaturisierung der Elektronikprodukte schreitet ungebrochen weiter. Bei portablen Geräten ist das, solange man sie noch sinnvoll bedienen und auf die vielen implementierten Funktionen dann auch zugreifen kann, sicher ein Wettbewerbsvorteil. Verharrte die Diskussion bis vor kurzem noch bei den bereits sowieso schon winzigen 0201-Komponenten, so ist nun der Fokus der weiteren Entwicklung auf die 01005-Baueile gerichtet. Zwar ist weder deren Verwendung noch jene von 0201 bisher ein Mainstream geworden, doch ist es nun angebracht, die Situation bei der weiteren Minimierung der Bauteilabmessungen zu skizzieren.RésuméLa miniaturisation des produits électroniques progresse sans cesse et représente sans aucun doute un avantage sur la concurrence dans le cas des portables, tant que ceux-ci restent agréables à utiliser et qu'il reste possible d'accéder aux nombreuses fonctions proposées. Alors qu'il y a encore peu de temps, on s'intéressait aux composants 0201, déjà minuscules, ce sont maintenant les composants 01005 qui concentrent sur eux tous les efforts. Certes, leur utilisation, de même que celle des 0201, ne représente pas encore un courant général, mais il convient désormais d'esquisser la situation dans le cadre de la miniaturisation continue.SommarioLa miniaturizzazione dei prodotti elettronici procede senza pause. Per gli apparecchi portatili questo sviluppo é sicuramente un elemento concorrenziale molto importante, sempre che essi possano essere ancora usati in maniera appropriata e che si possa accedere a tutte le funzioni in essi implementate. Mentre sino a poco tempo fa la tematica era incentrata sui piccolissimi componenti 0201, ora lo sviluppo é concentrato sui componenti di tipo 01005. Anche se né la loro applicazione né quella dei 0201 sia diventata un Mainstream, é indicato schizzare brevemente la situazione attuale nella minimizzazione dei componenti.Document epp0000020020308dxb10009o"
"97",2012-11-20,"Das Thema: An der Schnittstelle von Wirtschaft und Forschung - letzter Teil unserer Serie; Auch mit italienischem Akzent geht's nach Alsdorf; Spracherkennung ist eine hoch komplexe Wissenschaft. In Aachen wird intensiv darüber geforscht. Das hat ganz praktischen Nutzen, nicht nur im Auto.","Wirtschaft TitelDas Thema: An der Schnittstelle von Wirtschaft und Forschung - letzter Teil unserer Serie; Auch mit italienischem Akzent geht's nach Alsdorf; Spracherkennung ist eine hoch komplexe Wissenschaft. In Aachen wird intensiv darüber geforscht. Das hat ganz praktischen Nutzen, nicht nur im Auto.811 words11 September 2012Aachener NachrichtenAACHE21GermanCopyright (2012) Zeitungsverlag Aachen GmbH (ZVA).   Von Udo FoersterAachen. »Bitte nennen Sie Ihr Ziel«, fordert die weibliche Stimme des Navigationsgeräts. »Alsdorf«, lautet die Antwort. »Bitte wiederholen Sie die Eingabe!«, insistiert die virtuelle junge Dame. »Alsdorf!«. Auch diesmal funktioniert es nicht. »Bitte wiederholen Sie die Eingabe!« Genervt lautet die Antwort nun »Eschweiler« - und das System reagiert: »Fahren Sie . . .«Das große MissverständnisDiese Episode lässt nicht unbedingt darauf schließen, dass Navigationsmaschinen Vorlieben für bestimmte Städte in der Region haben. Die Ursachen für das Missverständnis können vielmehr mannigfaltig sein. Vielleicht hat sich der Sprecher undeutlich ausgedrückt und das System konnte ein eingeübtes Sprachmuster der Stimme nicht eindeutig zuordnen. Auch Nebengeräusche führen zur informationstechnischen Irritation. »Wenn Systeme dieser Art nicht optimale Ergebnisse liefern, liegt es in vielen Fällen an der Erfassung der Stimme über die vorhandene Mikrofontechnik«, sagt Professor Hermann Ney. Er ist Inhaber des Lehrstuhls für Informatik 6 - Sprachverarbeitung und Mustererkennung an der RWTH Aachen. Das Institut erforscht die Spracherkennung über die »gesprochene« Sprache, die Übersetzung von »geschriebener« Sprache sowie die Schrifterkennung (»fotografierte Sprache«). Auch das Erforschen automatisierter Systeme zum Erkennen von Elementen der Gebärdensprache und von Bildern gehört zum Aufgabenbereich.Bereits seit den 60er Jahren beschäftigen sich Wissenschaftler mit dem Thema Spracherkennung. Die Gründe, die es für einen Menschen schwierig machen, eine Sprache zu erlernen, gelten in ähnlicher Form für den Computer: Bei dem Versuch, eine fremde Sprache zu lernen, wird niemand zum kompetenten Sprecher und Hörer, indem er nur Vokabeln, Aussprache und grammatische Regeln kennt. Es ist viel besser, praktische Erfahrungen beim Zuhören, Sprechen und Lesen zu sammeln. Dementsprechend geht auch der Ansatz zur automatischen Spracherkennung davon aus, dass der Computer aus Beispieldaten lernen muss und mit Plausibilitätsbewertungen statt mit vorgegebenen kategorischen Regeln arbeitet. Dies geschieht mit Verfahren der Informationstheorie und der statistischen Mustererkennung. »Insgesamt handelt es sich um einen ganzheitlichen Ansatz, bei dem nicht einzelne Merkmale oder Laute isoliert verarbeitet werden, sondern der gesprochene Satz als eine Einheit betrachtet und in seiner Gesamtheit vom Computer erfasst werden muss«, sagt Ney. Die von ihm und seinen Mitarbeitern entwickelte Software lernt durch den Vergleich vieler Beispieltexte immer mehr dazu und wählt die wahrscheinlichste Übersetzung aus. »Je mehr Datensätze wir verarbeiten beziehungsweise vergleichen können, umso genauer sind die Ergebnisse«, ergänzt Volker Steinbiss, ein enger Mitarbeiter Neys, der wie der Professor auf dem Gebiet der Spracherkennung erste Berufserfahrungen im Aachener Forschungslabor des Philips-Konzerns sammelte.1993 war Ney als Professor an die RWTH Aachen gewechselt, um die Leitung des neu gegründeten Informatiklehrstuhls 6 zu übernehmen. Heute zählt das Aachener Team zu den weltweit führenden Experten auf diesem Fachgebiet. Bedeutende internationale Forschungsprojekte wie die deutsch-französische Initiative Quaero oder zur Entwicklung leistungsfähiger Sprachübersetzungssysteme für das EU-Parlament werden von den Aachener Forschern nicht nur wissenschaftlich koordiniert. Für deren administrative Steuerung zeichnet Steinbiss verantwortlich, der zu diesem Zweck ein eigenes Unternehmen gegründet hat. Forschungsschwerpunkte liegen darüber hinaus im Erkennen und Übersetzen der arabischen und chinesischen Sprache in ihren amtlichen und den Medien verwendeten Versionen.Wegen des exzellenten Rufs des Instituts sind Absolventen des Lehrstuhls in vielen großen Unternehmen weltweit willkommen. Franz Josef Och, einer der ersten Doktoranden, ist heute Leiter der Abteilung Softwareentwicklung bei Google, die den weltweiten Einsatz von Sprachübersetzungssystemen bearbeitet. Auch bei IBM sowie beim US-Konzern Nuance Communications sind zahlreiche Absolventen beschäftigt. Diese allerdings sind überwiegend in Aachen tätig, was einen besonderen Hintergrund hat. 2008 wurde das Philips Forschungslabor mit dem Schwerpunkt Spracherkennung als damalige Unternehmenssparte »Speech Recognition Systems« an Nuance - zunächst unter dem Namen Scansoft - verkauft.»Rund 100 Mitarbeiter - überwiegend Forscher und Entwickler - arbeiten heute an unserem Standort in Aachen«, sagt Nuance-Marketingleiterin Fatima Vital. Im Auftrag des milliardenschweren Konzerns - Umsatz 2011: 1,4 Milliarden Dollar - entwickeln sie Lösungen für den weltweiten Einsatz in Sachen Sprach- und Bildverarbeitungslösungen. Ein aktuelles Projekt wurde in Zusammenarbeit mit einem süddeutschen Unternehmen realisiert.E-Mails, die nicht ablenkenFür Neu-Fahrzeuge von BMW haben die in Aachen tätigen Informatiker und Ingenieure an einer hoch innovativen so genannten Messaging-Lösung gearbeitet. Dabei handelt es sich um einen vollständig integrierten mobilen Assistenten, mit dem der Fahrer Nachrichten und E-Mails allein durch Sprache eingeben und bearbeiten kann. Empfangene Nachrichten werden vorgelesen und können beantwortet werden, ohne dass man dazu die Hände vom Steuer nehmen oder den Blick von der Straße abwenden muss. Der Fahrer kann E-Mails mit einfachen Sprachbefehlen formatieren und neue Zeilen, Absätze, Satzzeichen und andere Formatierungen einfügen. Erhältlich ist das neue System in bestimmten Modellen als Teil der nächsten Generation des BMW-Navigationssystems. Es ist für die Anwendung in sechs Sprachen verfügbar: Deutsch, amerikanisches und britisches Englisch, Französisch, Italienisch und Spanisch. Damit sollte es ein Leichtes sein, auch mit italienischem Akzent den Weg nach Alsdorf zu finden.Zeitungsverlag Aachen GmbHDocument AACHE00020120911e89b0003b"
"98",2001-02-20,"Mein Computer versteht mich nicht.","Mein Computer versteht mich nicht.1577 words2 February 2001Neue Zürcher ZeitungNEUZZGerman(c) 2001Probleme mit der automatischen SpracherkennungComputer der Zukunft werden stets als sprechende und die gesprochene Sprache verstehende Maschinen porträtiert. Seit vielen Jahren wird der unmittelbar bevorstehende kommerzielle Durchbruch von Spracherkennungssoftware prophezeit. Verständige Computer sind aber nach wie vor Fiktion, in der Realität - etwa bei der belgischen Lernout & Hauspie - beschäftigt das Thema derzeit vor allem die Konkursverwalter.«Öffne die Tür, HAL!» - «Entschuldigung Dave, aber ich kann das nicht.» - «Wo liegt das Problem, HAL?» - «Ich denke, du weisst genauso gut wie ich, was das Problem ist.» Der Dialog zwischen dem Astronauten Dave und dem sprechenden und Sprache verstehenden Computer HAL 9000 ist eine der Schlüsselszenen im Film «2001, Odyssee im Weltraum». Wenig später versucht HAL, den Astronauten zu töten, der nach dem Dialog endgültig Gewissheit darüber hatte, «... was das Problem ist»: im Metatext seiner Antwort teilte der Computer mit, dass der Mensch das Problem sei und dass beide dies wissen.Als Stanley Kubrick und der Science-Fiction-Schreiber Arthur C. Clarke im Jahre 1966 an dieser Szene feilten, gingen sie selbstverständlich davon aus, dass Computer im Jahre 2001 im besten Englisch sprechen und weit mehr als einfache Befehle verstehen können. «Das Sprechen von HAL empfand ich als triviale Technik wie das Steuern von Raumschiffen durch den Computer», erklärte Clarke 1996 zum Jubiläum des Filmes, «nur beim schachspielenden Computer hatte ich meine Zweifel.» Heute spielen Computer glänzend Schach, ein Spezialcomputer von IBM konnte gar den Weltmeister Kasparow schlagen. Doch mit dem Verstehen von Sprache hapert es.Tauber KlotzDer Computerwissenschafter Ray Kurzweil, einer der Pioniere der Spracherkennung, brachte es zum besagten Jubiläum auf den Punkt: «Computer werden 1997 (dem fiktiven Geburtsjahr von HAL) unsere Sprache nicht verstehen können, und sie werden es auch 2001 nicht schaffen», behauptete er im Sammelband «HAL's Legacy: 2001's Computer as Dream and Reality».* Das Hintergrundwissen um die Sprache, die Metasprache, bleibe dem Computer verschlossen, so gut sein Grammatik-Parser auch funktioniere und so umfassend seine Wörterbücher auch trainiert seien.Ray Kurzweil hat Recht behalten. Die Computer verstehen uns noch immer nicht. Bis man mit Computern sprechen könne wie mit Menschen, werde es noch vier bis sechs Jahre dauern, sagen Experten. Das haben sie vor vier oder sechs Jahren auch schon gesagt.Zugangsbarrieren niederreissenDas hindert freilich niemanden daran, von den Wundern der Spracherkennung zu schwärmen und Software aller Art anzubieten, die den «natürlichen Umgang» mit dem Computer gestattet. Dergleichen hat Tradition: Im Jahr 2000 wurde Spracherkennung von der Zeitung «Computer Telephony» zum «Softwareparadigma des neuen Jahrhunderts» gewählt. Das Jahr 1998 brachte es schon einmal zum Ehrentitel als «Jahr der Spracherkennung», und von 1997 bis 1999 schafften es die Sprachprodukte Via Voice, Naturally Speaking und Dragon Dictate hintereinander, Innovationspreise des renommierten amerikanischen «PC Magazine» zu bekommen.«In zehn Jahren wird jeder mit seinem Computer, mit seinem Auto und mit den Geräten in seinem Haushalt reden. Jeder», erklärte der Franzose Gaston Bastiaens Ende 1999 in einem Interview, als er mit der belgischen Firma Lernout & Hauspie (L&H) durch geschickte Aufkäufe gerade Marktführer geworden war. «1999 ist das Schicksalsjahr von Telefon und Spracherkennung als neuer Benutzerschnittstelle zu E-Business und E-Commerce», radebrechte eine Presseerklärung der IBM zum hauseigenen Via-Voice-System, die mit der wenig deutsch klingenden Überschrift «That's Speech Recognition» verschickt wurde. Kein Geringerer als Bill Gates, dessen Microsoft zeitweise 20 Prozent von L&H gehörten, schwärmte schon 1998 in einer Rede auf der Computermesse Comdex von der Sprache als «natural interface» zum Computer, die alle Zugangsbarrieren niederreissen werde. «Millionen von Menschen, die sich nie vor einen Computer mit Tastatur setzen würden, werden ganz natürlich mit ihrem Rechner plaudern, ihm die Frage nach einer Flugverbindung stellen oder ihr Tagebuch sprechen.»Die Gewöhnung an die rudimentären Fähigkeiten der automatischen Spracherkennung der Call-Center ist weit fortgeschritten, auch Mobiltelefone, die beim Ruf eines Namens die Nummer der Person wählen, sind nicht mehr ungewöhnlich. Dennoch: Die Computer verstehen uns noch immer nicht. Einigermassen brauchbare Resultate liefert Spracherkennung bisher nur bei Anwendungen, bei denen der Satzbau - wie bei Fachsprachen - stark formalisiert ist oder bei denen nur ein sehr kleines Vokabular zum Einsatz kommt.Wo ist also - um mit HAL zu sprechen - das Problem? Menschen nuscheln, hüsteln, verschlucken Buchstaben oder verschmelzen Wörter, Menschen sind manchmal aufgeregt, manchmal müde und sprechen ein und dasselbe Wort kaum zweimal genau gleich aus. Es gibt Softwares, die sich an die Sprechweise eines bestimmten Benutzers gewöhnen können. Das ist aber sehr aufwendig. Zwischen 30 Minuten und zwei Stunden werden dafür benötigt, anschliessend rechnet auch der schnellste Rechner eine Stunde lang das sogenannte Sprachmodell zusammen. Das einmal gefundene Sprachmodell muss vom Sprechenden kontinuierlich weiter gepflegt werden. Es kann aber auch verhunzt werden: Zwischenrufe von anderen Personen sind nicht erwünscht.«Wreck a nice beach»Wenn sich Menschen die Zeit nehmen, eine Software zu trainieren, wenn sie langsam sprechen und zwischen den Wörtern kleine Pausen machen, kommen sie auf eine Erkennungsgenauigkeit von bis zu 95 Prozent. Aber das heisst immerhin noch: Jedes zwanzigste Wort wird falsch erkannt. Als Ray Kurzweil seine Software noch nicht an L&H verkauft hatte, amüsierte er die Anwesenden bei einer Pressevorführung mit dem vom Computer erkannten Satz «Let's talk about how to wreck a nice beach», der im Kontext einer Computermesse befremdlich klang. «Let's talk about how to recognize speech», hatte Kurzweil eigentlich sagen wollen.In einem Aufsatz zur Sprachtechnologie im Alltag berichtet Wolfgang Wahlster vom Deutschen Forschungszentrum für künstliche Intelligenz von Wortfehlerraten von 5 bis 10 Prozent, die moderne Diktiersoftware noch auf den leistungsfähigsten PC aufweist. Also auf Rechnern, die die Profile und Eigenheiten des Sprechers lernen können und seine Sätze mit umfangreichen Sprachmustern und Grammatik-Parsern von der Festplatte weg abgleichen können. Wahlster schreibt: «Vom amerikanischen Markt für Diktiersoftware wird bereits berichtet, dass schon ein Jahr nach der Anschaffung nur rund 10 Prozent der Käufer von Diktiersystemen diese überhaupt noch einsetzen, da sich die Kauferwartungen nicht erfüllt haben.»Entsprechend miserabel sehen die Marktzahlen aus. Von einstmals 25 Anbietern sind nur die wirklich grossen wie IBM und Siemens übrig geblieben, die Grundlagenforschung im grossen Stil betreiben. Viele kleinere Unternehmen - darunter die renommierte Firma Dragon Systems - wurden in einem furiosen Feldzug von L&H aufgekauft, und diese Firma steht derzeit am Rande des Abgrunds.Das Kartenhaus wanktUm aus einer halbwegs brauchbaren Spracherkennung eine bessere zu machen, braucht es einen grösseren Aufwand, als um eine halbwegs brauchbare von Grund auf neu zu entwickeln. Der Forschungs-und Entwicklungsaufwand, der zur Verbesserung der Produkte notwendig ist, steigt mit zunehmender Produktqualität exponentiell. Dies ist eine mögliche Erklärung für den Absturz der belgischen L&H, der dieser Tage ein aufsehenerregendes Schauspiel abgibt. Die Firma wollte schnell wachsen, so wie amerikanische PC-Software-Firmen, aber mit einem Produkt, das sich gegen das schnelle Wachstum sperrt. Die Firma begann ihre Existenz Ende der achtziger Jahre mit dem Aufkauf von Kurzweils Spracherkennungssoftware Voice Xpress, die sie weiterentwickeln und europäisieren wollte. Bald mussten die Gründer Jo Lernout und Pol Hauspie erkennen, dass sich Fortschritte und Verbesserungen nicht mit ein paar Verfeinerungen bei den Algorithmen und einem Ausbau der Wörterbücher erzielen lassen, dass Grundlagenforschung gefragt ist. Diese ist aber für eine kleine Firma schwer finanzierbar. L&H konnte den belgischen Staat animieren, Geld vorzuschiessen, um in diesem mehrsprachigen Land mit vielen Niederlassungen der EU-Verwaltungen die Forschung anzukurbeln. Über ein Investitionsprogramm flossen gleich so viele Millionen in L&H, dass die Firma sich an den Aufkauf anderer Firmen im grossen Stil machen konnte. Sie schluckte zahlreiche Konkurrenten und entwickelte ein schwer zu durchschauendes Geflecht von Firmen, Forschungsgruppen und Kreuzfinanzierungen.Schliesslich entstand in Flandern ein Forschungspark in Form einer Innenohr-Schnecke, in dem sich das Gemenge an Firmen ansiedelte. Ein sogenanntes Dictation Consortium wurde gegründet, das die Grundlagenforschung betreiben sollte - die Gelder für dieses Konsortium wurden aber zur Zwischenfinanzierung in anderen Firmen geparkt. Das belgische Kartenhaus kam ins Wanken, als L&H in Bereiche diversifizierte, die zwar mit Sprachverarbeitung, doch nur noch entfernt mit der Spracherkennung zu tun hatten, und zudem versuchte, auf asiatischen Märkten Fuss zu fassen. Dort wurde ein ähnliches Geflecht von Firmen aufgebaut, die offenbar nur dank Scheinverkäufen untereinander den Anschein von Rentabilität aufrechterhalten konnten. Gegen Ende des vergangenen Jahres gab L&H Fehler und Unregelmässigkeiten in der Buchhaltung zu, an der europäischen Easdaq und der amerikanischen Nasdaq wurden die Aktien der Firma vom Handel ausgesetzt, die amerikanische Börsenaufsicht und die belgische Justiz begannen zu ermitteln, die Firma beantragte Gläubigerschutz.Wirkliche Fortschritte erzielte man bei L&H in Zusammenarbeit mit Philips allein mit der Sparte Automotive Systems, in der Sprachausgabe wie einfache Wortsteuerung im Auto entwickelt wurden. Es ist dieser Bereich, den DaimlerChrysler derzeit aufkaufen möchte.Wo das Problem liegtIn der wissenschaftlichen Beschäftigung mit der automatischen Spracherkennung fand vor rund zwei Jahren ein Umschwung statt, den David Nahamoo, Senior Manager Human Language Technologies bei IBM, als «Abschied vom PC» bezeichnete. «Der PC und die Software, die auf dem PC läuft, sind untauglich für eine vernünftige Arbeit mit der Sprache.» In der Anwendung dieser Erkenntnis orientieren sich die Firmen nun an der Spracharbeit im Hintergrund, die über einen Browser, einen Handheld oder das ganz normale Telefon genutzt werden kann, weil die starken Server und leistungsfähigen Sprachprogramme irgendwo auf entfernten Grossrechnern laufen. Es sind grosse Computer wie HAL, die dann den richtigen Konversationsmodus beherrschen werden, wenn sie ihren Besitzern erklären: «Ich weiss, dass du weisst, wo das Problem liegt.»Detlef BorchersFussnote: * David C. Stork (Ed.): HAL's Legacy. 2001's Computer as Dream and Reality. MIT Press. Boston 1997.Document neuzz00020010714dx22003gh"
"99",2012-06-21,"Living Keyboard: Nuance rüstet Swype auf; Lernende Tastatur verknüpft Text und Speech-Recognition","HightechLiving Keyboard: Nuance rüstet Swype auf; Lernende Tastatur verknüpft Text und Speech-RecognitionGeorg Pichler, pressetext.redaktion   540 words21 June 201214:45PressetextPRESSEGerman© 2012 Pressetext – News and Press Service for opinion leaders. All rights reserved. For further information see   http://www.pressetext.de[http://www.pressetext.de]Burlington (pte025/21.06.2012/13:45) - Nuance http://nuance.com[http://nuance.com] hat die Touchscreen-Tastatur Swype http://swype.com[http://swype.com] erweitert. Diese wurde nun mit dem bekannten Spracherkennungs- und Diktationssystem ""Dragon"" verbunden und mit Lernfähigkeit ausgestattet. Dies ermöglicht durch die laufende Anpassung an den User die kontextuale Vorhersage und Ergänzung von Wörtern und Textbausteinen und lässt die App ein individuelles Wörterbuch anlegen.Burlington (pte025/21.06.2012/13:45) - Nuance http://nuance.com[http://nuance.com] hat die Touchscreen-Tastatur Swype http://swype.com[http://swype.com] erweitert. Diese wurde nun mit dem bekannten Spracherkennungs- und Diktationssystem ""Dragon"" verbunden und mit Lernfähigkeit ausgestattet. Dies ermöglicht durch die laufende Anpassung an den User die kontextuale Vorhersage und Ergänzung von Wörtern und Textbausteinen und lässt die App ein individuelles Wörterbuch anlegen.Worterkennung im KontextIn der Praxis soll dies dazu führen, dass Swype Wörter anhand des restlichen Satzes wählt und ausbessert. Das Tool, das einst vom Start-up Tegic entwickelt wurde, ist ursprünglich für die Texteingabe über Wischbewegungen bekannt. Jedoch liegen etwa die englischen Begriffe ""pot"" (Kessel) und ""put"" (stellen) auf dem gleichen ""Zeichenpfad"". Anhand des Kontexts kann die Software nun erkennen, welcher dieser Begriffe gemeint war und ergänzt somit ""put"" zu ""that away"" (stell' das weg) und ""pot"" zu ""tea"" (Teekessel).Das ""Living Keyboard"" für Android lernt auch aus der Häufigkeit vorkommender Begriffe. Schreibt der User etwa oft ""Opa Meier"", so kann es nach einiger Zeit ""Meier"" automatisch ergänzen, sobald das Wort ""Opa"" eingetippt wurde. Außerdem kann es auf dem Telefon gespeicherte Textnachrichten, E-Mails sowie Facebook-Postings und Twittereinträge des Users durchforsten und analysieren - ähnlich wie es das Konkurrenzprodukt ""SwiftKey"" tut.Individuelles Audio-WörterbuchZwar hat Swype bereits in früheren Versionen Spracheingabe unterstützt. Über die Anbindung zu den Vokabelservern von Dragon wird dabei eine höhere Genauigkeit in der Erkennung erzielt. Mit der neuen Version kann das Programm nun auch individuelle, gesprochene Wörter erlernen.Wird ein aufgenommener Begriff in der Datenbank nicht gefunden, so kann der User ihn eintippen - was etwa für diverse Namen wichtig ist. Durch die Verbindung zwischen dem Textinput und dem Audio-Sample entstehen neue Begriffe, die mit dem Telefon in Form eines persönlichen Wörterbuchs synchronisiert werden. Das neue Feature bringt Support für 55 Sprachen mit.Darüber hinaus beherrscht Swype auch weiterhin Zeichenerkennung und die klassische Input-Methode des Eintippens. Das Tool ist in der Regel vorinstalliert, jedoch bietet Nuance die Beta-Version der neuen Ausgabe auf seiner Homepage zum Download an. Diese soll problemlos neben bestehenden Swype-Installationen arbeiten.Innovations-Wettrennen bei TastaturenDas Trendbarometer bei Onscreen-Tastaturen weist weiter in Richtung automatischer Anpassung an den User, wobei kontext-orientiertes Lernen einer von mehreren Zugängen ist. So arbeiten Forscher etwa an einem Keyboard, das sein Layout gemäß der Tippgewohnheiten des Users optimiert (pressetext berichtete: http://pte.com/news/20120606022[http://pte.com/news/20120606022] ).Große Hoffnungen ruhen auch in haptischen Touchscreen-Oberflächen, die die Umsetzung plastischer Tasten erlauben. Eine flüssigkeitsbasierte Technologie namens ""Tactus"" könnte bereits 2013 in ersten Geräten Anwendung finden (pressetext berichtete: http://pte.com/news/20120609003[http://pte.com/news/20120609003] ).Download: http://beta.swype.com/[http://beta.swype.com/](Ende)Aussender: pressetext.redaktionAnsprechpartner: Georg PichlerTel.: +43-1-81140-303E-Mail: pichler@pressetext.comWebsite: www.pressetext.com[http://www.pressetext.com]Pressetext Nachrichtenagentur GmbHDocument PRESSE0020120621e86l001md"
"100",2016-02-23,"PRESSEMITTEILUNG/MYNEWSDESK Phonak Roger Pen mit dem iF DESIGN AWARD 2016 ausgezeichnet (mit Bildern)","PRESSEMITTEILUNG/MYNEWSDESK Phonak Roger Pen mit dem iF DESIGN AWARD 2016 ausgezeichnet (mit Bildern)717 words23 February 201609:00Dow Jones Newswires GermanRTDJGEGermanCopyright © 2016, Dow Jones & Company, Inc.   (Mynewsdesk) FELLBACH, DEUTSCHLAND and STÄFA, SCHWEIZ -- (Marketwired) -- 02/23/16 -- Phonak, weltweit führender Hersteller und Anbieter von Hörgeräten und drahtlosen Kommunikationslösungen, wurde für das herausragende Produktdesign des Phonak Roger Pen mit dem iF DESIGN AWARD 2016 in der Kategorie Medizin/Gesundheit ausgezeichnet. Das drahtlose Mikrofon ermöglicht Menschen mit Hörverlust ein deutlich besseres Verstehen in schwierigen Hörsituationen.Der Phonak Roger Pen kommt im funktionalen und dezenten Design eines stylishen Stifts daher - und wirkt eher wie ein Lifestyle-Gadget als ein medizinisches Produkt. Dieser Designansatz beeindruckte die internationale Jury des iF DESIGN AWARD, die den Phonak Roger Pen in der Kategorie Medizin/Gesundheit zum Sieger kürte.?Wir freuen uns sehr über die Auszeichnung mit dem iF DESIGN AWARD"", sagt Drs. Hans Mülder, Marketing Director von Phonak Communications. ?Der Award bestätigt unseren Ansatz, höchste Ansprüche an Hörleistung, Benutzerfreundlichkeit und Innovation mit einem herausragenden Design zu verbinden.""Cleveres Design für optimales Sprachverstehen in jeder Lebenslage Der Roger Pen überzeugte die Jury des renommierten Design-Preises außerdem mit Features wie einer Speziallackierung des Pens, die Reibungsgeräusche im Vergleich zu einer normalen Lackierung deutlich reduziert. Ein kleines Gewicht, das im Pen angebracht ist, sorgt dafür, dass der Stift stets mit dem Mikrofon nach oben auf dem Tisch zum Liegen kommt. Beide Funktionen erleichtern Menschen mit Hörverlust das Verstehen deutlich.Der intuitiv bedienbare Roger Pen sorgt für ein Extra an Unterstützung in schwierigen Hörsituationen, wie in lauten Umgebungen oder über Distanz, in denen selbst moderne Hörgeräte alleine Sprache in der Regel nicht mehr optimal übertragen können. Über kleine Empfänger, die unauffällig am Hörgerät befestigt werden, überträgt Phonak Roger Pen die Stimme des Sprechers direkt auf das Ohr - und ermöglicht Hörgeräteträgern sogar ein bis zu 62% besseres Sprachverstehen als Normalhörenden.* Der integrierte Breitband Bluetooth-Chip ermöglicht die drahtlose Anbindung ans Mobiltelefon und überträgt auch den Fernsehton kabellos auf die Hörgeräte des Nutzers.Bilder zu Phonak Roger Pen zum Download finden Sie in der Kategorie Wireless Communication Portfolio unter http://www.phonakpro.com/de/b2b/de/professional_tools/marketing/Bildd[http://www.phonakpro.com/de/b2b/de/professional_tools/marketing/Bildd] aten.html*Professor Thibodeau, Linda, PhD (2014), Comparison of speech recognition with adaptive digital and FM wireless technology by listeners who use hearing aids, University of Texas, Dallas, USA, The American Journal of Audiology. Volume 23, 201-210, June 2014.Über iF International Forum Design GmbHDas iF International Forum Design GmbH in Hannover ist eines der größten und renommiertesten Designzentren weltweit. Das iF Logo, das an die Gewinner der iF DESIGN AWARDs vergeben wird, gilt als international anerkanntes Gütesiegel für herausragendes Design. Jährlich gehen mehr als 20.000 Bewerbungen aus mehr als 60 Ländern für die begehrte Auszeichnung bei den iF DESIGN AWARDs ein. Weitere Informationen: www.ifdesign.de[http://www.ifdesign.de]Über Phonak Phonak, Mitglied der Sonova Gruppe, mit Hauptsitz in Stäfa, Schweiz, entwickelt, produziert und vertreibt seit mehr als 60 Jahren technologisch führende Hör- und Funksysteme. Dabei kombiniert Phonak die profunde Kenntnis in Hörtechnologie und Akustik mit einer intensiven Zusammenarbeit mit Hörgeräteakustikern, um Hörvermögen und Sprachverstehen von Menschen mit Hörminderung zu verbessern und somit ihre Lebensqualität zu erhöhen.Phonak bietet eine vollständige Produktpalette an digitalen Hör- und ergänzenden Funklösungen. Mit weltweiter Präsenz treibt Phonak Innovationen voran und setzt neue Maßstäbe in Miniaturisierung und Leistung.Phonak - Life is on Wir sind uns der Bedürfnisse derer bewusst, die sich auf unser Wissen, unsere Ideen und unsere Betreuung verlassen. Indem wir auf kreative Weise die Grenzen der Technologie durchbrechen, schaffen wir Lösungen, die Menschen darin unterstützen zu hören, zu verstehen und die reichhaltige Welt der Klänge zu erleben.Mühelose Interaktion. Grenzenlose Kommunikation. Leben ohne Kompromisse. Life is on.Bild verfügbar: http://www2.marketwire.com/mw/frame_mw?attachid=2967562[http://www2.marketwire.com/mw/frame_mw?attachid=2967562]Für weitere Informationen besuchen Sie bitte www.phonak.de[http://www.phonak.de] oder kontaktieren Sie:Phonak GmbH Jan-Christian Fross E-Mail: jan.fross@phonak.com Tel: +49 711 510 70 335BSKOM GmbH Eva Birle E-Mail: birle@bskom.de Tel: +49 89 13 95 78 27 11=== Phonak Roger Pen mit dem iF DESIGN AWARD 2016 ausgezeichnet (Bild) ===Shortlink: http://shortpr.com/kmxptw[http://shortpr.com/kmxptw]Permanentlink: http://www.themenportal.de/bilder/phonak-roger-pen-mit-dem-if-design[http://www.themenportal.de/bilder/phonak-roger-pen-mit-dem-if-design]- award-2016-ausgezeichnet      (Dies ist eine über Mynewsdesk verbreitete Pressemitteilung. Für den Inhalt ist ausschließlich das herausgebende Unternehmen verantwortlich.)(END) Dow Jones Newswires23-02-16 0800GMTDow Jones & Company, Inc.Document RTDJGE0020160223ec2n00056"
"101",2003-03-20,"Durchbruch bei Spracherkennung mit neuronalem Netzwerk  ","Durchbruch bei Spracherkennung mit neuronalem Netzwerk  595 words3 July 2003Computerwelt OnlineCMPONL20030703German© 2003 Info Technologie Verlag GmbH. All rights reserved. For further information see  http://www.computerwelt.at[http://www.computerwelt.at]Marina del Rey/Wien - US-Forscher der University of Southern California scheint der Durchbruch zu einem „übermenschlichen“ Spracherkennungssystem g...  Marina del Rey/Wien - US-Forscher der University of Southern California scheint der Durchbruch zu einem „übermenschlichen“ Spracherkennungssystem gelungen zu sein. In ersten Vergleichstests hatte das Berger-Liaw Neural Network Speaker Independent Speech Recognition System nicht nur sämtliche bekannten technischen Spracherkennungssysteme weit hinter sich gelassen, sondern auch das menschliche Hörvermögen übertroffen. Das war zuvor keinem anderen Sprecher-unabhängigen Computersystem gelungen, so Theodore W. Berger, Professor für biomedizinisches Ingenieurwesen und einer der Entwickler des Systems.  Besser als das  menschliche Ohr  Sogar aus tausendfach stärkerem Hintergrundlärm kann das System menschliche Stimmen herausfiltern, während das Ohr und Gehirn nur mit einem Bruchteil davon fertig werden. Wird beispielsweise die „Zielstimme“ von Stimmengewirr überdeckt, etwa bei Gruppendiskussionen oder in einer Bahnhofshalle, so versagen bisherige Spracherkennungssysteme bereits bei mehr als zehn Prozent.  Der Mensch muss bei etwas mehr Stimmengewirr aufgeben, während das Berger-Liaw-System noch zwei Drittel der gesprochenen Worte erkennt, wenn das Stimmengewirr darum herum 560 Mal lauter ist beziehungsweise wenn die gesprochenen Wörter für menschliche Gehörgänge in sogenanntem weißen Rauschen untergehen.  Neuronaler Ansatz  überarbeitet  Wie bei derzeit erhältlichen Spracherkennungssystemen, basiert auch das Verfahren der beiden USC-Wissenschaftler auf neuronalen Netzen.  Der Durchbruch gelang durch einen neuen Denkansatz: Berger und sein Kollege Jim-Shih Liaw orientierten sich beim Aufbau ihres neuronalen Netzwerkes mehr als bisher an der Art und Weise, wie Signalcharakteristiken im menschlichen Hirn verarbeitet werden. Sie fanden heraus, dass die Leistungsfähigkeit eines neuronalen Netzes gesteigert werden kann, wenn es in ganz bestimmten zeitlichen Abständen Impulse erhält. Menschliche Nervenzellen reagieren ebenfalls nur auf Eingaben, wenn sie in einer gewissen Zeit aufeinanderfolgen, erklärt Liaw. Liegen die beiden Impulse weiter auseinander oder enger zusammen, macht die Neurone keinen Mucks.  Das Timing ist  entscheidend  Diese Tatsache sei allerdings beim Bau herkömmlicher Elektronenhirne außer Acht gelassen worden, meint Berger. „Ich weiß, das ist kaum zu glauben, aber es ist nun mal so“, sagt er. Durch den Einbau der Timing-Funktion in ihr Spracherkennungssystem kommt es auch mit lediglich elf Neuronen und 30 Verbindungen aus. Herkömmliche künstliche Intelligenzbestien weisen dagegen 1.000 Neuronen mit 10.000 Verbindungen auf und können doch nur mit Ach und Krach Muster erkennen.  Trotzdem werden alle bisherigen Systeme übertroffen, so Berger, weil bei deren Aufbau stets eine wichtige Dimension übersehen worden sei, man habe die biologischen Vorbilder zu sehr simplifiziert: „Bisher hat man Silizium-Neuronen nur diskrete Signale unterschiedlicher Stärke übertragen lassen, ebenso getaktet wie der Computer, von unveränderlicher Signaldauer. Doch bei lebenden Zellen ist die zeitliche Dimension, sowohl beim anregenden Signal als auch bei der Antwort, ebenso wichtig wie die Intensität.“  Skeptische Kritiker  Auch wenn das System derzeit nur zwölf Wörter erkennt und mit acht Sprechern getestet worden ist, sei es durchaus ausbaufähig, behauptet Berger. Seine Kollegen sind sich nicht so sicher. „Ich bin da sehr skeptisch“, meint Steven Greenberg, ein Spracherkennungsexperte am kalifornischen International Computer Science Institute. „Das System wurde noch nicht richtig geprüft und die Technik erst in groben Zügen beschrieben“, moniert er.  Breites  Anwendungsspektrum  Dies hat seinen Grund: Die Erfinder wollen ihr „übermenschliches“ Verfahren erst patentieren lassen. Anwendungsgebiete könnten die Sprachsteuerung und Diktiersysteme für PC, Hörhilfen für Fluglotsen bis zum Erstellen automatischer Mitschriften von Debatten, bei welchen jeder einzelne Sprecher identifiziert wird. Auch für die Erkennung anderer Laute ließe es sich einsetzen, etwa für das Aufspüren von  U-Booten in der Geräuschkulisse der Weltmeere, oder zur Analyse medizinischer Überwachungsgeräte.  (IDG/el)  www.usc.edu/ext-relations/news_service/[http://www.usc.edu/ext-relations/news_service/] real/real_video.html  www.usc.edu/[http://www.usc.edu/]Info Technologie Verlag GmbHDocument CMPONL0020040929dz73008sq"
"102",2006-05-20,"Ericsson übernimmt IP-Anbieter Netwise Schweden sehen Übernahme als Abrundung des eigenen Portfolios  ","Ericsson übernimmt IP-Anbieter Netwise Schweden sehen Übernahme als Abrundung des eigenen Portfolios  455 words5 August 200614:25PressetextPRESSEGerman© 2006 Pressetext – News and Press Service for opinion leaders. All rights reserved. For further information see  http://www.pressetext.de[http://www.pressetext.de]Nach erfolgreichen Kooperationen in der Vergangenheit übernimmt der schwedische Telekommunikationsausrüster Ericsson  http://www.ericsson.com [http://www.ericsson.com] den VoIP-Anbieter und Contact Management Spezialisten Netwise  http://www.netwisecorp.com [http://www.netwisecorp.com] . Die Resonanz auf das Aktien-Übernahmeangebot verlaufe nach Angaben von Ericsson bisher sehr positiv und werde bis zum 25. August verlängert. Als Kaufpreis für die Netwise-Anteile zahlen die Schweden rund 300 Mio. schwedische Kronen, das entspricht 60 Kronen pro Aktie. Beide Unternehmen verfügen über langjährige Erfahrungen in der Sprach-Datenintegration und bieten unterschiedliche Applikationen in der klassischen und IP-basierten Telefonie.  Da Erreichbarkeit und Verfügbarkeit zu den wichtigsten Erfolgsfaktoren geworden sind, haben viele Unternehmen ihre Kommunikationsinfrastruktur an diesem Grundsatz ausgerichtet. Traditionelle Telefonanlagen oder IP-PBX-Systeme gewährleisten heute nahezu 100-prozentige Verfügbarkeit. Dazu zählt auch das Herzstück der Produktfamilie von Netwise: das IP-basierte Contact Management System CMG, das der Steigerung des Serviceniveaus und damit der Kundenzufriedenheit dient.  Außerdem unterstützen die Produkte Automatic Speech Recognition (ASR) und Text-to-Speech (TTS) und bilden damit die Voraussetzungen für automatisierte Sprachanwendungen im Call-Center. Jüngste Untersuchungen haben ergeben, dass der globale Markt für Internettelefonie und IP-basierte Dienstleistungen auf fast 20 Mio. Anwender angewachsen ist. Dieser Entwicklung trägt auch Netwise mit seinen Entwicklungen Rechnung.  Bei Ericsson sieht man den Erwerb des Unternehmens als notwendige und passgenaue Ergänzung zum eigenen Portfolio. ""Mit den modernen Lösungen für Call- und Contact Center und den IP-Anwendungen runden wir unser vorhandenes Angebot sinnvoll ab"", unterstreicht Mehdi Schröder, Vice President Enterprise bei Ericsson Deutschland. ""Auch bei Netwise wurde die Konvergenz von Festnetz und Mobilfunk in den letzten Jahren vorangetrieben."" Durch die Integration von Festnetz-Zentrale, IP-Anlage und des Mobilfunkgerätes könnten die gleichen Funktionen und Leistungen unabhängig vom der benutzten Infrastruktur genutzt werden. Mit den Netwise-Anwendungen für Unternehmen und den weltweiten Vertriebsmöglichkeiten will Ericsson das Enterprisegeschäft langfristig stärken.  Netwise war bisher außer in Skandinavien auf dem deutschen und französischen Markt tätig. Erst im letzten Jahr hatte Netwise die Stuttgarter Telesnap AG übernommen, einen Anbieter von Computer Telephony Integration (CTI). Deren Snapware mit umfassenden Teamfunktionen ermöglicht es beispielsweise, Informationen in Echtzeit über die eingebettete Chatfunktion auszutauschen. Und über einen so genannten Presence Manager werden Anrufe an den Adressaten umgeleitet.  Bisher hatten Ericsson und Netwise bereits bei der Realisierung des Vermittlungsplatzsystems Netwise Now - Ericsson MD110 kooperiert, einer Lösung für Telefonzentralen, mit der sich auch hohe Anruferzahlen bewältigen lassen. Automatische Popup-Fenster am PC informieren dabei direkt über den Anrufer und seine bisherigen Geschäftskontakte. Alle Daten aus dem Telefonverzeichnis können über Suchoptionen abgerufen werden. Gleichzeitig sind die Vermittler in der Telefonzentrale in der Lage, den zuständigen Mitarbeitern Informationen zukommen zu lassen, ohne dabei die Anwendung zu wechseln.  Pressetext Nachrichtenagentur GmbHDocument PRESSE0020060805e285000gp"
"103",2006-01-20,"Abkürzungen  ","Abkürzungen  61 words1 March 2006LANLineLANLN077German© 2006 Konradin Verlag. All Rights Reserved. For further information see  http://www.lanline.de[http://www.lanline.de]API Application Programming Interface  ASR Automatic Speech Recognition – Spracherkennung  CRM Customer Relationship Management – Kundendatenbank  IVR Interactive Voice Response – Sprachdialogsystem  MRCP Media Resource Control Protocol  RFC Request for Comments – Status des Standardisierungsverfahrens  RTP Realtime Transport Protocol  SIP Session Intitiation Protocol  SV Speaker Verification – Sprecherverifizierung  TTS Text to Speech – Sprachsynthese  UMS Unified Messaging System  VoIP Voice over IP  Konradin Verlag Robert Kohlhammer GmbHDocument LANLN00020060308e2310000p"
"104",2016-10-25,"PRESSEMITTEILUNG/MYNEWSDESK Phonak Roger Pen mit dem German Design Award 2017 ausgezeichnet","PRESSEMITTEILUNG/MYNEWSDESK Phonak Roger Pen mit dem German Design Award 2017 ausgezeichnet654 words25 October 201612:30mynewsdeskMYNEWDGermanCopyright 2016. mynewsdesk (Mynewsdesk) Fellbach, Deutschland / Stäfa, Schweiz  Phonak, weltweit führender Hersteller und Anbieter von Hörgeräten und drahtlosen Kommunikationslösungen, wurde für das herausragende Produktdesign des Phonak Roger Pen mit dem German Design Award 2017 in der Kategorie Medical, Rehabilitation and Health Care ausgezeichnet. Das drahtlose Mikrofon ermöglicht Menschen mit Hörgeräten und Cochlea Implantaten ein deutlich besseres Verstehen in schwierigen Hörsituationen.Der Phonak Roger Pen kommt im funktionalen und dezenten Design eines stylishen Stifts daher und wirkt eher wie ein Lifestyle-Gadget als ein medizinisches Produkt. Dieser Designansatz beeindruckte die hochkarätig besetzte, internationale Jury des German Design Award, die den Phonak Roger Pen in der Kategorie Medical, Rehabilitation and Health Care zum Sieger kürte.Wir freuen uns sehr über diese weitere Auszeichnung, sagt Drs. Hans Mülder, Marketing Director von Phonak Communications. Der Award bestätigt unseren Ansatz, außergewöhnliche Hörleistung mit einem herausragenden Design zu verbinden. Es ehrt uns besonders, dass es sich beim German Design Award um einen Preis handelt, bei dem Beiträge ausschließlich vom Rat für Formgebung oder von den Wirtschaftsministerien der deutschen Bundesländer nominiert werden können.Der Roger Pen ermöglicht Hörgeräteträgern in Situationen, in denen selbst moderne Hörgeräte aufgrund von Lärm und größeren Distanzen an ihre Grenzen kommen, ein bis zu 62% besseres Sprachverstehen als Normalhörende.(1) Das integrierte Breitband Bluetooth macht das Leben der Nutzer leichter: Es sorgt für eine drahtlose Anbindung an Mobiltelefone und überträgt auch den Fernsehton drahtlos auf die Hörgeräte. Für größere Gruppen lassen sich insgesamt bis zu zehn Roger Pens in einem Netzwerk kombinieren. So kann kann sich ein Hörgeräteträger gleichzeitig mit mehreren Sprechern drahtlos verbinden und damit auch die schwierigste Hörumgebung meistern: mehrere Gesprächspartnern über eine größere Distanz bei hohem Lärmpegel.(1) Professor Thibodeau, Linda, PhD (2014), Comparison of speech recognition with adaptive digital and FM wireless technology by listeners who use hearing aids, University of Texas, Dallas, USA, The American Journal of Audiology. Volume 23, 201-210, June 2014.Shortlink zu dieser Pressemitteilung: http://shortpr.com/92x9yo[http://shortpr.com/92x9yo]Permanentlink zu dieser Pressemitteilung: http://www.themenportal.de/gesundheit/phonak-roger-pen-mit-dem-german[http://www.themenportal.de/gesundheit/phonak-roger-pen-mit-dem-german] -design-award-2017-ausgezeichnet-37364=== Pressekontakt ===Herr Jan-Christian FrossPhonak Max-Eyth-Str. 20 70736 FellbachEMail: jan.fross@phonak.com Telefon: +49 711 510 70 335=== Pressekontakt ===Herr Florian FagnerBSKOM GmbH Herzogspitalstraße 5 80331 MünchenEMail: fagner@bskom.de Telefon: +49 89 13 95 78 27 16=== Über den German Design Award ===Vergeben wird der German Design Award vom Rat für Formgebung, der deutschen Marken- und Designinstanz. Sein Auftrag von höchster Stelle: das deutsche Designgeschehen zu repräsentieren. 1953 auf Initiative des Deutschen Bundestages als Stiftung gegründet, unterstützt er die Wirtschaft dabei, konsequent Markenmehrwert durch Design zu erzielen. Das macht den Rat für Formgebung zu einem der weltweit führenden Kompetenzzentren für Kommunikation und Markenführung im Bereich Design. Zum exklusiven Netzwerk der Stiftungsmitglieder gehören neben Wirtschaftsverbänden und Institutionen insbesondere die Inhalte und Markenlenker namhafter Unternehmen. Weitere Informationen unter: www.german-design-award.com[http://www.german-design-award.com]=== Über Phonak ===Phonak, Mitglied der Sonova Gruppe, mit Hauptsitz bei Zürich, Schweiz, wurde 1947 mit viel Leidenschaft und Begeisterung für Hörtechnologie gegründet. Auch 70 Jahre später ist dies weiter die treibende Kraft. Als führender Anbieter verfügt Phonak über das breiteste Produktportfolio von lebensverändernden Hörlösungen. Über kindgerechte Lösungen bis hin zur Versorgung von hochgradigem Hörverlust helfen wir Menschen dabei, sich sozial und emotional frei zu entfalten. Wir sind überzeugt, dass wir so die Lebensqualität verbessern können und eine Welt schaffen, in der jeder aktiv am Leben teilnehmen kann: Life is on.Bei Phonak sind wir der Überzeugung, dass gutes Hören eine Voraussetzung dafür ist, das Leben in vollen Zügen genießen zu können. Seit mehr als 70 Jahren verfolgen wir unser Ziel, der Entwicklung wegweisender Hörlösungen. Wir verbessern die Lebensqualität von Menschen, damit sie sich sozial und emotional frei entfalten können. Life is on.(Dies ist eine über Mynewsdesk verbreitete Pressemitteilung. Für den Inhalt ist ausschließlich das herausgebende Unternehmen verantwortlich.)Mynewsdesk GmbHDocument MYNEWD0020161025ecap0002u"
"105",2005-04-14,"News von FHplus  ","News von FHplus  773 words14 April 200517:47Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS""  Das Fachhochschul-Programm der FFG fördert weitere 23 neue Forschungsprojekte an Österreichs Fachhochschulen  Wien - Seit der ersten Ausschreibung von FHplus 2003 laufen bereits 20 Forschungsprojekte erfolgreich an Österreichs Fachhochschulen. Jetzt - nach der 2. Ausschreibung - folgen weitere 23 Projekte, die die Forschungskompetenzen weiter steigern und ausbauen.  Das Förderungsprogramm FHplus wurde im Auftrag des Bundesministeriums für Verkehr, Innovation und Technologie (BMVIT) sowie des Bundesministeriums für Bildung, Wissenschaft und Kultur (bm:bwk) von der Österreichischen Forschungsförderungsgesellschaft (FFG) entwickelt; seither ist der Bereich ""Strukturprogramme"" der FFG für das Programmmanagement verantwortlich.  Ziel von FHplus ist es, durch den Aufbau von F&E-Kapazitäten an Österreichs Fachhochschulen die Voraussetzungen dafür zu schaffen, dass systematisch und langfristig anwendungsbezogene F&E an den FHs möglich wird. Damit soll die Attraktivität der Fachhochschulen als F&E-Ansprechpartner für die Wirtschaft verbessert und die Qualität der Lehre an den FHs laufend weiter gesteigert werden.  Konkret werden durch FHplus F&E Projekte gefördert, die dem Aufbau von F&E-Strukturen für die Etablierung zusätzlicher Forschungsthemen an den Fachhochschulen dienen (Strukturaufbau-Vorhaben). Bestehen bereits solche Strukturen, können durch ambitionierte Kooperationsvorhaben gemeinsam mit der Wirtschaft die Kompetenzen der FH und die Zusammenarbeit mit WirtschaftspartnerInnen weiterentwickelt werden. Dabei müssen die WirtschaftspartnerInnen immerhin mindestens 30% der Kosten tragen. Die seit 2003 laufenden 20 FHplus-Projekte zeitigen bereits Erfolge: Es konnten neue ForscherInnen aufgenommen, Forschungsgruppen etabliert, Folgeprojekte aus der Wirtschaft akquiriert und Forschungskooperationen - zB in EU-Projekten - aufgebaut und vertieft werden.  Auf Empfehlung des Rates für Forschung und Technologieentwicklung wurden im letzten Jahr für die zweite Ausschreibung von FHplus Mittel in der Höhe von 7,5 Mio. Euro empfohlen und durch das BMVIT zur Verfügung gestellt. Die Auswahl der besten Projekte fand durch eine mit nationalen und internationalen ExpertInnen besetzten Jury statt. Von insgesamt 69 eingereichten Projekten wurden 23, die den Zielen von FHplus am besten entsprachen, durch die Jury zur Förderung empfohlen und von Bundesministerin E. Gehrer und Bundesminister H. Gorbach genehmigt. Damit konnte die verfügbare Förderungssumme von 7,5 Mio. Euro ausgeschöpft werden. Weitere drei Projekte wurden durch die Jury für förderungswürdig gehalten.  Die 23 neu ausgewählten Projekte werden die bisherigen Erfolge weiter unterstützen und ausbauen - es werden neue Forschungsfelder etabliert und bestehende Kompetenzen ausgebaut, sodass sich die österreichischen Fachhochschulen als bedeutende Knotenpunkte in F&E-Exzellenznetzwerken etablieren. Diese Projekte generieren mit der vergebenen Bundesförderung von 7,5 Mio. EUR ein gesamtes Projektvolumen von 12,4 Mio. EUR, mit 9,1 Mio. EUR werden dabei FH-Forschungspersonal finanziert. Im Unterschied zur ersten Ausschreibung, bei der die Strukturaufbau-Vorhaben den weitaus größten Teil ausmachten, sind nun von den 23 geförderten Projekte 14 Kooperationsvorhaben und nur mehr 9 Strukturaufbau-Vorhaben - ein Indiz dafür, dass die FHs zunehmend an Bedeutung gewinnen als F&E-Kooperationspartner für die Wirtschaft.  Die genehmigten FHplus Projekte spiegeln die thematische Breite, die die Fachhochschulen anbieten, wider. Die Bandbreite reicht dabei von Einsatz und Design von modernsten Breitbandtechnologienetzen über zellbasierte Testsysteme und die Herstellung komplexer therapeutischer Proteine bis hin zur Qualitätssteigerung in der Notfallmedizin. Thema ist aber auch die Personalisierung von mobilem Marketing oder die einfache Erschließung von Audiodaten nach persönlichen Vorlieben.  Strukturprogramm:  FH (Antragsteller)    Art     Titel der ProjekteFH Burgenland         SV      Internationales Kompetenzzentrum für                              WeinmanagementFH Technikum Kärnten  KV      Qualitätssteigerung in der                              Notfallmedizin durch wireless wearable                              computingFH Technikum Kärnten  SV      Konzeption und Aufbau eines                              populationsbezogenen Pflegeregisters                              für das Land Kärnten: CAREN (Carinthia                              Registry of Nursing)FH Technikum Kärnten  SV      NETQUEST - Kostensimulation und                              Optimierung von Glasfasernetzen auf                              der Last MileFH St. Pölten         KV      AllThatSounds - assoziativ,                              semantische Erschließung von                              Audiodaten in intermodalen KontextenIMC Krems             SV      Zellbasierende Testsysteme für                              bioaktive SubstanzenFH OÖ                 KV      Aktive ThermografieFH OÖ                 KV      PC-gestützter Simulationsbeschleuniger                              auf Basis neuester BustechnologieFH OÖ                 KV      Speech Recognition Services for                              Wireless Mobile Devices / GulliverFH OÖ                 KV      Intelligente, selbst lernende                              NavigationskartenFH OÖ                 KV      Near Field CommunicationFH OÖ                 KV      Office Of Tomorrow - Arbeiten im Büro                              der ZukunftFH OÖ                 KV      Gestaltung und Implementierung                              eines Prozessmodells und                              darauf aufbauender kundenorientierter,                              flexibler OrganisationsformenFH OÖ                 KV      Hohe Personalfluktuation bei Anbietern                              sozialer Dienstleistungen                              Analyse - Kosten - MaßnahmenFH Salzburg           KV      Mobile MarketingFH Salzburg           KV      Steuerung der Qualität bei der                              thermischen Modifikation von HolzFH Salzburg           SV      Forschungszentrum DesignmanagementFH Joanneum           SV      Advanced Methods in Embedded Signal                              ProcessingFH Joanneum           SV      Sportwissenschaftliche                              Untersuchungsstelle SüdoststeiermarkFHS Kufstein          KV      Österreichische Immobiliendatenbank -                              Forschungsinstitut für BenchmarkingFH Vorarlberg         KV      Portfoliomanagement für                              StromversorgungsunternehmenFH Campus Wien        SV      Optimierung einer Produktionsplattform                              für die Herstellung therapeutischer                              ProteineFH Technikum Wien     SV      Designmethoden für Embedded Control                              Systems  Rückfragehinweis:   DI Dr. Sabine Mayer   FFG - Forschungsförderungsgesellschaft   sabine.mayer@ffg.at   +43-(0)5-7755-2301  *** OTS-ORIGINALTEXT UNTER AUSSCHLIESSLICHER INHALTLICHER VERANTWORTUNG DES AUSSENDERS ***  Austria Presse AgenturDocument AUPAG00020050414e14e008px"
"106",2014-09-16,"Studie belegt: Phonak Roger überbrückt Lärm und Distanz / Neue Technologie ermöglicht Menschen mit Hörverlust sogar einen Vorteil gegenüber Normalhörenden (FOTO)","Studie belegt: Phonak Roger überbrückt Lärm und Distanz / Neue Technologie ermöglicht Menschen mit Hörverlust sogar einen Vorteil gegenüber Normalhörenden (FOTO)733 words16 September 2014news aktuell OTS - OriginaltextserviceOTSGerman(c) 2014 news aktuell   Fellbach-Oeffingen (ots) -- Querverweis: Bildmaterial ist abrufbar unter http://www.presseportal.de/galerie.htx?type=obs[http://www.presseportal.de/galerie.htx?type=obs] -Gespräche in lauten Umgebungen sind anstrengend: Das weiß jeder aus eigener Erfahrung. Für Hörgeräteträger sind sie eine nahezu unüberwindbare Herausforderung. Eine neuartige Hörtechnologie macht Schluss damit: Sie überbrückt den Lärm und verschafft Menschen mit starkem Hörverlust sogar einen Vorteil gegenüber Normalhörenden. Das hat eine Studie von Professor Linda Thibodeau (Universität Texas) gezeigt: Der sogenannte Roger Pen, der die Stimme des Sprechers drahtlos auf ein oder zwei Hörgeräte überträgt, lässt Hörgeräteträger in lauter Umgebung Sprache bis zu 62%* besser verstehen als Menschen ohne Hörverlust.Professor Linda Thibodeau, Leiterin des Advanced Hearing Research Center an der Universität von Texas, hat in einer Studie das Sprachverstehen von Hörgeräteträgern und Normalhörenden in verschiedenen Hörumgebungen getestet.69% vs. 7%: Vorteil für Hörgeräteträger mit Phonak RogerDas Ergebnis: Hörgeräteträger, die die neuartige Roger Technologie nutzten, konnten Sprache in lauten Umgebungen nicht nur deutlich besser als zuvor verstehen, sondern übertrafen sogar normalhörende Menschen. So fiel bei 75 dB - das entspricht in etwa dem durchschnittlichen Verkehrslärm - der Vergleich eindeutig aus: Die Roger Nutzer verstanden 69% der Wörter korrekt, die Normalhörenden nur 7%.""Die Hörgerätetechnologie hat sich in den letzten Jahren rasant weiterentwickelt. Dennoch wissen wir: In lauten Umgebungen und über Distanz stehen Hörgeräteträger immer wieder vor einer großen Herausforderung. Das wollten wir mit Roger ändern,"" sagt Dr. Roger Baumann, Geschäftsführer der Phonak GmbH. ""Dass es uns dabei gelungen ist, Menschen mit Hörgerät sogar einen Vorteil gegenüber Normalhörenden zu verschaffen, macht uns stolz.""Der Stift macht den UnterschiedWie das möglich ist? Die Roger Technologie wurde über mehrere Jahre in der Schweiz entwickelt. Die Ingenieure wandten dabei einen radikal neuen Ansatz an: Komplexe mathematische Berechnungen sorgen dafür, dass Roger Sprache zuverlässig von Hintergrundgeräuschen unterscheiden kann. Verpackt wurde die neue Spitzentechnologie in ein stylishes Stift-Design, den Roger Pen. Dieser überträgt Sprache kristallklar per Funk auf das Hörgerät - und überwindet dabei problemlos Distanzen von mehreren Metern oder Lärmkulissen. Für den Träger heißt das: Er legt beispielsweise den unauffälligen Stift einfach auf den Tisch und hört so kristallklar das, was am anderen Ende gesprochen wird.Auch Gaby Fuchs, hochgradig schwerhörige Grundschullehrerin im Stuttgarter Großraum, hat selbst erlebt, welchen Unterschied der Roger Pen machen kann: ""Ich nutze den Roger Pen sehr gerne im Lehrerzimmer, bei Besprechungen oder um mich mit Kollegen auszutauschen. Dort ist insbesondere in den Pausen viel los, und das Zuhören strengte mich bisher extrem an. Mit Roger ist das entspannter für mich geworden. Auch im Privaten profitiere ich stark von meinem Pen: So konnte ich mich neulich erstmals mit Freunden auf meinem Balkon unterhalten, obwohl der Nachbar nebenan seinen Rasen gemäht hat.""Die Studie auf einen Blick- Studiengruppe: 11 Jugendliche / Erwachsene von 16-78 Jahren mit starkem Hörverlust, ausgestattet mit Hörgeräten verschiedener Hersteller- Kontrollgruppe: 15 Normalhörende von 18-30 Jahren- Methode: objektive und subjektive Messung der Spracherkennung bei unterschiedlichen GeräuschpegelnDie wichtigsten Ergebnisse:- Roger übertrifft andere Technologien deutlich- Ab einem Geräuschpegel von 65dB hören Hörgeräteträger mit Roger besser als Normalhörende- Ein Studienteilnehmer steigerte mit Roger sein Sprachverstehen von 0% auf 90% (bei 75 dB)* Professor Thibodeau, Linda, PhD (2014), Comparison of speech recognition with adaptive digital and FM wireless technology by listeners who use hearing aids, University of Texas, Dallas, USA, The American Journal of Audiology. Volume 23, 201-210, June 2014.Über PhonakPhonak, Mitglied der Sonova Gruppe, mit Hauptsitz in Stäfa, Schweiz, entwickelt, produziert und vertreibt seit mehr als 60 Jahren technologisch führende Hör- und Funksysteme. Dabei kombiniert Phonak die profunde Kenntnis in Hörtechnologie und Akustik mit einer intensiven Zusammenarbeit mit Hörgeräteakustikern, um Hörvermögen und Sprachverstehen von Menschen mit Hörminderung zu verbessern und somit ihre Lebensqualität zu erhöhen.Phonak bietet eine vollständige Produktpalette an digitalen Hör- und ergänzenden Funklösungen. Mit weltweiter Präsenz treibt Phonak Innovationen voran und setzt neue Massstäbe in Miniaturisierung und Leistung.OTS: Phonak GmbH newsroom: http://www.presseportal.de/pm/18689[http://www.presseportal.de/pm/18689] newsroom via RSS: http://www.presseportal.de/rss/pm_18689.rss2[http://www.presseportal.de/rss/pm_18689.rss2]Pressekontakt: Für weitere Informationen besuchen Sie bitte www.phonak.de/roger[http://www.phonak.de/roger] oder kontaktieren Sie:Phonak GmbH Jan-Christian Fross E-Mail: jan.fross@phonak.com Tel: +49 711 510 70 335BSKOM GmbH Eva Birle / Florian Fagner E-Mail: birle@bskom.de / fagner@bskom.de Tel: +49 89 13 95 78 27 112831809news aktuell GmbHDocument OTS0000020140916ea9g002e8"
"107",2003-09-30,"SandCherry and Unisys Corporation Collaborate for Delivery of Advanced Speech Services.  ","SandCherry and Unisys Corporation Collaborate for Delivery of Advanced Speech Services.  629 words30 September 200314:20Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS""  New York, NY (OTS) - Unisys to Offer Solutions Using SandCherry's AppRun(TM) VXML and SoftServer(TM) Platform to Worldwide Clientele  SpeechTEK - SandCherry, Inc., a provider of software for cost-effectively building and deploying advanced speech-enabled services, announces collaboration with worldwide IT services and solutions company Unisys Corporation (NYSE: UIS) for deployment of advanced speech services that will enhance the competitive advantage Unisys provides its clients. By deploying advanced speech applications using SandCherry's AppDev VXML and SoftServer platform, Unisys can offer its customers the immediate revenue and efficiency benefits of advanced speech solutions while providing the flexibility to readily support future applications, technologies and service growth. ""SoftServer is one of the most comprehensive, advanced and flexible speech platforms available,"" said Bill Scholz, Unisys Architecture Director, Global Communications. ""SandCherry's IP-based products already provide the next-generation solution required for Unisys clients across the globe.""  Unisys plans to use its advanced speech application development tools and integration capabilities in conjunction with SandCherry's platform components to create advanced service provider and enterprise offerings for customer service, workforce management, voice portals and speech-enabled network services.  At SpeechTEK, the companies will jointly showcase Unisys applications on the SoftServer platform, including:        --  Service provider displays using voice dialing and voicenavigation of Internet content.    --  Enterprise displays of field force automation at work.  ""Unisys' global strength makes it an ideal partner in advancing SandCherry's pledge to extend applications and information beyond the Web to phone and mobile users,"" noted Charles Corfield, SandCherry president and CEO. ""With this relationship, Unisys further demonstrates its established leadership in speech solutions and technology.""  Zwtl.: About SandCherry  SandCherry, Inc. ( http://www.sandcherry.com[http://www.sandcherry.com]) develops software platforms and products for extending applications to phone, mobile and multimodal devices using speech and touch-tone. By upgrading existing Web and telecom software technology with superior media management and control software, SandCherry's products empower service providers and enterprises to unleash the combined value of speech recognition software, application servers, and media servers to deliver enhanced services that can increase carrier revenues or reduce enterprise costs. Headquartered in Boulder, Colorado, the company was founded by telecom and software industry veterans from companies including AT&T, IBM, Lockheed Martin, Telcordia Technologies and Alcatel. For more information about SandCherry, please visit  www.sandcherry.com[http://www.sandcherry.com].  Zwtl.: About SoftServer(TM)  The SoftServer(TM) platform, SandCherry's flagship product, is an applications and media resource broker that simplifies the integration, deployment, and management of enhanced and next-generation services. It melds Web capabilities with traditional technologies to provide a flexible and scalable solution meeting the size and needs of each specific customer and easily adapting as additional service enhancements become viable in the future. It is this platform that allows customers to easily and cost-effectively deliver enhanced services such as voice-enabled customer service, voice dialing, and multimodal voice/data applications to subscribers. As a result, service providers can quickly and cost-effectively mass deploy these new services creating new revenue opportunities, and enterprises can reduce costs while offering improved services.  Zwtl.: About Unisys  Unisys is a worldwide information technology services and solutions company. Our people combine expertise in systems integration, outsourcing, infrastructure, server technology and consulting with precision thinking and relentless execution to help clients, in more than 100 countries, quickly and efficiently achieve competitive advantage. For more information, visit  www.unisys.com[http://www.unisys.com].  SandCherry SoftServer and AppRun are trademarks of SandCherry, Inc. All other trademarks and registered trademarks are the property of their respective owners. ots Original Text Service: SandCherry  Rückfragehinweis: Wendy Simmons Vendeloo Incorporated Phone: +1 212-475-3738 mailto:wendy@vendeloo.com  *** OTS-ORIGINALTEXT UNTER AUSSCHLIESSLICHER INHALTLICHER VERANTWORTUNG DES AUSSENDERS ***  OTS186 2003-09-30/14:17.   Austria Presse AgenturDocument AUPAG00020030930dz9u004bu"
"108",1998-04-19,"Lernen die PDAs bald das Sprechen?","Lernen die PDAs bald das Sprechen?Von Cloer, Thomas.205 words4 September 1998ComputerwocheCPWCHEGerman(c) Computerwoche 1998MÜNCHEN (CW) - Dank neuer Software aus dem Haus Lucent Technologies (ehemals Bell Labs) und Hardware vom Halbleiterriesen Intel könnten Spracherkennung und - synthese bald den Sprung vom Desktop auf Handheld-Rechner und Personal Digital Assistants (PDAs) schaffen.Grundlage ist die von Lucent in mehr als 20 Jahren Forschungsarbeit entwickelte ""Speech Application Platform"", die aus den mehrsprachigen Modulen ""Automatic Speech Recognition"" (ASR) und ""Text to Speech"" (TTS) besteht und für Entwickler knapp 500 Dollar kostet. Bisher war die Software nur für interne Belange von Telco-Riesen wie AT&T erhältlich.Lucents Programmierschnittstelle (Application Programming Interface = API) soll die Ent-wicklung sprachunterstützter Windows-Anwendungen enorm erleichtern.Vorstellbar sind etwa Vorleseprogramme für E-Mails und News oder Web-Browser, die sich durch gesprochene Kommandos steuern lassen. Solche Anwendungen wären gerade für tragbare Rechner interessant. So könnte ein Benutzer etwa die auf seinem Arbeitsplatzrechner eingegangenen Botschaften auf einen Handheld übertragen und sich die Texte später im Auto oder Flugzeug vorlesen lassen.Intel setzt große Hoffnungen auf die Technologie: ""Mit der TTS-Engine können auch Handhelds höchste Sprachqualität erzielen"", meint Produkt-Manager Skip Matthews. Ob Intel das Lucent-Produkt auch für die ""Strong arm""-CPUs nutzt, ist unklar.(c) Computerwoche 1998.Document cpwche0020010922du94002n4"
"109",2002-02-21,"SPRACHERKENNUNG - Mobile Geräte lernen verstehen.","SPRACHERKENNUNG - Mobile Geräte lernen verstehen.113 words21 February 2002Computer ZeitungCZTUNG2German(c) 2002 Konradin Verlag.All Rights Reserved. For further information see http://www.computer-zeitung.deBoston/Tokio (ag) - Der Sprachtechnikanbieter Speechworks und Motorolaarbeiten gemeinsam an einer verteilten Spracherkennung (DistributedSpeech Recognition; DSR). Ihre Architektur soll lo-kaleSprachverarbeitung auf mobilen Geräten mit komplexen Anwendungen aufzentralen Servern sowie mit Sprachausgabe verknüpfen. WesentlicheAufgabe der lokalen Software ist es dabei, Nebengeräuscheherauszufiltern. Erste Beispielanwendungen der Partner sind eineDatenbankabfrage für mobile Mitarbeiter sowie einFluginformationssystem. Eine so genannte Sprach-Middleware fürEmbedded-Systeme in Autos oder Handhelds hat derweil Toshibavorgestellt. Das System der Japaner siebt etwa Fahrgeräusche an Ort undStelle aus. Außerdem lässt sich das Modul für die eigentlicheVokabelerkennung austauschen, um mobile Geräte auf Anwendungen inverschiedenen Sprachen zu trimmen.Document cztung0020020228dy2l0001m"
"110",2007-01-09,"Sikom setzt auf TTS-Technik von Loquendo Text-to-Speech-(TTS)-Technik erlaubt automatisiertes Vorlesen   ","Sikom setzt auf TTS-Technik von Loquendo Text-to-Speech-(TTS)-Technik erlaubt automatisiertes Vorlesen   441 words9 January 200709:57PressetextPRESSEGerman© 2007 Pressetext – News and Press Service for opinion leaders. All rights reserved. For further information see   http://www.pressetext.de[http://www.pressetext.de]Sikom integriert künftig die Text-to-Speech-(TTS)-Technik von Loquendo in das Sprachportal VoiceMan® 7.0 und liefert es als Standardmodul mit der Software aus. In Sprachportalen mit TTS-Funktion liest ein automatischer Sprecher dem Anrufer die gewünschten Texte vor. Diese Technik wird beispielsweise für Auskunftdienste oder Bestellvorgänge genutzt, um kostensparend große Anrufvolumina zu bewältigen.   Rosanna Duce, die Vicepräsidentin von Loquendo freut sich über die Zusammenarbeit: ""Sikom ist ein führendes Unternehmen in den Bereichen Sprachportale, Call-Center und Computertelefonie."" Für Loquendo ist die Kooperation ein wichtiger Meilenstein im strategisch wichtigen deutschen Markt.   Sikom-Geschäftsführer Jürgen Hoffmeister kommentiert die Zusammenarbeit: ""Loquendo ist der führende europäische Sprachtechnologiehersteller mit einer breiten Unterstützung von Betriebssystemen und Landessprachen."" Darüber hinaus öffnet Loquendo Sikom einen direkten Zugang zu den Entwicklungsressourcen in den Labors. Die Anwender von VoiceMan profitieren von der Integration, da der Projektablauf und die Optimierung wesentlich vereinfacht werden. Beispielsweise ist es nicht weiter notwendig, Prompts nach jedem Entwicklungszyklus aufzuzeichnen. Als Prompts werden zusammengesetzte Satzteile einer Ansage in einem Sprachdialog bezeichnet. Das Voice-Portal VoiceMan ist eine universelle Sprachdialogplattform mit einer brillanten und natürlichen synthetischen Sprache, mit der sich Sprachanwendungen einfach, schnell und individuell realisieren lassen.   Über Loquendo   Loquendo ist ein Tochterunternehmen von Telecom Italia mit Stammsitz im italienischen Turin. Mit über 30 Jahren Erfahrung im Bereich Spracherkennung und Sprachsynthese ist Loquendo eines der weltweit erfahrensten Unternehmen. Loquendo TTS, Loquendo Automated Speech Recognition (ASR), Loquendo Sprecherverifikation (Speaker Verification) und die Loquendo VoxNauta Plattform sind Hochleistungslösungen, die den Systemintegratoren 18 Sprachen mit 42 verschiedenen Sprechern bieten. Loquendo ist der einzige Anbieter, der über die komplette Palette für Server, Desktop-PCs, PDAs und embedded Systemen verfügt. Viele Unternehmen und Netzbetreiber setzen weltweit bereits für Millionen von Anrufen die Programme von Loquendo ein. Weitere Informationen über Loquendo erhalten Sie unter   www.loquendo.com  [http://www.loquendo.com].   Über Sikom   Die Sikom Software GmbH ist führender Anbieter von automatisierten Sprachanwendungen und zählt zu den innovativsten Unternehmen im Bereich Telekommunikation. Auf der Basis offener Standards realisiert Sikom leistungsstarke und zukunftssichere Lösungen zur Optimierung von Kommunikationsprozessen in allen Branchen. Mittelpunkt des umfassenden Produkt-Portfolios sind die mehrfach ausgezeichnete, multimodale Sprachdialogplattform VoiceMan® sowie AgentOne® (Contact-Center-Lösung) und t.e.o. (Billingsystem). Sikom gründet den Erfolg auf starken Partnerschaften, u.a. in großen Forschungsprojekten mit Universitäten und industriellen Partnern. Zu den Kunden zählen Unternehmen wie T-Online, Karstadt-Quelle, Arvato, Schering, Henkel, Microsoft, zahlreiche Banken und Sparkassen, sowie Behörden und Kommunen. Das 1998 gegründete Unternehmen hat derzeit rund 30 Mitarbeiter und ist bundesweit mit Niederlassungen in Heidelberg, Zwickau, Hamburg, Hannover, Wuppertal und München vertreten.   Pressetext Nachrichtenagentur GmbHDocument PRESSE0020070109e319000m9"
"111",1997-06-19,"SPRACHERKENNUNG IST MEIST AUF SPEZIALANWENDUNGEN BESCHRÄNKT.","SPRACHERKENNUNG IST MEIST AUF SPEZIALANWENDUNGEN BESCHRÄNKT.Von Ueberhorst, Stefan.1336 words6 June 1997ComputerwocheCPWCHEGerman(c) Computerwoche 1997Auch wenn man sich mit Computern noch nicht so unterhalten kann, wie seinerzeit Scottie vom Raumschiff Enterprise, der in die Maus seines Macintosh sprach. Die Spracherkennung ist auf dem besten Weg, den Kinderschuhen zu entwachsen. Sonja Hübner* beschreibt den Stand der Technik und gibt eine Übersicht über die verfügbaren Produkte, deren Einsatzgebiete sich heute allerdings noch weitgehend auf Fachbereiche wie medizinische und juristische Anwendungen beschränken.Der Markt für Spracherkennung floriert: IBM beispielsweise meldet für die ""Voicetype""-Reihe über 30000 verkaufte Lizenzen in den ersten vier Monaten dieses Jahres. Nach einer Studie der internationalen Unternehmensberatung Frost & Sullivan soll sich der europäische Markt für sprachverarbeitende Systeme in den nächsten Jahren nahezu verdreifachen und bis zum Jahr 2002 einen Wert von knapp drei Milliarden Dollar erreichen.Die Erkennungsgenauigkeit der besten Programme liegt nach Angaben der Hersteller bei 95 bis 97 Prozent. Allerdings wird sie erst ""nach einer Trainingsphase erreicht, in der die Software die individuelle Sprechweise erlernt,"" betont Anne-Marie Derouault, international zuständig für Marketing und Vertrieb der IBM-Spracherkennungsprodukte. Das Training steigere die Erkennungsleistung deutlich. Hierzu sei es allerdings unerläßlich, fehlerhaft erkannte Begriffe über die Spracherkennung zu korrigieren. Unterbleibe die Korrektur oder werde sie beispielsweise direkt über die Tastatur ausgeführt, könnten die Systeme aus ihren eigenen Fehlern nicht lernen und daher auch die Erkennungsrate nicht verbessern.Nach Erfahrungen von Hans-Erhard Jacobs, Technologieberater bei der R+V-Versicherung in Wiesbaden, beträgt die Produktivitätssteigerung durch das Diktieren von Texten gut 50 Prozent. Auch das Versandhaus Quelle konnte Geschwindigkeit und Qualität bei der schriftlichen Bearbeitung von Fragen und Reklamationen durch Spracherkennung erheblich steigern.Wichtig für die meisten Anwender: Ihr Programm soll ""sprecherunabhängig"" sein. Dies behaupten zwar die meisten Hersteller von ihren Produkten, um jedoch eine akzeptable Erkennungsgenauigkeit zu erreichen, müssen viele Systeme anfangs auf einen Sprecher trainiert werden. Ein weiterer Punkt auf der Wunschliste ist die Möglichkeit, den Text ohne lange Pausen zwischen den einzelnen Worten diktieren und das Ergebnis sofort auf dem Bildschirm sehen zu können. Dank gewachsener PC-Rechenleistung und der Kombination verschiedener mathematisch-statistischer Modelle sollen sich diese Forderungen nach jahrelanger Forschung nun endlich erfüllen - leider noch nicht alle gleichzeitig in einem System.Begnügt man sich mit einem speziellen, klar abgegrenzten und nicht sehr umfangreichen Vokabular, beispielsweise dem der Radiologie, so reicht die PC-Rechenkapazität inzwischen für die kontinuierliche Echtzeitspracherkennung aus. In allen anderen Fällen bleibt einem das Sprechen mit zumindest einer kleinen Pause zwischen den Worten noch nicht erspart (diskreter Spracherkennung in Echtzeit). Kann man jedoch auf die Echtzeitverarbeitung des Sprachsignals (das sofortige Erscheinen des Wortes auf dem Bildschirm) verzichten, dann erlauben Programme, die im Batchmodus arbeiten, das ""natürliche"" Sprechen mit dem PC. Auf keinen Fall sollte bei einer Automatic Speech Recognition (ASR) an den Hardware-Ressourcen gespart werden. Erst ein größeres System, beispielsweise ein 166-Megahertz-Pentium-Rechner mit 32 MB RAM aufwärts, führt nach Erfahrungen des Stuttgarter Diplom-Informatikers Martin Hiller dazu, daß die Programme auch Spaß machen.Programme im ÜberblickDie Zukunftsträchtigkeit der Technologie steht bei Firmen wie Apple, Articulate Systems, Dragon Systems, IBM, Kurzweil, Philips und Siemens-Nixdorf nicht in Frage. Sie alle haben ASR-Produkte auf dem Markt.Die Mac-Company brachte 1993 ""Plaintalk"" heraus, ein Betriebssystem mit Funktionen zur Spracherkennung und Sprachsynthese. Allerdings nur in Englisch, später auch in Spanisch. In den USA liefert Apple einige Modelle des Power-Mac mit Spracherkennungsfunktionen aus, und es existieren auch Diktierlösungen auf Plaintalk-Basis. Für die deutsche Sprache sieht das Unternehmen laut Produkt-Manager Michael Dickschat bisher noch keine lohnenden Marktchancen.Der amerikanische Anbieter Articulate Systems bietet seine Add-on-Produkte zur Sprachverarbeitung via Internet an. Das System ""Powersecretary"" für den Mac - ursprünglich auf der Basis von Dragon-Sprachtreibern entwickelt und inzwischen auch von Dragon vertrieben - verfügt ausschließlich über ein englisches Vokabular von 60000 Wörtern.""Dragon Dictate"" für Windows 3.x, Windows 95 und Windows NT (ab Sommer 1997) ist in einer Classic-Version mit 30000 Wörtern und einer Power-Version mit 60000 Wörtern erhältlich. Spezielle Wörterbücher, sogenannte Dragon-Pro-Module, gibt es für Medizin, Recht, Geschäft und Finanzen, Computertechnik und Presse. Allerdings ist nur der Wortschatz für Juristen auf Deutsch erhältlich.IBM sieht sich selbst als Pionier bei Spracherkennungsprodukten. Die Applikationen basieren auf dem IBM Personal Dictation System (IPDS) für OS/2 aus dem Jahr 1993 und sind für unterschiedliche Anwendergruppen konzipiert: ""Voicetype 3.0"" für den professionellen Einsatz, ""Voicetype Developers Toolkit"" für Entwickler und ""Voicetype Simply Speaking"" für Home-Office-Anwender.Die Programme für OS/2, Windows 3.x und Windows 95 arbeiten nach dem Prinzip der Einzelworterkennung. Während des Diktats muß man daher jedes Wort durch eine kurze Sprechpause kennzeichnen. Im Navigationsmodus erkennt die Software jedoch auch kontinuierlich gesprochene Befehle, zum Beispiel ""Drucke Seite eins bis acht"". Dies ist möglich, weil zur Navigation nur ein begrenzter, zuvor definierter Wortschatz verwendet wird.Der Basiswortschatz von Voicetype umfaßt 30000 Wörter, darüber hinaus läßt sich Fachvokabular mit einem Wortschatz zwischen 27500 und 40000 Wörtern erwerben - derzeit in deutscher Sprache für Medizin (allgemeine Medizin, Gynäkologie, HNO, innere Medizin, Orthopädie, Pathologie, Radiologie, Unfallchirurgie), Recht und Wirtschaft sowie technische Gutachten.Kontinuierliches Sprechen auch im Diktatmodus gibt es von IBM in den USA für Radiologen unter der Bezeichnung ""Medspeak/Radiology"". Ende des Jahres sollen auch die deutschen und französischen Kollegen in den Genuß dieser kontinuierlichen Echtzeitspracherkennung kommen.Der amerikanische Hersteller Kurzweil bietet seine Spracherkennungsprodukte nur für US-Englisch an und hat auch keine Vertretung in Deutschland. Das Low-cost-Produkt ""Voice Pad Pro"" kann Diktate lediglich mit einem mitgelieferten Textprogramm aufnehmen. Das Wörterbuch umfaßt 17000 Wörter und kann um weitere 3000 oder eigene Makros ergänzt werden. ""Kurzweil Voice"" für Windows eignet sich in der Version mit 30000 Wörtern für jedes Textprogramm, die Pro-Variante mit 60000 Wörtern kann für medizinische Zwecke angepaßt werden.Philips Spracherkennung ""Speech Magic"" arbeitet im Unterschied zu den bisher genannten Produkten nicht im Echtzeit-, sondern im Batchmodus. Den Text kann man also nicht sofort auf dem Bildschirm sehen, sondern erst nach Abschluß des Diktats. Mit PCs der oberen Leistungsklasse bleibt die Zeitverzögerung nach Unternehmensangaben inzwischen unter der Länge des Diktats.Batchmodus erlaubt kontinuierliches SprechenEin anderer, für den Einsatz des Philips-Produkts vorteilhafter Unterschied: Das System basiert nicht auf dem Prinzip der diskreten, sondern auf dem der kontinuierlichen Spracherkennung, so daß sich ohne künstliche Pausen in die Maschine sprechen läßt. Erkennungsfehler können nach der Aufzeichnung in einem speziellen Editor korrigiert werden, der das gleichzeitige Bearbeiten und Abhören von Diktaten erlaubt.Der niederländische Konzern, der nach eigenen Angaben 42 Prozent des europäischen und 27 Prozent des internationalen Markts an Diktiersystemen hält, hat mittlerweile eine Serie modularer Spracherkennungsprodukte für Windows vorgestellt. Sie lassen sich in andere Anwendungen integrieren und sind unabhängig von der Hardware. Auf Maus und Tastatur kann der Anwender zwar beim Diktieren verzichten, die Systemnavigation ist jedoch nicht sprachgesteuert. Mit einem Kombigerät aus Mikrofon, Lautsprecher und Trackball sind aber beide Arbeiten mit einem Eingabemedium möglich.Speech Magic wird von mehreren Zusatzprodukten wie Workflow-und Entwicklungs-Tools begleitet und steht auch als Komplettlösung, der sogenannten ""Speech Magic Suite"", zur Verfügung. Spezialwortschätze gibt es in Deutsch derzeit nur für Radiologie und Wirtschaftsrecht (mit verschiedenen juristischen Fachgebieten). Medizinische Wörterbücher werden von unabhängigen Softwarehäusern entwickelt und sollen im Laufe des Jahres auf den Markt kommen.Auch Siemens-Nixdorf hat für die Radiologen ein System mit der Bezeichnung ""Speech Base"" entwickelt. Nach Herstellerangaben verfügt das System über ein radiologisches Vokabular von 11000 Wörtern, arbeitet sprecherunabhängig in Echtzeit und erlaubt natürliches Sprechen ohne Pausen. Auch ein Training ist laut SNI nicht notwendig. Texte, die über den Sprachgebrauch der Radiologie hinausgehen, müssen allerdings über die Tastatur eingegeben werden.Ärzte und Juristen sind aufgrund ihres eingrenzbaren, akkuraten und stark formalisierten Wortschatzes für die Spracherkennung besonders geeignet. Schriftsteller dagegen werden besser noch einige Zeit besser mit einer Tastatur arbeiten. Damit kontinuierliche Echtzeitspracherkennung im täglichen Leben Einsatz finden kann, werden nach Überzeugung von Karl Sahora, Präsident von Philips Speech Processing USA, Programme mit einem Vokabular von rund 200000 Wörtern benötigt. Deren Verarbeitung erfordere eine Prozessorleistung, die die heute verfügbare bei weitem übersteigt.*Sonja Hübner ist freie Journalistin in Stuttgart.(c) Computerwoche 1997.Document cpwche0020011001dt66003zo"
"112",2005-09-19,"Sparc verbessert Diktate    ","Sparc verbessert Diktate    jokl   350 words19 September 2005Der StandardDSTANGerman(c) 2005, Der Standard.  http://www.derstandard.at/[http://www.derstandard.at/]Österreichische Forscher wollen Spracherkennungssysteme verbessern   In Österreich beschäftigen sich zahlreiche Wissenschafter mit der Entwicklung von Semantic Services. Das vom Förderprogramm FIT-IT unterstützte Projekt Sparc   (Semantic Phonetic Automatic ReConstruction of dictations) soll die Spracherkennung bei Diktaten mit Computern durch semantisches Wissen erheblich verbessern. Vor allem im medizinischen Bereich. Dazu arbeiten hier Philips Austria, das Österreichische Forschungsinstitut für Artificial Intelligence (OFAI) und das Institut für Signalverarbeitung und Sprachkommunikation der TU Graz zusammen.   ""Normale Spracherkennungssysteme übersetzen gesprochene Sprache in geschriebene nach einem Regelwerk, das auf Grammatik beruht"", erklärt Heinrich Bartosik, Leiter der Abteilung Speech Recognition Technology bei Philips Austria. ""Wir wollen die Erkennung durch semantische Regeln aus allgemeinen Wissensquellen erweitern und verbessern.""   Bisherige Systeme übersetzen ein Diktat Wort für Wort, aber auch trainierte Personen formulieren nicht immer druckreif. Dazu kommt, dass es erhebliche Unterschiede zwischen gesprochener und geschriebener Sprache gibt. ""Es gibt Menschen, die gähnen oder dreimal hintereinander ‚äh‘ sagen, sie wiederholen etwas oder haben etwas vergessen"", sagt Johannes Matiasek vom Österreichischen Forschungsinstitut für Artificial Intelligence. So gibt es immer wieder Probleme bei klinischen Diagnosen und Operationsberichten.   Dazu kommt, ""dass alle diese Berichte einem bestimmten Schema folgen"", so Stefan Petrik von der TU Graz. Normalerweise müssen die Texte deswegen von Menschenhand überarbeitet werden. ""Der Transkribent bekommt das Rohmanuskript und bessert es aus. Es wird umformuliert und geglättet."" Sparc soll dem Computer jetzt beibringen, wie er formal korrekte Texte erzeugen kann. Matiasek: ""Wir wollen semantisches Wissen einsetzen, um die Diktiersysteme im medizinischen Bereich zu optimieren.""   Wie das geht? ""Wir trainieren den Computer, in dem wir ihn mit Millionen von Worten der korrekten Texte zusammen mit den Sounddaten füttern"", sagt Bartosik. Aus dem Vergleich kann der Computer lernen. Bei ärztlichen Befunden ist es oft so, dass derselbe Textblock mehrmals erscheinen muss. ""Wenn der Arzt ‚siehe oben‘ sagt, weiß der Computer, dass er einen Text mit einer gewissen Wahrscheinlichkeit von oben nach unten kopieren soll, statt ‚siehe oben‘ zu schreiben.""   Ende 2006 soll das Projekt schließlich beendet sein. ""Auch für andere Expertenbereiche wie Rechtsanwälte oder für Standardgeschäftsbriefe kann so ein Erkennungssystem sehr nützlich sein"", findet Petrik. (jokl)   Standard Verlagsgesellschaft M.B.H.Document DSTAN00020050918e19j0003v"
"113",1999-03-17,"Computer, bitte zum Diktat.","Computer, bitte zum Diktat.666 words17 March 1999Süddeutsche ZeitungSDDZGerman(c) 1999 Süddeutsche ZeitungTrotz moderner Technik verstehen Spracherkennungssysteme immer noch Nebengeräusche als WörterGlaubt man den Ankündigungen zur Cebit, hat die Technik der maschinellen Spracherkennung wieder einen neuen Stand erreicht. Ein ganzes Arsenal von Diktiergeräten, Anrufbeantwortern und kleinen, doch leistungsfähigen Speicherchips steht bereit, die Technik unter das Volk zu bringen. Der Computer kommt in den verschiedenen Szenarien kaum vor. Denn die Technik entwickelt sich dahin, den Manager wie gewohnt diktieren, den Anrufbeantworter wie bisher besprechen zu lassen. Diese Aufnahmen werden digital gespeichert und können dann sofort vom Computer und einer Fachkraft bearbeitet werden.Noch in den letzten Jahren waren Spracherkennungen schwer zu bedienen: Sie funktionierten nach der sogenannten ""diskreten Spracherkennung"". Bei dieser Technik muß zwischen jedem Wort eine Pause liegen, damit der Computer erkennen kann, wann ein Wort zu Ende ist. Heute ist dieses Kunst-Stottern (auf das trainierte Zeitgenossen nach wie vor schwören) durch die ""kontinuierliche Spracherkennung"" abgelöst. Bei ihr spricht man wie in ein Diktiergerät, von gesprochenen Start-und Stop-Befehlen des ""Bandes"" einmal abgesehen. Der Fortschritt verdankt sich vor allem der Weiterentwicklung der Rechnertechnik, denn Spracherkennung braucht wirklich leistungsfähige Systeme: Unter einem 300-MHz-Pentium und 48-MByte-Arbeitsspeicher fallen die kontinuierlich mitrechnenden Spracherkenner unweigerlich in den alten Stottermodus zurück. Büroausstatter wie Olympus und Sony haben entsprechende Diktiergeräte im Angebot, bei denen das herkömmliche Bandlaufwerk durch eine Speicherkarte ersetzt ist. Olympus setzt bei seinem Angebot auf IBMs ViaVoice 98. Die Konkurrenz ist Naturally Speaking der amerikanischen Dragon Systems und Voice Xpress der belgischen Firma Lern-out & Hauspie. Diese drei Firmen teilen den gesamten Markt praktisch unter sich auf, da alle anderen Anbieter mit Sublizenzen der großen drei arbeiten.So ist das System von Philips fast schon ein Newcomer, da es lange Zeit nur für spezielle Anwendungen wie etwa Arztberichte im Einsatz war. Abgespeckte Versionen aller Sprachspezialisten finden gerade Eingang in die intelligenten Bordsysteme der Zukunftsautos oder in die Vermittlungsstellen und Sprachboxen der Telephonsysteme, die aus einem Anruf eine E-Mail machen. ""1999 ist das Jahr von Telephon und Spracherkennung als neue Benutzerschnittstelle zu E-Business und E-Commerce"" radebrecht eine Presserklärung der IBM. Deshalb widmet sie heuer dem Publikum auf der Cebit gleich einen kompletten Stand unter dem Motto ""That's @Speech Recognition"".In der deutschen Sprache sei das Motto nicht ""klingend"", so die Auskunft der Sprachspezialisten. Wie gut die Spracherkenner sind, darüber gehen die Meinungen weit auseinander. Geworben wird mit einer Trefferquote von 100 Prozent, doch sie ist im Alltag illusionär. Je disziplinierter eine Person das herkömmliche Diktieren beherrscht, desto besser fallen die Resultate aus. Erfahrene Diktierer kommen auf 95 Prozent und das heißt immerhin noch: Jedes zwanzigste Wort wird falsch erkannt. Einigkeit besteht indes darin, daß das Training durch den Sprechenden die allerwichtigste Komponente der Sprachverarbeitung ist. Das Versprechen der Anbieter, bereits in fünf Minuten akzeptable Ergebnisse zu erzielen, ist pure Bauernfängerei. Jeder Spracherkenner muß für den Sprechenden ein sogenanntes ""Sprachmodell"" erzeugen. Zwischen 30 Minuten und zwei Stunden werden dafür benötigt, anschließend rechnet auch der schnellste Rechner eine Stunde lang das Sprachmodell zusammen. Das einmal gefundene Sprachmodell muß vom Sprechenden kontinuierlich weitergepflegt werden. Es kann aber auch verhunzt werden: Andere Personen können nicht ""mal eben"" mit der Software arbeiten. Ähnliches gilt für Sprecherzustände, die die Stimmlage verändern. Alle Programme warnen davor, die Spracherkennung im Falle einer Erkältung einzusetzen. Auch die Akustik eines Raumes hat einen Einfluß auf die Spracherkennung: die Software eignet sich wohl nur für Steuerberater, die in einem einzigen Raum ihre Diktate aufnehmen. Zudem darf dieser Raum über keine extreme Geräuschkulisse verfügen: Denn die Erkenner versuchen, auch das Türenschlagen oder Telephonklingeln als Wort zu interpretieren.Diese gravierende Einschränkung für Großraumbüros soll indes bald fallen: IBM entwickelt zusammen mit dem Konkurrenten Lernout & Hauspie billige ""intelligente Mikrophone"", die die Nebengeräusche wegfiltern. Gegenwärtig kosten diese softwaregesteuerten Mikrophone um 4000 Mark und sind damit für den Normalsprecher viel zu teuer.Der ""Heilige Gral"" der Sprachforscher, die absolut nahtlose Integration der Spracheingabe in die tägliche Arbeit, liegt eben immer ein Stückchen in der Zukunft. DETLEF BORCHERS.(c) 1999 Süddeutsche Zeitung.Document sddz000020010910dv3h00sxc"
"114",2003-09-30,"VoiceGenie stellt Version 6.0 seiner VoiceXML-Plattform vor  ","VoiceGenie stellt Version 6.0 seiner VoiceXML-Plattform vor  1090 words30 September 200317:07PR Newswire EuropePRNWDEGermanCopyright (c) 2003 PR Newswire Europe Limited. Alle Rechte vorbehalten.  New York, September 30 /PRNewswire/ -- - Neue und verbesserte Funktionen u. a.: zentrales Clustermanagement in Echtzeit, erweiterter Support für ASR/TTS und ein besseres Anwendungsentwicklungspaket  VoiceGenie Technologies, der führende Anbieter von VoiceXML-Plattformen, Entwicklungsumgebungen und Tools für modernste Sprachtelefoniesysteme, hat heute den Verkaufsstart der Version 6.0 ihrer führenden Plattform auf Basis offener Standards bekannt gegeben. Die aktuellste Generation der Plattform wartet mit zahlreichen neuen Funktionen und Merkmalen auf, darunter Support für die neueste VoiceXML-Spezifikation, kompletter Gesprächsaufzeichnung, einem neuen MRCP-Client und Support für über 14 verschiedene ASR/TTS-Engines. Die ebenfalls neu hinzugekommene VoiceGenie Management Console (VMC) bildet einen integralen Bestandteil der Cluster Management Platform. Mit ihrer leistungsfähigen und gleichwohl intuitiven Weboberfläche können Sie mehrere Implementierungen auf den unterschiedlichsten Plattformen von einem zentralen Punkt aus in Echtzeit überwachen und verwalten - das verbessert die Automatisierung und Steuerung von Abläufen im Büro (Office Automation & Management, OA&M).  Die VoiceGenie-Plattform bildet schon heute die zugrunde liegende Infrastruktur für einige der weltweit umfangreichsten sprachbasierten Implementierungen von VoiceXML 2.0. Sie bewältigt derzeit bereits Millionen von Kundengesprächen täglich. Mit einem Höchstmaß an Skalierbarkeit, Flexibilität, Leistung und Zuverlässigkeit ermöglicht die VoiceGenie-Plattform die problemlose Entwicklung von Anwendungen sowie deren nahtlose Integration in nahezu jede vorhandene Umgebung. Damit werden Investitionen in neue und bestehende Anwendungen selbst dann geschützt, wenn die Umgebung sich weiter entwickelt.  ""Die neuen Funktionen in der Version 6.0 sind auch das Ergebnis des umfangreichen Kompetenzpools einer großen Versammlung unabhängiger VoiceXML-Entwickler, die sich in unserem Developer Workshop zusammengeschlossen haben. Hinzu kommen die Beiträge von über 100 Unternehmen und Telekommunikationsfirmen rund um den Globus"", erklärte Adrian Lee-Kwen, Vice President of Engineering bei VoiceGenie. ""Mit der erstmaligen Einführung der neuen VoiceGenie Management Console bieten wir umfangreiche Funktionen für die Verwaltung und Überwachung der verschiedensten VoiceGenie-Implementierungen. Wenn die Kunden die Aktivitäten des Systems von einem einzigen zentralen Punkt aus in Echtzeit überwachen können, sind sie auch in der Lage, unverzüglich auf problematische Situationen zu reagieren. Mit unserem erweiterten Support für 14 verschiedene ASR/TTS-Produkte sowie den verbesserten Entwicklungstools lösen wir unser Versprechen ein, die fortschrittlichste VoiceXML-Plattform am Markt für ausgeklügelte Sprachtelefoniesysteme anzubieten.""  Neue und verbesserte Funktionen in der VoiceGenie-Plattform Version 6.0 (u. a.):  Verbessertes Plattformmanagement  Die VoiceGenie Cluster Management Platform (CMP) ist ein praktisches Modul der VoiceGenie-Plattform zur besseren zentralisierten Verwaltung, Konfiguration und Überwachung aller Komponenten einer VoiceGenie-Implementierung samt entsprechendem Berichtswesen.  Neu in der Version 6.0 ist auch die VoiceGenie Management Console (VMC), ein Kernbestandteil der CMP, die sich in vorhandene Büroautomatisierungssysteme (OA&M) einbindet. Sie stellt eine mächtige und intuitive Weboberfläche für die zentralisierte und umfassende Verwaltung von Systemen jeder Größe bereit. Schlüsselfunktionen der VoiceGenie Management Console sind u. a.:  - Cluster-übergreifende Verwaltungsfunktionen zum Hinzufügen, Entfernen und Bearbeiten sowie Hoch- und Herunterfahren einzelner Server und Cluster sowie zur Verwaltung der Systemadministratoren und für das Cache Management.  - Zentralisierte Konfiguration von Servern und Clustern, u. a. Installieren bzw. Entfernen neuer Softwaremodule und Lizenzen sowie der Software dritter Anbieter.  - Anschlussüberwachung mit Gesamtübersicht über den Echtzeitstatus aller Kanäle der VoiceGenie-Server.  - Überwachung des Systemstatus mit detaillierten und aktuellen Informationen zu jedem Server, samt Prozessor- und Speicherauslastung.  - Berichtsfunktionen mit allen Gesprächsprotokollen und regelmäßiger Auswertung von Gesprächsaufkommen, -dauer, Anschlussbelegung und vielem mehr.  - Verbesserte SNMP-Verwaltung  MRCP-Client V1.0  Der neue VoiceGenie MRCP-Client stellt eine standardgemäße API bereit, welche die unkomplizierte Einbindung neuer ASR-Engines gewährleistet. Derzeit ist MRCP-Support für Loquendo, Nuance 8 und Scansoft (SpeechPearl) integriert.  Erweitertes Anwendungsentwicklungspaket  VoiceGenie 6.0 bietet eine umfangreiche Sammlung an Tools für die raschere und problemlosere Entwicklung von Sprachtelefonieanwendungen, u. a.:  - Genie IDE - ermöglicht das Debuggen vom Laptop aus - für noch schnellere Entwicklung von VoiceXML-Anwendungen mit Support für OSDMs. Mit VoiceGenie VoiceXML-Interpreter.  - GenieBuilder - eine visuelle Entwicklungsumgebung, die speziell für SpeechGenie optimiert wurde. Unterstützt die Entwicklung mächtiger Sprachtelefonieanwendungen sowie die Umstellung von eigenen IVR-Systemen (interaktive Sprachsteuerung).  - GenieFactory - ein grafisches Verwaltungstool für Sprachtelefonieanwendungen, welches die komplette IDE und die bewährte Serverarchitektur mit der VoiceGenie-Plattform zusammenführt.  - VoiceGenie Developer Workshop - Zugriff auf den jeweils aktuellsten Programmcode und die einzige Adresse für den Support von PSTN/VoIP und mehreren ASR/TTS-Engines auf einer einzigen Plattform. Die Anmeldung ist für Anwender kostenlos:  http://developer.voicegenie.com/[http://developer.voicegenie.com/] oder  http://speechgenie.developer.com/[http://speechgenie.developer.com/].  Support für PSTN, VoIP und Callcenter  VoiceGenie 6.0 unterstützt Umgebungen für PSTN (öffentliches Telefonnetz) und VoIP (Internettelefonie) sowie die handelsüblichen Telefonieschnittstellen und -protokolle, u. a. für Telefonanlagen. Weiterhin unterstützt die Plattform gemischte Umgebungen mit PSTN und VoIP, darunter auch solche Implementierungen, die Telefongespräche über das öffentliche Telefonnetz abwickeln und VoIP für die Signalübermittlung nutzen.  ""Mit dem Support für Umgebungen mit VoIP/SIP und PSTN können wir eine noch breitere Auswahl verschiedenster Architekturen und Implementierungen abdecken sowie die Grundlage für Distributed Speech Recognition (Verteilte Spracherkennung, DSR) und multimodale Anwendungen bereitstellen"", erklärte Mark Scott, Chief Technology Officer bei VoiceGenie Technologies.  VoiceGenie wird die VoiceGenie Management Console (VMC) auf der Fachmesse SpeechTEK an Stand 102 in der Praxis vorführen.  Informationen zu VoiceGenie  VoiceGenie Technologies Inc. ist der weltweit führende Anbieter von Lösungen für VoiceXML-Plattformen sowie Entwicklungsumgebungen und Tools für moderne Sprachtelefoniesysteme auf Basis offener Standards. VoiceGenies Plattform ermöglicht Unternehmen und Telekombetreibern die Entwicklung und Einführung anspruchsvoller IVR-Applikationen (Interactive Voice Response), sprachfähiger Dienste und Voice-Portale. Sie schafft damit weltweit die Voraussetzungen für unternehmenskritische Großanwendungen, die täglich Millionen von Anrufen bewältigen. Sie bietet ein Höchstmaß an Skalierbarkeit, Flexibilität, Leistung und Zuverlässigkeit, und ermöglicht so nahtloses Deployment in fast jeder Umgebung. Hierdurch werden Investitionen in neue wie vorhandene Anwendungen voll ausgenutzt, während sich die Umgebung gleichzeitig weiter entwickelt. VoiceGenie unterstützte als erster Anbieter am Markt die Spezifikationen VoiceXML 1.0 und 2.0. Als zahlendes Vorstandsmitglied des VoiceXML Forum und Mitglied im W3C unterstützt und fördert VoiceGenie kontinuierlich die Entwicklung dieses Standards.  Offiziell wurde VoiceGenie im Januar 2000 gegründet, die ersten Vorarbeiten am Kernprodukt der Firma fanden jedoch bereits 1995 statt. Gegenwärtig werden die Plattformlösungen von VoiceGenie von über 2000 Unternehmen in aller Welt eingesetzt so etwa AT&T, AIG, Air Canada, France Telecom, Merck, Mobilkom Austria, Orange, SBC Technology, Verizon, Vesta u.a. Unter den Partnern von VoiceGenie befinden sich führende Organisationen wie Audium, AT&T, BBN, Brooktrout, IBM, Intel, Loquendo, Nuasis, Oracle, Rhetorical, ScanSoft, VoiceObjects, Voxeo u.v.a. Nähere Informationen finden Sie unter  http://www.voicegenie.com/[http://www.voicegenie.com/].  VoiceGenie VoiceXML Gateway ist ein Warenzeichen der VoiceGenie Technologies Inc. Alle übrigen Warenzeichen sind Eigentum ihrer jeweiligen Inhaber.  Cheryl Alden, Marketing Manager, VoiceGenie Technologies Inc. Tel.: +1-416-736-0905, Durchwahl: 236, E-Mail: calden@voicegenie.com  PR Newswire Association, Inc.Document PRNWDE0020030930dz9u000rt"
"115",2003-09-30,"VoiceGenie und Loquendo arbeiten zusammen um sprachgesteuerte Dienste weltweit voranzubringen  ","VoiceGenie und Loquendo arbeiten zusammen um sprachgesteuerte Dienste weltweit voranzubringen  940 words30 September 200323:30PR Newswire EuropePRNWDEGermanCopyright (c) 2003 PR Newswire Europe Limited. Alle Rechte vorbehalten.  Toronto, Kanada und New York, September 30 /PRNewswire/ -- - Die Sprachsynthese und Spracherkennungstechnologie von Loquendo, die in der hochentwickelten VoiceGenie Sprachplattform steckt, soll leistungsfähige Lösungen rund um den Globus voranbringen  VoiceGenie Technologies Inc., weltweit führender Anbieter von VoiceXML Plattformen, Entwicklungsumgebungen und Tools für hochentwickelte Sprachsteuerungslösungen und Loquendo, global führendes Sprachtechnologie-Unternehmen, gaben heute bekannt, künftig zusammen arbeiten zu wollen. Die Unternehmen wollen technologisch und im Marketingbereich zusammenarbeiten, um anspruchsvolle sprachgesteuerte Dienste für Unternehmen und Netzbetreiber anbieten zu können. Der Integration von Loquendos innovativer Spracherkennungstechnologie in die VoiceXML Speech-Plattform von VoiceGenie wird progressivere und effektivere automatisierte Sprachtechnisch gesteuerte Lösungen auf dem Markt voran bringen.  Im gleichen Maße, wie die Spracherkennungstechnologie international wächst, wird die die Nachfrage nach kompatiblen und flexiblen Kommunikationssystemen mit effizienten Benutzerschnittstellen zunehmend wichtiger. Unternehmen und Netzbetreiber führen sprachgesteuerte Lösungen auf der Grundlage offener Standards ein, um neue Effizienz und benutzerfreundliche Zugänglichkeit gewährleisten zu können.  Loquendos sprachtechnologischer Ansatz bietet die für den Einsatz von Sprachdiensten jedweder Komplexität erforderliche Skalierbarkeit und Betriebssicherheit. Durch hochentwickelte, mehrsprachige technologische Komponenten wie die Sprecher-unabhängige Spracherkennungs-Engine, Loquendo ASR (Automatic Speech Recognition) sowie die multilinguale Loquendo TTS Software (TTS, Text-To-Speech) kann Loquendo die realistische, ""naturgetreue Stimme"" für die dynamischen Daten und Abfragen bieten, die von komplexen Sprachsteuerungen erwartet wird. Gekoppelt mit der VoiceGenie Plattform, bietet das Packet einen vollständigen Satz an einsatzbereiten Frameworks und Automated Agents.  ""Wir freuen uns sehr, die Allianz zwischen VoiceGenie und Loquendo verkünden zu können"" so Eric Jackson, Vice President, Strategy and Business Development bei VoiceGenie. ""Unsere sich ergänzenden Technologien versetzen Unternehmen in die Lage, überlegene Voice-Solutions zum Einsatz zu bringen. Sie ermöglichen die Anpassung von Kundenbetreuungsanwendungen an spezielle Anforderungen. Sie verbessern gleichzeitig in erheblichem Maß die Produktivität und die Ansprechempfindlichkeit gegenüber dem Kunden und helfen, die Kosten für die Unternehmen zu reduzieren.""  Sie Sprachplattform von VoiceGenie erlaubt es Unternehmen wie Netzbetreibern, hochentwickelte IVR-Anwendungen (IVR, Interactive Voice Response), sprachgesteuerte Dienstleistungen und Voice-Portale zu entwickeln und zum Einsatz zu bringen. Die Plattform ermöglicht die Einrichtung von VAD- (voice activated dialing, sprachgesteuertes Wählen) und anderer Sprachsteuerungsanwendungen. Dazu bietet sie hochentwickelte Infrastruktur-Lösungen und -Services, die zur Schaffung solcher Anwendungen notwendig sind. Zudem erlaubt die Plattform Benutzern, Webinformationen zu suchen und Online-Transaktionen durchzuführen. Personalisierte Kommunikationsdienste (wie E-Mail, Voice-Mail oder sprachgesteuertes Wählen) können ebenfalls, schnell und einfach, mit der eigenen Stimme ins Mikrophone eines beliebigen Telefons gesprochen, vom Nutzer verwaltet werden - jederzeit und überall.  Loquendos robuste ""naturnahe"" Synthese-Engine und der hochgenaue Spracherkennungstechnologie-Satz, die für die schnelle Integration in Unternehmens- und Netzbetreiber-Umgebungen konstruiert sind, stellen eine ideale Ergänzung zur offenen Speech-Plattform von VoiceGenie dar und ermöglichen hochentwickelte und verlässliche Audio-Processing und Voice-Verification Funktionen. By providing a simple and natural interface, customer satisfaction levels are increased tremendously through a more efficient response. Companies experience reduced call on-hold times and higher call completion rates, resulting in operational cost savings of up to 90%.  ""Bei Loquendo ist man hocherfreut über diese neue Partnerschaft, weil es uns damit möglich ist, unsere in Loquendo TTS und Loquendo ASR steckenden Technologien einem internationalen Publikum zugängig zu machen. Über VoiceGenies hochskalierbare und robuste Speech-Plattform können wir somit die Anforderungen einer breiten Bandbreite von Umgebungen weltweit anpassen"", so Rosanna Duce, Vizepräsidentin von Loquendo. ""Unsere Produkte üben einen Synergieeffekt aufeinander aus, da sie Unternehmen, die auf ultimative Kundenbetreuung aus sind, darin bestärken werden, diese neuen Wege mit uns zu gehen.""  - Informationen über VoiceGenie  VoiceGenie Technologies Inc. ist weltweit der führende Anbieter von offenen VoiceXML Plattform-Lösungen, Entwicklungsumgebungen und Tools. VoiceGenies Plattform ermöglicht Unternehmen und Telekombetreibern die Entwicklung und Einführung anspruchsvoller IVR-Applikationen (Interactive Voice Response), sprachfähiger Dienste und Voice-Portale. Sie schafft damit weltweit die Voraussetzungen für missionskritische Großanwendungen, die täglich Millionen von Anrufen bewältigen. Sie bietet ein Höchstmaß an Skalierbarkeit, Flexibilität, Leistung und Zuverlässigkeit, und ermöglicht so nahtloses Deployment in fast jeder Umgebung. Hierdurch werden Investitionen in neue wie vorhandene Anwendungen voll ausgenutzt, während sich die Umgebung gleichzeitig weiterentwickelt. Die VoiceGenie Plattform wurde mit mehreren unabhängigen Produktpreisen ausgezeichnet.  Offiziell wurde VoiceGenie im Januar 2000 gegründet, die ersten Vorarbeiten am Kernprodukt der Firma fanden jedoch bereits 1995 statt. Heute nutzen weltweit über 2000 Unternehmen die Gateway-Lösungen von VoiceGenie, so etwa AT&T, AIG, Air Canada, France Telecom, Merck, Mobilkom Austria, Orange, SBC Technology, Verizon, Vesta u.a. Unter den Partnern von VoiceGenie befinden sich führende Organisationen wie AT&T, BBN, IBM, Intel, Oracle, Rhetorical, Scansoft, Voxeo u.v.a. Nähere Informationen finden Sie unter  http://www.voicegenie.com/[http://www.voicegenie.com/].  - Informationen über Loquendo - Vocal Technology and Services  Mit über 30 Jahren Forschungsefahrung positioniert sich Loquendo in vorderster Front im weltweiten Speech-Technologies-Markt. Die qualitativ anspruchsvollen Hochleistungstechnologien (Loquendo TTS, Text-to-Speech-Engine, Loquendo Embedded TTS, Text-to-Speech-Engine, Loquendo ASR, automatische Spracherkennung) und Plattformen garantieren die besten Lösungen in 15 Sprachen. Gegenwärtig werden die folgenden Sprachen unterstützt britisches und amerikanisches Englisch, Italienisch, kastilisches Spanisch, Französisch, Deutsch, Brasilianisch, Portugiesisch, Mandarin, Griechisch, Mexikanisch, Chilenisch, Argentinisch, Schwedish, Katalanisch, und bald noch anderen Sprachen.  Vom Unternehmenssitz in Turin (Italien) ausgehend bringt Loquendo seine Technologien auf dem Telekommunikations- und Unternehmensmarkt zum Einsatz und verarbeitet damit 2.000.000 Anrufe täglich überall in der Welt.  Weitere Informationen finden Sie unter  http://www.loquendo.com/[http://www.loquendo.com/]Ansprechpartner Medien: Cheryl Alden, Marketing Manager, VoiceGenie Technologies Inc. Tel.: +1.416.736.6105. Say ""Cheryl Alden"", E-Mail: calden@voicegenie.com Gaea Vilage, Marketing Manager, Loquendo - Vocal Technology and Services, Tel.: +39 011.757.6153, E-Mail: gaea.vilage@loquendo.com Gaea Vilage, Marketing Manager, Loquendo - Vocal Technology and Services, Tel.: +39 011.757.6153, E-Mail:  PR Newswire Association, Inc.Document PRNWDE0020030930dz9u000xd"
"116",2011-04-18,"In einem exklusiven Interview mit SpeechTEK Europe spricht Googles Engineering Director Dave Burke über Smartphones, Android und Pläne für Sprachfunktionen im Internet","In einem exklusiven Interview mit SpeechTEK Europe spricht Googles Engineering Director Dave Burke über Smartphones, Android und Pläne für Sprachfunktionen im Internet913 words18 April 201122:45Business WireBWRGERGerman(c) 2011 Business Wire.  All Rights Reserved.   LONDON - (BUSINESS WIRE) - Dave Burke, Engineering Director bei Google, wird nächsten Monat auf dem europäischen Sprachtechnologie-Forum SpeechTEK Europe[http://cts.businesswire.com/ct/CT?id=smartlink&url=http%3A%2F%2Fwww.speechtek.com%2Feurope2011&esheet=6687335&lan=de-DE&anchor=SpeechTEK+Europe&index=1&md5=0328c7cfed8e72c99ed256e638e1064a] (London, 25. und 26. Mai 2011) die Eröffnungsrede halten. Während einer Frage- und Antwortrunde mit SpeechTEK Europe beschreibt Burke einige der Herausforderungen bei der Entwicklung von Sprachtechnologien für Android und äußert sich auch zur Akzeptanz der Sprachtechnologien bei Smartphone-Benutzern.Was ist Ihre Vision für das Mobiltelefon?Das Smartphone ist das bestimmende Kultprodukt unserer Zeit. Die Umstellung auf mobile Computer schreitet mit hoher Geschwindigkeit voran – die Akzeptanz des mobilen Internets wächst in den USA achtmal schneller als die der Desktoprechner Mitte der Neunzigerjahre. In zwei Jahren werden wir einen Wendepunkt erreichen – die Anzahl der Smartphones wird die Anzahl der verkauften PCs übertreffen. Bei Mobile Computing geht es nicht allein um die Geräte, sondern um eine allgegenwärtige Konnektivität. Ihr Telefon ist rund um die Uhr mit dem Internet verbunden, ganz gleich, wo Sie sich aufhalten. Dies hat einen weitreichenden Einfluss auf die Art, wie Menschen auf Informationen und Dienste zugreifen. Dialogbasierte Dienste werden einer reichhaltigen interaktiven Benutzererfahrung in Form von webbasierten und maßgeschneiderten Anwendungen Platz machen. Sprachfunktionen werden Teil der Anwendungen werden und mit anderen Eingabeformen wie Touchscreens harmonisch zusammenwirken.Android-Mobiltelefone unterstützen bereits Sprachtechnologie. Mit welchen neuen Fähigkeiten werden Android-Geräte durch die Arbeit Ihrer Gruppe ausgestattet?Wir haben hart daran gearbeitet, die Android-Sprachfunktionen mit jeder neuen Version der Plattform zu verbessern. Zu Beginn haben wir die sprachgesteuerte Suchfunktion entwickelt. Dann haben wir eine Funktion ergänzt, bei der ein Mikrofon im Tastaturfeld gesprochenen Text erfasst, der in jede Texteingabebox eingefügt werden kann. Im vergangenen Jahr haben wir unter dem Namen „Voice Actions“ ein neues Funktionsset eingeführt, mit dem eine Sprachsteuerung für komplexe Vorgänge mit mehreren Schritten möglich geworden ist. Beispiele sind Befehle zum Versenden von Textnachrichten und E-Mails, Anrufe bei Unternehmen oder Kontakten, das Abspielen von Musik oder die Einrichtung der Alarmfunktion. Für die nahe Zukunft können Sie weitere Funktionen und Sprachen, eine schnellere Spracherkennung und Synthese sowie API-Verbesserungen erwarten.Aus welchem Grund sollten Smartphone-Benutzer Sprachsteuerungen nutzen, wenn sie sich bereits daran gewöhnt haben, Informationen durch die Berührung des Android-Bildschirms einzugeben?Sprachfunktionen bedeuten eine zusätzliche Modalität für das moderne Smartphone. Sprachtechnologien fordern dem Benutzer eine geringere Aufmerksamkeit ab als herkömmliche Benutzeroberflächen. So ist es etwa möglich, eine Textnachricht schnell und mühelos zu versenden und dabei auf der Straße zu laufen. Auch in anderen Situationen, beispielsweise am Steuer, ist ein händefreier Bedienmodus eindeutig von Vorteil. Und trotz der erheblichen Verbesserungen der Touchscreens und Tastaturen ist es nach wie vor einfacher zu sprechen als zu tippen. Ich denke nicht, dass Sprache jemals andere Eingabemodi vollständig ersetzen wird, aber in vielen Situationen bedeutet die Möglichkeit, Anforderungen oder Befehle schnell per Sprache einzugeben, eine starke Aufwertung der Benutzeroberfläche des Geräts.An welchen anderen Sprachtechnologien arbeitet Google derzeit?Eine Plattform, auf der noch immer keine in großem Maßstab implementierten Sprachfunktionen verfügbar sind, ist das Internet. Im vergangenen Jahr gab es ein Treffen zwischen Google und den Unternehmen Microsoft, Nuance und Voxeo, um eine neue Speech XG Incubator Group innerhalb des World Wide Web Consortiums (W3C) zu gründen. Die Speech XG-Gruppe verfolgt das Ziel, HTML 5 um die Fähigkeit zu erweitern, Spracherkennung und -synthese für den Webbrowser nutzbar zu machen. Dabei soll es Webentwicklern so einfach wie möglich gemacht werden, ihre Anwendungen mit Sprachfunktionen auszustatten.Der Vortrag von Dave Burke auf der SpeechTEK Europe mit dem Titel Cloud-based Speech Recognition for Mobile and the Web[http://cts.businesswire.com/ct/CT?id=smartlink&url=http%3A%2F%2Fwww.speechtek.com%2Feurope2011%2Fkeynotes.aspx&esheet=6687335&lan=de-DE&anchor=Cloud-based+Speech+Recognition+for+Mobile+and+the+Web&index=2&md5=693c7af9005a3e040ca7fbb764c89de7] steht am Mittwoch, 25. Mai 2011, auf dem Programm.Die SpeechTEK Europe erwartet mehr als 50 Referenten aus allen Regionen der Welt und von einem breiten Spektrum an Unternehmen wie Google, Barclays Bank, Deutsche Telekom, Nuance, Loquendo, Openstream, Voxeo, Belgian Railways, Telecom Italia und Cable & Wireless.Das komplette Programm der SpeechTEK Europe 2011 ist hier erhältlich[http://cts.businesswire.com/ct/CT?id=smartlink&url=http%3A%2F%2Fwww.speechtek.com%2Feurope2011%2FProgramme.aspx&esheet=6687335&lan=de-DE&anchor=Programm+der+SpeechTEK+Europe+2011+ist+hier+erh%C3%A4ltlich&index=3&md5=58723c6fead0fa2bbe990ce509de9054]. Die Konferenz bietet eine Ermäßigung für Frühentschlossene (bis 29. April) sowie eine Ermäßigung von 40 Prozent für teilnehmende Kollegen.Hinweis für RedakteureEin Pressepass ist ab sofort für legitimierte Medienvertreter erhältlich. Bitte wenden Sie sich an caroline@infotoday.com[mailto:caroline@infotoday.com] oder füllen Sie das Online-Antragsformular aus.[http://cts.businesswire.com/ct/CT?id=smartlink&url=http%3A%2F%2Fwww.speechtek.com%2Feurope2011%2FPressZone.aspx&esheet=6687335&lan=de-DE&anchor=Online-Antragsformular+aus.&index=4&md5=026222d1ba49f820535031a4d69e2394]SpeechTEK Europe 201125. und 26. MaiSpeechTEK University Workshops 24. MaiCopthorne Tara Hotel, Londonwww.speechtek.com/europe2011[http://www.speechtek.com/europe2011]E-Mail: europe@speechtek.com[mailto:europe@speechtek.com]Tel.: +44 (0)1865 327813Über SpeechTEKDie Veranstaltungsreihe SpeechTEK (www.speechtek.com[http://www.speechtek.com]), die 1995 in den USA begann, genießt weltweite Anerkennung als führender Event für Produkte, Dienstleistungen, Anwendungen, Lösungen und Innovationen im Zusammenhang mit Sprachtechnologien. Die Messe- und Konferenzveranstaltung ist Anziehungspunkt für ein globales Netzwerk aus Sprach- und Technologieanbietern an sowie Unternehmen aus allen Regionen der Welt, die Sprachlösungen entwickeln und implementieren. Der SpeechTEK USA 2011 findet vom 8. bis 10. August im Hilton New York statt.Über Information TodayInformation Today (www.infotoday.com[http://www.infotoday.com]), Veranstalter der SpeechTEK und Herausgeber des Magazins Speech Technology, ist einer der führenden Business- und Technologie-Verlage sowie Konferenzorganisator. Außer Speech Technology veröffentlicht Information Today die Magazine Streaming Media, CRM, KMWorld, Database Trends and Applications, EventDV und eContent, sowie verschiedene Ressourcen für die globale Informationsgesellschaft.Die Ausgangssprache, in der der Originaltext veröffentlicht wird, ist die offizielle und autorisierte Version. Übersetzungen werden zur besseren Verständigung mitgeliefert. Nur die Sprachversion, die im Original veröffentlicht wurde, ist rechtsgültig. Gleichen Sie deshalb Übersetzungen mit der originalen Sprachversion der Veröffentlichung ab.Information Today Caroline Milner, +44 (0)1865 327813   caroline@infotoday.com[mailto:caroline@infotoday.com]Business WireDocument BWRGER0020110418e74i00105"
"117",2000-01-20,"Sprachsteuerung für Oszilloskope.","Sprachsteuerung für Oszilloskope.157 words1 May 2000epp Elektronik Produktion & PrüftechnikEPPGerman(c) 2000 Konradin Verlag.All Rights Reserved. For further information see http://www.epp-online.deDie Sprachsteuerungs-Option für die Hochleistungs-Oszilloskope Infiniium von Agilent ermöglicht, das Gerät über gesprochene Befehle zu steuern. Der Benutzer hat dadurch beide Hände für die Kontaktierung des Testobjekts frei. Er kann beispielsweise Meßparameter verändern oder Ergebnisse ausdrucken lassen, ohne den Tastkopf loslassen zu müssen, ideal auch bei Arbeiten an schwierig zu kontaktierenden Fine-Pitch-Strukturen. Die Sprachsteuerung reduziert die Kurzschlussgefahr beim Kontaktieren enger Boardstrukturen erheblich und spart außerdem Zeit. Der Sprachbefehlssatz umfasst englische Befehle mit denen sich die wichtigsten Gerätefunktionen steuern lassen. Das auf der Speech Recognition Engine ASR1600 von Lernout & Hauspie basierende Spracherkennungssystem arbeitet nicht personenspezifisch und erfordert keine Trainingsphase. Der Benutzer kann auf die vier am häufigsten benötigten Meßfunktionen und auf die Druckfunktion zugreifen. Falls er Hilfe beim Formulieren von Sprachbefehlenbenötigt, kann er die Online-Hilfe aufrufen.www.hp.com/go/bi[http://www.hp.com/go/bi]EPP 247(c) 2000 [Publisher: EPP Elektronik Produktion & Prüftechnik, Heft 5, 2000 @], [Year: 2000] (KOBRA).Document epp0000020020307dw51000um"
"118",2016-12-23,"Darum braucht man 2017 einen PC","Darum braucht man 2017 einen PC1233 words23 December 2016COMPUTERWOCHE OnlineCOMWOLGerman(c) 2016 COMPUTERWOCHE. www.computerwoche.de[http://www.computerwoche.de] Ein ganzes Bündel neuer Technologien verspricht neuen Aufwind für den Personal Computer. Wir sagen Ihnen, warum Sie auch 2017 einen PC brauchen.Personal Computer kaufen?Darum braucht man 2017 einen PCvon Florian?Maier?und?Agam?Shah?(US-Korrespondent IDG News Service)Einige Kritiker stehen wirklich total darauf, den Personal Computer regelmäßig als ""Dinosaurier"" abzutun. Aber: mobile und stationäre PCs sind auch 2016 wieder ein Stückchen sexier, leistungsfähiger und teilweise auch smarter geworden. Auf jeden Blue Screen of Death kommen so ganze Scharen technologischer Neuerungen, die den Personal Computer in eine neue Ära führen wollen. Virtual Reality, 4K Ultra-HD und 5G-Konnektivität sind nur der Anfang für ein großangelegtes Revival des Personal Computers. Vielleicht. Hier kommen die Top-Ten-PC-Trends für 2017.Personal Computer als holografische WearablesVirtual-, Augmented- und Mixed-Reality-Devices werden im kommenden Jahr in etlichen neuen Variationen, Formen und Größen aus dem Boden schießen. Einige dieser Geräte werden zu PCs ""zum Aufsetzen"". So wollen unter anderem Dell, Asus, Acer, Lenovo und HP neue Mixed-Reality-Headsets auf den Markt bringen, die den Usern die Interaktionen mit dreidimensionalen Objekten ermöglichen. Die virtuellen Objekte werden dabei ""über"" die reale Umgebung gelegt.Diese Devices werden eine völlig neue Ära der Mensch-Maschine-Interaktion herbeiführen in der es mehr Spaß denn je macht, 3D-Objekte zu kreieren, Games zu spielen, Filme zu sehen - oder interaktiv per Skype zu kommunizieren. Solche holografischen Computer - wie sie auch gerne genannt werden - werden Intel-Chips, integrierte GPUs und möglicherweise auch 3D-Kameras an Bord haben und völlig neue Perspektiven eröffnen.Speicher für PCs wird wieder teurerDie Preise von Solid State Disks werden wegen Lieferengpässen wieder anziehen. Das dürfte sich auch auf die Preise von Notebooks und 2-in-1-Devices auswirken. In den USA ist beispielsweise Dells XPS 13 mit Kaby-Lake-Chipsatz und 512 GB SSD derzeit nicht erhältlich, während andere Geräte mit 512 GB SSD zu teilweise völlig überhöhten Preisen angeboten werden.Die Standardgröße bei werksseitig verbauten Solid State Disks liegt bei den meisten PC-Herstellern zwischen 128 GB und 256 GB. Wenn Sie sich also 2017 einen Personal Computer zulegen wollen, sollten Sie sich unbedingt vorher Gedanken über Storage machen. Ein superdünnes Laptop-Tablet-Hybrid aufzuschrauben, um eine SSD auszutauschen ist nämlich kein Spass.Fatal-4-Way der digitalen Assistenten Die ""Blutfehde"" zwischen Apples Siri, Amazons Alexa, dem Google Assistant und Microsofts Cortana dürfte sich im kommenden Jahr weiter intensivieren. Windows-10-User dürfen Cortana künftig aus größerer Distanz Befehle zurufen: Dank einer sogenannten ""far-field speech recognition""-Technologie, an der Microsoft und Intel derzeit werkeln. Bis jetzt funktionierte Cortana am allerbesten, wenn man direkt vor dem PC sitzt. Durch diese Entwicklung könnte Microsofts digitaler Assistent allerdings plötzlich auch zum Konkurrenten für Amazon Echo werden. Wobei Cortana deutlich mehr kann als Echo: zum Beispiel auf die Cloud zugreifen, mit Chatbots interagieren oder E-Mails checken.Intel und AMD kämpfen wieder härterSeit mehr als einem Jahrzehnt ist Intel der unangefochtene King der Personal-Computer-Sparte. Nun will AMD mit dem Ryzen-Prozessor zurückschlagen, der 2017 auf den Markt kommen soll. Eine gesunde Rivalität zwischen Intel und AMD wird vor allem den PC-Usern zu Gute kommen - eventuell werden auch einige Nutzer das ""Lager"" wechseln. AMD jedenfalls behauptet, dass der Ryzen-Chip 40 Prozent schneller ist, als derzeitige PC-Chips. Auf dem Papier klingt das schon mal ziemlich beeindruckend.Zuerst dürften die Ryzen-Chips in Gaming-PCs Einzug halten, im weiteren Verlauf des Jahres auch bei Mainstream-Desktops und -Laptops. Dabei wird sich AMDs neuer Chip zunächst gegen Intels Kaby-Lake-Familie behaupten müssen, bevor der Marktführer dann in der zweiten Jahreshälfte seine neuen 10-nm-Cannonlake-CPUs enthüllt.Personal Computer loves ARM?Der erste Versuch, Personal Computer mit ARM-Chipsätzen zu vermarkten (Stichwort Windows RT), war nicht mehr als ein ausgewachsenes Desaster. Schlimmer noch: Viele User schlugen sich diese Idee hiernach aus dem Kopf. Aber Microsoft hat das Konzept immer noch nicht aufgegeben und sieht mit der Etablierung des 5G-Standards eine neue Chance für Windows-Notebooks mit ARM-Architektur. Der Windows-Riese hat angekündigt, dass Qualcomms ARM-Chip Snapdragon 835 2017 erstmals auch für Personal Computer zur Verfügung stehen wird. Die Vorteile: Die Geräte können superdünn gebaut werden und bieten trotzdem noch genug Platz für ein integriertes Modem. Die Akkuleistung dürfte durch die sparsamen ARM-CPUs um ein Vielfaches steigen. Die ARM-PCs sollen Win32-Anwendungen nutzen, die auf normalen x86-Geräten emuliert werden.Bislang hat allerdings noch kein PC-Hersteller ARM-basierte Windows-Geräte vorgestellt oder angekündigt. Das Windows-RT-Fiasko dürfte dabei auch eine nicht zu unterschätzende Rolle spielen. Zudem gibt es bei der Umsetzung auch einige Herausforderungen: Ein Snapdragon kann in Sachen Leistung nicht mit einem Highend-x86-Chipsatz von Intel oder AMD mithalten und wird standardmäßig auch keine 64bit-Applikationen unterstützen.PCs mit Fernbeziehungen dank Bluetooth 5Laptops, Notebooks und Hybrid-Devices werden 2017 mit der neuesten Bluetooth-Wireless-Spezifikation ausgestattet. Bluetooth 5 verspricht die kabellose Komunikation zwischen einem Personal Computer und verschiedensten Devices auf bis zu 400 Meter Entfernung. Laut Analysten ist eine Reichweite von 120 Metern in der Praxis allerdings realistischer. Daneben ermöglicht Bluetooth 5 gut die doppelte Übertragungsgeschwindigkeit seines Vorgängers: Bis zu 2 Mbps sollen drin sein.Schärferes sehen mit 4K und HDRGeräte wie Lenovos Yoga 910 oder Dells XPS 13 haben wunderschöne Edge-to-Edge-Screens zu bieten - ein Feature, das im nächsten Jahr speziell bei Laptops und Hybrid-PCs gefragt sein dürfte. Dazu werden Technologien wie 4K Ultra-HD und HDR (High Dynamic Range) unsere Games und Filme noch atemberaubender aussehen lassen.Insbesondere HDR ist stark im Kommen und sorgt für lebhaftere Bilder und intensivere Farben. So will beispielsweise Netflix sein HDR-Angebot 2017 deutlich ausweiten. Der ""Formatkrieg"" der bezüglich HDR zur Zeit zwischen DolbyVision und HBR3 entbrannt ist, müssen Sie nicht fürchten: Die Hersteller der Grafikchips unterstützen beides.Neue Speichertechnologien für den PCIntel hat mit seiner ""Optane""-Technologie ein heißes (und superschnelles) Speicher-Eisen im Feuer, das Storage und Memory endlich zusammenführen könnte. Das könnte wiederum zu einer fundamentalen Veränderung der Architektur eines Personal Computers führen. Aber bis dahin werden noch einige Jahre ins Land ziehen - bisher übt man sich bei Intel in Sachen Optane zudem in Bescheidenheit.Die Optane-Technologie basiert auf einer anderen Technologie namens 3D Xpoint, die Intel zusammen mit Micron entwickelt hat. SSDs mit dieser Technologie werden bereits ab 2017 erhältlich sein.Nicht ohne mein KeyboardSchon 2016 haben wir einige interessante Entwicklungen im Bereich der PC-Tastaturen erlebt: Apple hat die Touch Bar erfunden, Lenovo bei seinem Yoga Book gleich das ganze Keyboard virtualisiert. Dieses Touch-Keyboard soll - wenn es nach Lenovo geht - künftig wegen seiner Vielseitigkeit in einigen Chromebooks und auch 2-in-1-Devices zum Einsatz kommen.Ein Vorteil der Lenovo-Tastatur: Sie erlaubt auch den Einsatz eines Stylus - etwa zum Zeichnen oder Notizen machen. Lenovo geht davon aus, dass ""Smartphone-Tipper"" das neue vielseitige Keyboard schnell adaptieren werden, während Liebhaber mechanischer Oldschool-Tastaturen nur schwer zu überzeugen sein werden.Schwerer Port-AbschiedDie PC-Hersteller werden sich wohl nicht trauen, Klinkenbuchse und SD-Kartenslots von heute auf morgen aus dem PC-Gehäuse zu verbannen - aber zumindest USB 2.0-Ports könnten 2017 endlich verschwinden. Einige Hersteller verzichten auch zu Gunsten von USB-C auf Legacy-Ports: Diese sind vielseitiger und ermöglichen nicht nur das Aufladen von Geräten, sondern auch die Verbindung mit externen Displays oder Storage Devices.Mit Material von IDG News Service.IDG Communications Verlag AGDocument COMWOL0020161223eccn00007"
"119",2003-06-20,"Ein BMW Z 8, wie Bertone ihn sieht.","Ein BMW Z 8, wie Bertone ihn sieht.Von Roger Gloor.1035 words6 March 2003Automobil RevueAUTOR32German© 2003 Automobil Revue. Erste europäische Automobilzeitung, gegründet 1906. Wöchentlich neue Testberichte von unseren unabhängigen Schweizer Testingenieuren. Alle Rechte vorbehalten.Für die Erstpräsentation auf dem Genfer Salon hat die Design-undEntwicklungsfirma Carrozzeria Bertone SpA den Concept-car Birusa gebaut, einSportcoupé der (elektronischen) Superklasse. Es basiert auf dem offenenSportzweisitzer BMW Z8.Birusa stammt aus dem Piemonteser Dialekt, wird «Biruss» ausgesprochen undist da Synonym für eine besonders aktive Persönlichkeit. Aber trotz dieseslokalen Rückhalts haben die Bertone-Designer in ihre jüngste SchöpfungBMW-markentypische Formdetails einfliessen lassen. Mit der Münchner Markeverbindet Bertone ohnehin eine bis auf das Jahr 1960 zurückreichendeTradition.Bertone schuf in den Sechzigerjahren BMW-Limousinenstudien und produziertefür BMW von 1961 bis 1965 das Luxus-coupé 3200 CS. Nach diesem in 597Exemplaren entstandenen Sammelstück folgten die ebenso sportlichenConcept-cars Spicup (1969), Garmisch (1970) und Pickster (1998). Mit demBirusa wird nun ein neuer Schritt Richtung München vollzogen.Klassisches KonzeptDer Birusa entspricht einem klassischen Sportwagenkonzept mit vorneliegendem Motor, zweisitzigem Cockpit und angetriebenen Hinterrädern. AlsUnterbau diente die Plattform des BMW Z8, der mit seinem 400 PS leistenden4941-cm3-V8 zu den Spitzenmodellen seiner Kategorie zählt. Der Radstandwurde für den Birusa allerdings von 250,5 auf 285 cm gestreckt, während dieGesamtlänge von 440 cm unverändert blieb.Diese eher ungewohnten Proportionen zeigen sich in den äusserst knappgehaltenen Überhängen. Statt der aerodynamisch horizontal zugespitzten Frontmit unter Klarglas abgedeckten Scheinwerfern hat der Birusa eine Steilfrontmit senkrecht stehendem Lufteinlass-Mittelteil. Hinten endet die Karosserieim Oberteil mit dem hintersten Reifenteil, an den sich das Radhaus nur nochganz knapp anschliesst.Der Bertone Birusa ist als Fastbackcoupé konzipiert. Die obere Fensterliniefolgt konsequent dem Dachverlauf, der bereits am oberen Ende dergrossflächigen Windschutzscheibe seinen Kulminationspunkt erreicht. Dieuntere Fensterlinie jedoch biegt sich elegant nach unten und sorgt für einetropfenförmige Seitenfenstersilhouette.Das Resultat ist ein aggressiv-dynamisch wirkendes Erscheinungsbild. Dieeinzelnen Stylingelemente ergänzen sich und erbringen ein Gesamtdesign, dasan keine bestehenden Sportwagenkreationen erinnert.Der Zugang erfolgt - wieder einmal - durch Flügeltüren. Diese sind ausGewichtsgründen aus Kohlefasermaterial hergestellt. Sie schliessen sichunmittelbar an die Windschutzscheibe an und werden elektrisch bedient.Anstatt Aussenspiegel wurden zwei Videokameras vorgesehen.Ein mobiles GlasdachEine konstruktive Besonderheit wird von der Heckscheibe verkörpert. Dieseist nämlich Teil eines zweiteiligen gläsernen Sonnendachs, das sichvollumfänglich unter den Kofferdeckel verschieben lässt. Der Birusa istsomit eine Art Targacoupé. Bei geschlossenem Dach sorgen die laminiertenSicherheitsglasflächen für ein lichtdurchflutetes Interieur, halten jedochdie UV-Strahlung ab.Dieses von der Firma Inalfa entwickelte Dachsystem arbeitet mit zweiElektromotoren. Sie bewegen die beiden Glasdachteile über Kabel inspeziellen Schienen und mit einer automatischen Kippvorrichtung unsichtbarin den Kofferraum, wobei eine elektronische Steuerung für die korrekteAbfolge sorgt. Die Bedienung kann auf Knopfdruck, durch Fernsteuerung oderauch auf Stimmbefehl erfolgen.Für die Verwirklichung der beiden Glasdachteile wurden von der Firma SocarICS höchste Präzisionsanforderungen erfüllt. Es schliesst ebenso perfekt,wie es sich genau bewegt. Hierzu wurde eine einzige grosse Glasflächehergestellt, die darauf mit der Wasserstrahlmethode mit 4000 AtmosphärenDruck zweigeteilt wurde. Die Seitenfenster wurden ebenfalls aus einemEinzelstück herausgeschnitten. Ihrer Einfärbung diente die neueChromaticbeschichtung von Solutia.Dieses Solutia-Glas - übrigens zeigen sich hier ebenfalls die mitConcept-cars gebotenen Propagierungsmöglichkeiten für neue technischeProdukte - bietet Vorteile, die über das Design bzw. die Farbgebung hinausgehen. So ist dieses Glas bis 10 % leichter als jenes herkömmlicherSeitenfenster. Zudem nimmt die Beschichtung im Kollisionsfall einen gu-tenTeil der Energie auf und reduziert die Splittergefahr. Zudem soll sie gemässHersteller zu einer verbesserten Geräuschdämpfung beitragen und dieUV-Strahlen zu 95 % zurückhalten.Für SprAchkommandiAutomobile, die sich buchstäblich «ansprechen» lassen, sind an sich nichtneu. In etlichen Concept-cars wurde die Zahl der bisher via Fahrerstimmeaus-lösbaren Funktionen Schritt um Schritt erweitert. Im Birusa wurde dasVoxDrive-System der Firma Loquendo eingebaut. Diese raffinierte, «AutomaticSpeech Recognition and Text To Speech» geheissene Anlage erkennt die Stimmedes Fahrers und von weiteren als «berechtigt» geltenden Personen.Das auf mehrere Sprachen ausrichtbare System vermag verschiedene Befehleauszuführen und kann zudem (in der Sprache des Fahrers) antworten. SeineDialogfähigkeit führt mitunter dazu, dass das System bei kritischen Befehlennoch eine Bestätigung verlangt. Es ist auch in der Lage, etwa von Kindernausgegebene Kommandi nur in limitiertem Umfang entgegenzunehmen. Damitausgesprochene Worte nicht ungewollt als Kommando ausgelegt werden, muss zurAktivierung des Systems ein Knopf gedrückt werden.Die leistungsfähige Stimmtechnologie von Loquendo ist das Resultatdreissigjähriger Entwicklungsarbeit. Sie wurde auf die Benutzung im Auto mitseinem stets wechselnden Geräuschhintergrund angepasst. Mit Sprachkommandilässt sich nicht nur die Handhabung vereinfachen, sie können auch derKommunikation mit der Aussenwelt dienen. So lassen sich etwa beientsprechender Anbindung auf einfachste Weise Verkehrs-auskunftsstellenaufrufen.Segway HT mit dabeiErstmals wurde der Segway Human Transporter (HT), dieses ungewohnte,originelle Kurztransportmittel, sozusagen in das Konzept eines Automobilsintegriert. Der batteriebetriebene Seg way - ein elektrischesParallelrad-trottinett - dient umweltschonend dem individuellen Nahverkehr.Er ist optimal manövrierfähig und nur minim breiter als die Schultern einer«Durchschnittsperson». Bertone motivierte Segway zur Entwicklung einerBirusa-kompatiblen Version im Sinne einer sich ergänzenden Transportlösung.Dieser spezielle Segway wurde von Bertone in einem zum Birusa passenden Stileingekleidet. Hierzu dienten Schaumstoff und Alcantara-Überzüge, wie siesich auch im Birusa finden. Und wie dieser ist der Segway auch mitBeleuchtung, einer Navigationseinheit und einem Bose-Stereosystemausgerüstet! Da wird man sich ja bis zum Ziel, selbst wenn es «im engstenKorridor» liegt, nicht mehr verirren können ...Raffinessen auch im Kofferraum: Dieser bietet eine Schublade für leichtesBe-und Entladen. Ganz nach Wunsch kann diese Schublade aber auch als Rampefür den Segway dienen! Das Bose-System für den Birusa wurde ganz speziellauf dessen Innenraum ausgerichtet. So konnten beispielsweise zwei Bassmoduleauf kleinstem Raum unter den Sitzen platziert werden. Insgesamt liessen sichelf Neodymium-Lautsprecher dank ihrer Kleinheit und ihres leichten Gewichtsauf ideale Weise unterbringen.Im Cockpit herrscht pure Sportlichkeit. Die beiden anatomisch ausgeformtenSitze wer-den durch eine Mittelkonsole getrennt, auf deren vorderem En-deein zusätzliches Instrumentarium aufgebaut wurde. Immer wenn dieelektrischen Flügeltüren bewegt werden, gleiten auch sie automatisch zurückbzw. wieder nach vorne. Die Innenraumauskleidung wurde mit der neuenAbteilung Trend & Design der Alcantara SpA ausgearbeitet. Diese hatgleichzeitig einen neuartig geprägten Alcantara-Überzug beigesteuert.Büchler Grafino AGDocument autor00020030305dz3600011"
"120",2000-08-23,"Nortel Networks schließt Lizenzvertrag über die Text-to-Speech-Engine von Fonix ab.","Nortel Networks schließt Lizenzvertrag über die Text-to-Speech-Engine von Fonix ab.719 words23 August 200016:12Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS""Bohemia, N.Y., und Salt Lake City (ots-PRNewswire) - Nortel Networks* und die Fonix Corporation (OTC Bulletin Board: FONX) gaben die Unterzeichnung eines OEM-Lizenzvertrages bekannt, durch den Nortel Networks Unterlizenzen für die Text-To-Speech-Produkte (TTS) von Fonix an seine Kunden in aller Welt vergeben wird, mit denen die Computer-Telefonie integriert und verbesserte Dienstleistungen für Telekommunikationsnetzwerke angeboten werden können. Durch diesen Vertrag wird die TTS-Technologie von Fonix das derzeitige Angebot von Nortel Networks mit Technologien und Produkten wie Interactive Voice Response (IVR), Voiceportal, Spracheingabe, Nachrichtenübermittlung, Fax und Web-Browser ergänzen. Nortel Networks unterstützt Unternehmen dabei, profitable Kundenbeziehungen zu beginnen, zu behalten, zu unterhalten und auszubauen, indem der nahtlose Kontakt zu den Kunden über alle Berührungspunkte ermöglicht wird - über das Web, per E-Mail, Telefon, Telefax, persönlich, am Stand und über Handgeräte. Die Produkte und Dienstleistungen werden genutzt, um das Hochleistungsinternet zu fördern, das im Hintergrund die Transaktionen für Aufbau, Pflege und Erweiterung der Beziehungen online und offline bewältigt.Fonix TTS ist eine qualitativ hochwertige Text-to-Speech-Maschine, die über praktisch unbegrenzte, benutzerdefinierte Wortschatzkomponenten und ein anpassungsfähiges Benutzer-Wörterbuch verfügt, so dass bei Bedarf auch ungewöhnliche Wörter aufgenommen werden können. Zu Fonix TTS gehört auch ein hochwertiges FAAST (Fonix Accelerated Application Solutions Technology) API-Set, durch das Text-to-Speech unkompliziert in elektronische Produkte und Lösungen wie VCR, Voiceportale, Call-Center, Help Desks, Handgeräte oder Elektronik für den Verbraucher eingebunden werden kann.Thomas A. Murdock, President und Chief Executive Officer der Fonix Corporation, erklärt hierzu: ""Fonix TTS ist die TTS-Technologie mit einer Stimmqualität, die zu den besten gehört. Es ist unser vorrangigstes Ziel, unseren Geschäftspartnern und Kunden Speech-Produkte in hervorragender Qualität zu liefern. Der Zusammengang mit Nortel Networks bietet Fonix die Möglichkeit, seinen Anteil am internationalen Markt zu vergrößern, und Nortel Networks kann so seinen Kunden die besten TTS-Produkte zur Verfügung stellen.""Fonix CorporationDie Fonix Corporation (OTC Bulletin Board: FONX) ist ein führender Anbieter von Human-User-Interface (HUI) Technologien und Lösungen für schnurlose und mobile Geräte, Internet-und Fernsprechsysteme sowie Fahrzeug-Telematik. Führende Chip-Hersteller, unabhängige Soft-und Hardware-Verkäufer und Anbieter von Internet-Content und -Dienstleistungen beziehen die Technologie von Fonix mit ein, um ihren Kunden einfachere und benutzerfreundlichere Lösungen und damit größere Zufriedenheit bieten zu können. Zu den Fonix-Produkte gehören Text-To-Speech (TTS), Automatic Speech Recognition (ASR) und Handwriting Recognition (HWR). Diese Produkte können einfach in Mehrfach-Mikroprozessoren und Betriebssysteme implementiert werden und bieten unter den derzeit verfügbaren Kommunikationslösungen die wirksamste an. Mehr Informationen erhalten Sie, wenn Sie uns unterwww.fonix.com[http://www.fonix.com/]besuchen oder die Nummer +1 (801) 553-6600 anrufen.Verkaufs-und Produktinformationen erhalten Sie von Scott Lindsey, Tel: +1 (801) 553-6600, sales@fonix.comMedia Information: Bitte wenden Sie sich an Kurt Herrmann, +1 (801) 553-6600, mediarel@fonix.com.Investor Information: Bitte wenden Sie sich an Michelle Aamodt, +1 (801) 328-0161 invrel@fonix.com.Anmerkung: Die Angaben der Fonix Corporation sind nicht rein rückblickend, sondern auch zukunftsbezogen im Sinne der Vorbehaltsklauseln gemäß ""Private Securities Litigation Reform Act"" von 1995, und enthalten auch Aussagen bezüglich der Erwartungen, Hoffnungen, Absichten und Strategien des Unternehmens für die Zukunft. Investoren werden darauf aufmerksam gemacht, dass zukunftsbezogene Erklärungen Risiken und Unwägbarkeiten in sich bergen, die Auswirkungen auf die Geschäftsaussichten und Leistungen des Unternehmens haben können. Es ist wichtig zu berücksichtigen, dass die tatsächlichen Ergebnisse des Unternehmens erheblich von den zukunftsbezogenen Aussagen abweichen können. Risikofaktoren, einschließlich allgemein wirtschaftlicher, wettbewerblicher, staatlicher und technologischer Faktoren werden in den Formblättern 10-K, 10-Q und 8-K dargelegt, die in den Akten der SEC (Börsenaufsichtsbehörde) hinterlegt sind. Das Unternehmen übernimmt keinerlei Verantwortung bezüglich der Aktualisierung der zukunftsbezogenen Aussagen aus dieser Meldung.*Nortel Networks ist eine Handelsmarke von Nortel Networks.ots Originaltext: Fonix Corporation Im Internet recherchierbar:http://recherche.newsaktuell.de[http://recherche.newsaktuell.de]Rückfragen bitte an: Handels-und Produktinformationen, Scott Lindsey, Tel: +1 801-553-6600, sales@fonix.com, Medieninformationen, Kurt Herrmann, Tel: +1 801-553-6600, mediarel@fonix.com, oder Investor-Informationen, Michelle Aamodt, Tel: +1 801-328-0161, invrel@fonix.com, alle bei Fonix Corporation Website:http://www.fonix.com[http://www.fonix.com]*** OTS-ORIGINALTEXT UNTER AUSSCHLIESSLICHER INHALTLICHER		 VERANTWORTUNG DES AUSSENDERS ***OTS216    2000-08-23/16:11.Document aupag00020010804dw8n005o5"
"121",2013-02-28,"IBM MobileFirst bündelt Mobile-Expertise auf neuer Plattform","Umfassendes Portfolio aus Produkten und Services für das mobile UnternehmenIBM MobileFirst bündelt Mobile-Expertise auf neuer Plattform873 words28 February 2013IT-BUSINESS NEWS OnlineITBUSWGermanCopyright 2013. Vogel Business Media GmbH & Co. KG   Branchenriese IBM fasst sein gesamtes Mobile-Portfolio künftig auf einer neuen Plattform IBM MobileFirst zusammen. Hier sind neue Lösungen zur mobilen Sicherheit und Analytik sowie Software zur Anwendungsentwicklung mit Cloud-basierten Services vereint.Mit IBM MobileFirst sollen Unternehmen sämtliche mobilen Prozesse steuern und optimieren können – von der Koordination der mobilen Endgeräte der Mitarbeiter bis hin zur Entwicklung kommerzieller Apps. Dabei sind integrierte Strategien für die Bereiche Mobility, Cloud, Big Data, Social Business und Sicherheit gefordert, um Unternehmen dahin gehend zu unterstützen, maßgeschneiderte mobile Strategien zu entwickeln und die passenden Lösungen zu implementieren. IBM liefert hierfür seinen Kunden Tools und Services, damit diese mit mobilen Lösungen neue Geschäftschancen entwickeln können.Breitgefächertes Angebot an mobilen LösungenDas Lösungsportfolio von IBM soll die Grundbausteine für mobile Plattform-Lösungen und Services für das Management mobiler Anwendungen sowie für mobile Sicherheits- und Analytics-Komponenten liefern. Die cloud-basierte flexible Infrastruktur soll Social- und Cloud Services sowie Back-End-Technologien für die Überwachung der Prozesse in den Unternehmen integrieren und bereitstellen. Das Mobile-Portfolio umfasst hierfür folgende Komponenten:IBM MobileFirst Platform: Updates erweitern die Funktionalitäten von IBM Worklight und vereinfachen die Anwendung. Möglich ist jetzt außerdem das Single Sign On für mehrere Anwendungen. Eine neue Beta-Version von Rational Test Workbench verbessert zudem die Qualität und Zuverlässigkeit von mobilen Apps.IBM MobileFirst Security: IBM baut hier seine kontext-basierten Sicherheitslösungen aus, indem beispielsweise AppScan um neue Funktionen zum Aufspüren von Sicherheitslücken erweitert wird, die kompatibel zu allen aktuellen Versionen der geläufigen Betriebssysteme sind.IBM MobileFirst Managementbeinhaltet neue Updates für den IBM Endpoint Manager sowie einen verbesserten Support für alle Bring-Your-Own-Device-(BYOD)-Programme und erhöhte Sicherheitsstandards, die vor allem für Ministerien oder das öffentliche Verwaltungswesen sowie andere, stark regulierte Wirtschaftsbereiche von Vorteil sind.Mit IBM MobileFirst Analyticserweitert IBM seine Tealeaf CX Mobile Lösung, mit der es möglich ist, die Reaktionen eines Kunden, der mobil auf die Webseiten des Unternehmens zugreift, nachzuvollziehen und auszuwerten. Auf dieser Grundlage können Schwachstellen besser erkannt, Verbesserungen vorgeschlagen und für ein nachhaltiges Nutzerverhalten beim Einsatz mobiler Endgeräten gesorgt werden.Alle genannten Lösungen können auch über Cloud- oder Managed Services implementiert werden, was Firmen maximale Flexibilität ermöglicht.Mobile Services für KundenDas IBM MobileFirst Portfolio umfasst zusätzlich eine Reihe von Services, die Kunden dabei unterstützen, mobile Strategien zu entwickeln, sie zu designen und zu implementieren. Hierzu gehören:Innerhalb der IBM MobileFirst Strategy and Design Serviceswerden unternehmensweite mobile Geschäftsstrategien, Konzepte und Designs entwickelt, bei denen sowohl existierende strukturelle Grundlagen berücksichtigt wie auch das passende Portfolio mobiler Interaktionen, Designelemente und Funktionalitäten auf Basis geeigneter mobiler Infrastrukturen zusammengestellt werden. Wesentliches Ergebnis ist dabei eine umsetzbare Roadmap zu einem „MobileFirst Enterprise“, das mobile Geschäftschancen optimiert und das time-to-market beschleunigt. Methodisch bewährte innovative Workshops unterstützen dabei, die passenden Lösungen zu gestalten.Die IBM MobileFirst Development and Integration Servicesbeinhalten verschiedene Komponenten zur Implementierung mobiler Infrastrukturen und die Koordination von BYOD-Umgebungen. Verbesserte Netzwerk Infrastruktur Services bieten passende IT-Netzwerk-Strategien, erleichtern zudem die Optimierung, die Integration und das Management. Mobile Enterprise Services für Managed Mobility helfen, Smartphones, Tablets und andere Endgeräte innerhalb eines Unternehmens zu verwalten und zu sichern. Das Mobile Application Platform Management beschleunigt den Einsatz mobiler Plattformen, um Anwendungen leichter und schneller entwickeln zu können.Die Mobile & Collaboration Services– moderne Collaboration-Infrastrukturen – bringen Anwendungen wie Voice, Video und Chat auf einem virtuellen Desktop in eine einheitliche Infrastruktur. Dies erlaubt integrierte Echtzeitkommunikation in einer Cloud-Umgebung unter dem Einsatz mobiler Endgeräten. Latenzzeiten durch mangelnde Performance sollen dabei ausgeschlossen sein.Laut dem aktuellem Tech Trends Report der IBM besitzt nur eine von zehn Firmen das nötige Know-how, um neue Technologien wie Mobile Computing effektiv anzuwenden. Um diese Wissenslücke zu schließen, stellt IBM zusätzliche Instrumentarien für Geschäftspartner, Entwickler und Studenten bereit.Mobile Ressourcen und ProgrammeSo soll die neue Partnerschaft mit AT&T Entwicklern ermöglichen, mobile Apps mit IBM Worklight zu verbessern und über die Cloud Zugriff auf APIs von AT&T zu haben. So können Programmierer ab sofort Apps mit vielseitigen Features entwickeln, beispielsweise Speech Recognition oder Rapid Payment. IBM nutzt die Plattformen „developerWorks“ und „CodeRally“, beides Communities für Entwickler, um neue technologische Assets zur Verfügung zu stellen, mit denen bestehende Lücken im Know-how geschlossen werden können.Geschäftspartner, insbesondere Independent Software Vendors (ISVs) können mit Hilfe von Ready for IBM MobileFirst mobile Technologien in ihre Lösungen integrieren. Mit Software Value Plus haben sie zudem ab sofort Zugriff auf Zertifizierungen, Workshops und Incentives für Reseller und Systemintegratoren, bei denen mobile Lösungen im Mittelpunkt stehen.Um die Ausbildung einer neuen „Mobile Generation“ an Universitäten und Fachhochschulen zu fördern, bietet IBM neue Fachbereichsstipendien und die Entwicklung neuer Lehrpläne an. IBM stellt außerdem Worklight in Seminarräumen und über Online-Trainings kostenlos zur Verfügung, um Studenten und dem Lehrpersonal den Einstieg in mobile Plattformen zu erleichtern.Mit MobileFirst will IBM Unternehmen bei der Entwicklung maßgeschneiderter mobiler Strategien und Lösungen unterstützen und sie bei ihrer Entwicklung hin zu einem mobilen Unternehmen als verlässlicher Partner begleiten.Bild: drubig-photo, FotoliaMit MobileFirst will IBM Unternehmen bei der Entwicklung maßgeschneiderter mobiler Strategien und Lösungen unterstützen und sie bei ihrer Entwicklung hin zu einem mobilen Unternehmen als verlässlicher Partner begleiten.Bild: drubig-photo, FotoliaVogel Business Media GmbH & Co. KGDocument ITBUSW0020130228e92s00003"
"122",2012-06-20,"Haus der Barmherzigkeit: Wir erinnern uns gerne an morgen","Haus der Barmherzigkeit: Wir erinnern uns gerne an morgen363 words6 September 201210:53Austria Presse Agentur-OTSAUPAGGermanOTS - ""ORIGINAL TEXT-SERVICE UNTER VERANTWORTUNG DES AUSSENDERS""   LOWE GGK wirbt für Zukunftsprojekte der PflegeeinrichtungWien - BewohnerInnen des Haus der Barmherzigkeit blicken durch einen alten, goldenen Bilderrahmen in ihre Zukunft. Dabei wird die Asymmetrie der Zeit aufgehoben: Wir erinnern uns gerne an morgen. ""Wer in der Zukunft lesen will, muss in der Vergangenheit blättern"" - dieser Spruch von André Malraux leitet das erste Anzeigensujet der Werbekampagne ein und beschreibt gleichzeitig deren Inhalt: Innovative Zukunftsprojekte, die die gemeinnützige Pflegeeinrichtung Haus der Barmherzigkeit plant und umsetzt.Knowhow gibt das Haus der Barmherzigkeit (HB) gerne weiter: Eines der Zukunftsprojekte ist der Generationen-Campus, Kindergarten, Fachhochschule und Pflegeeinrichtung an einem Standort - das ist die Idee am alten Stammsitz des HB in der Vinzenzgasse. Studierenden der MedUni Wien erfahren im ersten Semester den empathischen Umgang mit PatientInnen, vor allem auch schwer kranker und sterbender sowie mehrfachbehinderter Menschen. Das HB bietet außerdem Langzeit-Betreuung für psychisch kranke alte Menschen.Für professionelle Pflege mit Lebensqualität baut das HB aus: Ab Herbst 2014 eröffnet das Stephansheim mit 140 Betten in unmittelbarer Nähe des Krankenhaus Horn. Und das HB forscht: Ein Pflegeroboter soll alleinlebende Menschen künftig unterstützen, möglichst lange selbstständig zu Hause zu leben. Auch Emotional Speech Recognition ist eine Idee für die Zukunft: Es wird dann schon an der Stimme erkennbar sein, ob akuter Betreuungsbedarf besteht.Der neue Werbeauftritt erzählt die Zukunft des Haus der Barmherzigkeit und seiner BewohnerInnen wie ein modernes Märchen. Geschaltet werden sechs großformatige Anzeigensujets in den Tageszeitungen Der Standard und KURIER. Kampagnenstart ist Anfang September sowie erstmals auch Online-Werbung auf derstandard.at. Ergänzend wird ein vorweihnachtliches Großplakat von EPAMEDIA affichiert.Übrigens: Das Haus der Barmherzigkeit finden Sie im Internet unter www.hausderbarmherzigkeit.at/news-presse/unsere-werbung/2012/[http://www.hausderbarmherzigkeit.at/news-presse/unsere-werbung/2012/]www.facebook.com/hausderbarmherzigkeit[http://www.facebook.com/hausderbarmherzigkeit]Credits: Auftraggeber: Haus der Barmherzigkeit, Prim. Univ.-Prof. Dr. Christoph Gisinger & Mag. Eva Bauer Agentur: Lowe GGK Gestaltung: Walther Salvenmoser Bilder: Dieter Brasch Bildbearbeitung: Blaupapier/ Helmut KanskyRückfragehinweis:   Haus der Barmherzigkeit   Mag.(FH) Corinna Dietrich   Tel.: +43/1/40199 DW 1322   mailto:corinna.dietrich@hausderbarmherzigkeit.at   http://www.hausderbarmherzigkeit.at[http://www.hausderbarmherzigkeit.at]Digitale Pressemappe: http://www.ots.at/pressemappe/104/aom[http://www.ots.at/pressemappe/104/aom]*** OTS-ORIGINALTEXT PRESSEAUSSENDUNG UNTER AUSSCHLIESSLICHER INHALTLICHER VERANTWORTUNG DES AUSSENDERS - WWW.OTS.AT ***Austria Presse AgenturDocument AUPAG00020120906e896002bd"
